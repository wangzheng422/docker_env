> [!WARNING]
> working in progress
# serving mini model on cpu only with ocp ai through vllm

We wants to testing openshift ai, focusing on networking, not focusing on gpu, and we do not have nvidia gpu. So we need a small llm model, and serving it on cpu, and try to expose it using http.

We will use DistilGPT-2, it is a very small model, the performance of this model is really bad, but it does not matter, it can produce response at least.

# deploy ocp ai

We will deploy ocp ai, for exposing http, we keep kserve using raw deployment mode. This will not involve the service mesh and serverless, which is useless for my use case.

We also need minio for s3, and we 

## download model

First, we need to download the llm model it self.

```bash
# on vultr

# download the model first
dnf install -y conda


mkdir -p /data/env/
conda create -y -p /data/env/hg_cli python=3.11

conda init bash

conda activate /data/env/hg_cli
# conda deactivate

# python -m pip install --upgrade pip setuptools wheel

pip install --upgrade huggingface_hub


# for distilbert/distilgpt2
VAR_NAME=distilbert/distilgpt2

VAR_NAME_FULL=${VAR_NAME//\//-}
echo $VAR_NAME_FULL
# THUDM-ChatGLM2-6B

mkdir -p /data/huggingface/${VAR_NAME_FULL}
cd /data/huggingface/${VAR_NAME_FULL}

while true; do
    huggingface-cli download --repo-type model --revision main --cache-dir /data/huggingface/cache --local-dir ./ --local-dir-use-symlinks False --exclude "*.pt"  --resume-download ${VAR_NAME} 
    if [ $? -eq 0 ]; then
        break
    fi
    sleep 1  # Optional: waits for 1 second before trying again
done

```

## create minio image with llm model

We will need a s3 service, which we will use minio to provide. We will embed llm model to the minio image, so after the minio start, the llm is there.

```bash

# create the model image
mkdir -p /data/workspace/llm
cd /data/workspace/llm

rsync -avz /data/huggingface/${VAR_NAME_FULL}/ /data/workspace/llm/models/

cat << 'EOF' > Dockerfile
FROM docker.io/minio/minio:RELEASE.2021-06-17T00-10-46Z.hotfix.35a0912ff as minio-examples

EXPOSE 9000

ARG MODEL_DIR=/data1/

USER root

RUN useradd -u 1000 -g 0 modelmesh
RUN mkdir -p ${MODEL_DIR}
RUN chown -R 1000:0 /data1 && \
    chgrp -R 0 /data1 && \
    chmod -R g=u /data1

# COPY --chown=1000:0 models ${MODEL_DIR}/models/models
COPY models ${MODEL_DIR}/models/models

USER 1000

EOF

podman build -t quay.io/wangzheng422/qimgs:minio-distilgpt2-v03 -f Dockerfile .

podman push quay.io/wangzheng422/qimgs:minio-distilgpt2-v03

```

## deploy the minio

We deploy the minio.

```bash

# on helper
S3_NAME='distilgpt2'
S3_NS='demo-llm'
S3_IMAGE='quay.io/wangzheng422/qimgs:minio-distilgpt2-v03'

oc new-project ${S3_NS}
oc label --overwrite ns ${S3_NS} \
   pod-security.kubernetes.io/enforce=privileged

cat << EOF > ${BASE_DIR}/data/install/s3-codellama.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: minio-${S3_NAME}
spec:
  ports:
    - name: minio-client-port
      port: 9000
      protocol: TCP
      targetPort: 9000
    - name: minio-webui-port
      port: 9001
      protocol: TCP
      targetPort: 9001
  selector:
    app: minio-${S3_NAME}

---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: s3-${S3_NAME}
spec:
  to:
    kind: Service
    name: minio-${S3_NAME}
  port:
    targetPort: 9000

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: minio-${S3_NAME}
  labels:
    app: minio-${S3_NAME}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: minio-${S3_NAME}
  template:
    metadata:
      labels:
        app: minio-${S3_NAME}
    spec:
      containers:
        - args:
            - server
            - /data1
          env:
            - name: MINIO_ACCESS_KEY
              value:  admin
            - name: MINIO_SECRET_KEY
              value: password
          image: ${S3_IMAGE}
          imagePullPolicy: IfNotPresent
          name: minio
        #   nodeSelector:
        #     kubernetes.io/hostname: "worker-01-demo"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
                drop:
                - ALL
            runAsNonRoot: true
            seccompProfile:
                type: RuntimeDefault

---
apiVersion: v1
kind: Secret
metadata:
  annotations:
    serving.kserve.io/s3-endpoint: minio-${S3_NAME}.${S3_NS}.svc:9000 # replace with your s3 endpoint e.g minio-service.kubeflow:9000
    serving.kserve.io/s3-usehttps: "0" # by default 1, if testing with minio you can set to 0
    serving.kserve.io/s3-region: "us-east-2"
    serving.kserve.io/s3-useanoncredential: "false" # omitting this is the same as false, if true will ignore provided credential and use anonymous credentials
  name: storage-config-${S3_NAME}
stringData:
  "AWS_ACCESS_KEY_ID": "admin"
  "AWS_SECRET_ACCESS_KEY": "password"

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: sa-${S3_NAME}
secrets:
- name: storage-config-${S3_NAME}

EOF

oc apply -n ${S3_NS} -f ${BASE_DIR}/data/install/s3-codellama.yaml

# oc delete -n ${S3_NS} -f ${BASE_DIR}/data/install/s3-codellama.yaml

# open in browser to check
# http://s3-codellama-llm-demo.apps.demo-gpu.wzhlab.top/minio/models/
# http://s3-codellama-llm-demo.apps.demo-gpu.wzhlab.top/minio/models/CodeLlama-13b-Instruct-hf/


```

## create a serving runtime

We need to create a vllm serving runtime for cpu only. The default one in ocp ai, will only work with nvidia gpu.

Here is the reference.
- https://github.com/rh-aiservices-bu/llm-on-openshift/blob/main/serving-runtimes/vllm_runtime/vll-runtime-cpu.yaml

```yaml
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
labels:
  opendatahub.io/dashboard: "true"
metadata:
  annotations:
    openshift.io/display-name: "vLLM for CPU"
  name: vllm-runtime-cpu
spec:
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "8080"
  builtInAdapter:
    modelLoadingTimeoutMillis: 90000
  containers:
    - args:
        - --model
        - /mnt/models/
        - --download-dir
        - /models-cache
        - --port
        - "8080"
        - --max-model-len
        - "1024"
      image: quay.io/rh-aiservices-bu/vllm-cpu-openai-ubi9:0.2
      command:
        - python
        - -m
        - vllm.entrypoints.openai.api_server
      name: kserve-container
      ports:
        - containerPort: 8080
          name: http1
          protocol: TCP
  multiModel: false
  supportedModelFormats:
    - autoSelect: true
      name: vLLM
```
## create model servering

## expose http route
After create a model servering, it will create pod and service, we expose the service by route.

```yaml
kind: Route
apiVersion: route.openshift.io/v1
metadata:
  name: llm
spec:
  host: llm-wzh-ai-01.apps.demo-01-rhsys.wzhlab.top
  to:
    kind: Service
    name: distilgpt2-predictor
    weight: 100
  port:
    targetPort: http1
  wildcardPolicy: None
```

## curl to testing

curl to test, we can see, it expose the service using http.

```bash

curl -v http://llm-wzh-ai-01.apps.demo-01-rhsys.wzhlab.top/v1/chat/completions \
-H "Content-Type: application/json" \
-d '{"model":"/mnt/models/",
     "messages":
        [{"role":"user",
          "content":
             [{"type":"text", "text":"give me example"
              }
             ]
         }
        ]
    }'
# *   Trying 192.168.50.23:80...
# * Connected to llm-wzh-ai-01.apps.demo-01-rhsys.wzhlab.top (192.168.50.23) port 80 (#0)
# > POST /v1/chat/completions HTTP/1.1
# > Host: llm-wzh-ai-01.apps.demo-01-rhsys.wzhlab.top
# > User-Agent: curl/7.76.1
# > Accept: */*
# > Content-Type: application/json
# > Content-Length: 200
# >
# * Mark bundle as not supporting multiuse
# < HTTP/1.1 200 OK
# < date: Fri, 06 Dec 2024 15:13:25 GMT
# < server: uvicorn
# < content-length: 1427
# < content-type: application/json
# < set-cookie: fb2e7f517fef0f3147f969b6e5c6592e=ccc239021d83b5fedb39f97374356160; path=/; HttpOnly
# <
# {"id":"cmpl-bd4205e5a732496d928283dc467cf0b5","object":"chat.completion","created":1733498006,"model":"/mnt/models/","choices":[{"index":0,"message":{"role":"assistant","content":"As a result, we have released a new version of the P-N-A-R-R for Android. This is the first release of P-N-A-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-","tool_calls":[]},"logprobs":null,"finish_reason":"length","stop_reason":null}],"usage":{"prompt_tokens":4,"total_tokens":1024,"completion_tokens":1020}}

```

# end