# running llama factory using ray on openshift ai


# build image

To run llama factory using rayjob, we need to build the image contains the rayjob and llama factory.

## build on os

We try to run everything on a rocky linux server. As a best practice, it is better to run everything on a vm, if everything is ok, we can wrap it into a container later.

```bash

# download the model first
dnf install -y conda


mkdir -p /data/env/
conda create -y -p /data/env/hg_cli python=3.11

conda init bash

conda activate /data/env/hg_cli
# conda deactivate

# python -m pip install --upgrade pip setuptools wheel

pip install --upgrade huggingface_hub

# for Maykeye/TinyLLama-v0
VAR_NAME=Maykeye/TinyLLama-v0

VAR_NAME_FULL=${VAR_NAME//\//-}
echo $VAR_NAME_FULL
# THUDM-ChatGLM2-6B

mkdir -p /data/huggingface/${VAR_NAME_FULL}
cd /data/huggingface/${VAR_NAME_FULL}

while true; do
    huggingface-cli download --repo-type model --revision main --cache-dir /data/huggingface/cache --local-dir ./ --local-dir-use-symlinks False --exclude "*.pt"  --resume-download ${VAR_NAME} 
    if [ $? -eq 0 ]; then
        break
    fi
    sleep 1  # Optional: waits for 1 second before trying again
done


# install and try llama factory
mkdir -p /data/git
cd /data/git

git clone -b wzh-stable https://github.com/wangzheng422/LLaMA-Factory

cd /data/git/LLaMA-Factory

pip install -e ".[torch,metrics]"


/bin/rm -rf saves/tinyllama
llamafactory-cli train wzh/tinyllama_lora_sft.yaml

```

## wrap into a container

We can run smoothly on os, now we wrap it into container.

```bash

# build the docker image based on offical pytorch image
cd /data/git/LLaMA-Factory

/bin/rm -rf ./huggingface/Maykeye-TinyLLama-v0
mkdir -p ./huggingface/Maykeye-TinyLLama-v0
/bin/cp -r /data/huggingface/Maykeye-TinyLLama-v0 ./huggingface/Maykeye-TinyLLama-v0

podman build -t quay.io/wangzheng422/qimgs:llama-factory-20241225-v01 -f wzh/cuda.Dockerfile .

podman run --rm -it quay.io/wangzheng422/qimgs:llama-factory-20241225-v01 /bin/bash

podman push quay.io/wangzheng422/qimgs:llama-factory-20241225-v01

```

## try run multiple instances of the LLaMA factory

We have successfully set up the environment and built the container image, next, we will try to run multiple instances of the LLaMA factory using docker/podman.

```bash
# create a pod network
podman network create llama-factory-network --subnet 10.5.0.0/24

podman pod create --name llama-factory-pod --network llama-factory-network

mkdir -p /data/instance/output
mkdir -p /data/instance/saves

# first instance

podman run --rm -it --pod llama-factory-pod \
--network llama-factory-network \
--ip 10.5.0.3 \
-v /data/instance/output:/app/output:Z \
-v /data/instance/saves:/app/saves:Z \
quay.io/wangzheng422/qimgs:llama-factory-20241225-v01 \
/bin/bash

FORCE_TORCHRUN=1 NNODES=2 NODE_RANK=0 MASTER_ADDR=10.5.0.3 MASTER_PORT=29500 NPROC_PER_NODE=1 llamafactory-cli train wzh/tinyllama_lora_sft.yaml




# 2nd instance

podman run --rm -it --pod llama-factory-pod \
--network llama-factory-network \
-v /data/instance/output:/app/output:Z \
-v /data/instance/saves:/app/saves:Z \
quay.io/wangzheng422/qimgs:llama-factory-20241225-v01 \
/bin/bash


FORCE_TORCHRUN=1 NNODES=2 NODE_RANK=1 MASTER_ADDR=10.5.0.3 MASTER_PORT=29500 NPROC_PER_NODE=1 llamafactory-cli train wzh/tinyllama_lora_sft.yaml


```

We can not make it run on multiple nodes, because we do not have nvidia gpu, the torchrun will fallback to MPI, which llama factory seems has bugs, it can not check local rank, all the node will be `rank=0`, which will make the `DistributedDataParallel` failed.

But this will not stop us, our target is to run the multiple node task using `ray job` to manage the distributed training effectively. The job failed does not deter us from exploring alternative solutions.


# run multiple instance using deployment on ocp

## build image

```bash

cd /data/git/LLaMA-Factory

podman build -t quay.io/wangzheng422/qimgs:llama-factory-ray-20250106-v07 -f wzh/ray.dockerfile .


podman run --rm -it quay.io/wangzheng422/qimgs:llama-factory-ray-20250106-v07 /bin/bash


podman push quay.io/wangzheng422/qimgs:llama-factory-ray-20250106-v07

```

using venv

```bash
cd /data/git/LLaMA-Factory

podman build -t quay.io/wangzheng422/qimgs:llama-factory-ray-20250106-v06 -f wzh/ray.venv.dockerfile .


podman run --rm -it quay.io/wangzheng422/qimgs:llama-factory-ray-20250106-v06 /bin/bash


podman push quay.io/wangzheng422/qimgs:llama-factory-ray-20250106-v06

```

## try to run multipe node

```bash

# first instance

podman run --rm -it --pod llama-factory-pod \
--network llama-factory-network \
--ip 10.5.0.3 \
-v /data/instance/output:/app/output:Z \
-v /data/instance/saves:/app/saves:Z \
quay.io/wangzheng422/qimgs:llama-factory-ray-20241226-v01 \
/bin/bash

FORCE_TORCHRUN=1 NNODES=2 NODE_RANK=0 MASTER_ADDR=10.5.0.3 MASTER_PORT=29500 NPROC_PER_NODE=1 llamafactory-cli train wzh/tinyllama_lora_sft.yaml




# 2nd instance

podman run --rm -it --pod llama-factory-pod \
--network llama-factory-network \
-v /data/instance/output:/app/output:Z \
-v /data/instance/saves:/app/saves:Z \
quay.io/wangzheng422/qimgs:llama-factory-ray-20241226-v01 \
/bin/bash


FORCE_TORCHRUN=1 NNODES=2 NODE_RANK=1 MASTER_ADDR=10.5.0.3 MASTER_PORT=29500 NPROC_PER_NODE=1 llamafactory-cli train wzh/tinyllama_lora_sft.yaml

```

## try to run in deepspeed

```bash

# first instance

podman run --rm -it --pod llama-factory-pod \
--network llama-factory-network \
--ip 10.5.0.3 \
quay.io/wangzheng422/qimgs:llama-factory-ray-20250106-v07 \
/bin/bash

FORCE_TORCHRUN=1 NNODES=2 NODE_RANK=0 MASTER_ADDR=10.5.0.3 MASTER_PORT=29500 NPROC_PER_NODE=1 llamafactory-cli train wzh/tinyllama_lora_sft_dp.yaml




# 2nd instance

podman run --rm -it \
--pod llama-factory-pod \
--network llama-factory-network \
quay.io/wangzheng422/qimgs:llama-factory-ray-20250106-v07 \
/bin/bash


FORCE_TORCHRUN=1 NNODES=2 NODE_RANK=1 MASTER_ADDR=10.5.0.3 MASTER_PORT=29500 NPROC_PER_NODE=1 llamafactory-cli train wzh/tinyllama_lora_sft_dp.yaml


```

## try basic ray

![](imgs/2024.12.llama.factory.md/2024-12-26-21-10-39.png)

![](imgs/2024.12.llama.factory.md/2024-12-26-21-10-52.png)


# end