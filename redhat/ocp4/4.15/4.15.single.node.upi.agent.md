# openshift 4.14 UPI in agent way, single node

OpenShift的安装方式很多了，现在又多了一种，agent based installer。最大的特点是，不需要额外的bootstrap节点了。这可是天大的好消息，因为，以前安装之前，和客户交流，客户总是不理解，为什么红帽说支持3节点部署，但是却要求提供4台服务器。也不能怪客户，按照一般的理解，之前红帽是不支持严格意义上的3节点部署，就因为有这个bootstrap. 现在好了，agent based installer是真正世俗意义上的支持3节点部署了。

从官方文档来看，能压缩掉bootstrap，是因为bootstrap相关的服务，都压缩到一个master节点上，并使用了assisted installer流程，来达到真正的3节点安装的。

- https://docs.openshift.com/container-platform/4.13/installing/installing_with_agent_based_installer/preparing-to-install-with-agent-based-installer.html

本文，就用agent based installer来装一个单节点的ocp集群。

# on helper node

```bash

# switch to you install version

export BUILDNUMBER=4.14.10

pushd /data/ocp4/${BUILDNUMBER}
tar -xzf openshift-client-linux-${BUILDNUMBER}.tar.gz -C /usr/local/bin/
tar -xzf openshift-install-linux-${BUILDNUMBER}.tar.gz -C /usr/local/bin/
# tar -xzf oc-mirror.tar.gz -C /usr/local/bin/
# chmod +x /usr/local/bin/oc-mirror
install -m 755 /data/ocp4/clients/butane-amd64 /usr/local/bin/butane
install -m 755 /data/ocp4/clients/coreos-installer_amd64 /usr/local/bin/coreos-installer
popd

# create a user and create the cluster under the user

useradd -m 3node

su - 3node

ssh-keygen

cat << EOF > ~/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null
EOF

chmod 600 ~/.ssh/config

cat << 'EOF' >> ~/.bashrc

export BASE_DIR='/home/3node/'

EOF

# export BASE_DIR='/home/3node/'

export BUILDNUMBER=4.14.10

pushd /data/ocp-${BUILDNUMBER}
tar -xzf openshift-client-linux-${BUILDNUMBER}.tar.gz -C ~/.local/bin/
tar -xzf openshift-install-linux-${BUILDNUMBER}.tar.gz -C ~/.local/bin/
# tar -xzf oc-mirror.tar.gz -C /usr/local/bin/
# chmod +x /usr/local/bin/oc-mirror
install -m 755 /data/ocp4/clients/butane-amd64 ~/.local/bin//butane
install -m 755 /data/ocp4/clients/coreos-installer_amd64 ~/.local/bin//coreos-installer
popd

export BUILDNUMBER=4.14.10

mkdir -p ${BASE_DIR}/data/{sno/disconnected,install}

# set some parameter of you rcluster

NODE_SSH_KEY="$(cat ${BASE_DIR}/.ssh/id_rsa.pub)"
INSTALL_IMAGE_REGISTRY=quaylab.infra.wzhlab.top:5443

# PULL_SECRET='{"auths":{"registry.redhat.io": {"auth": "ZHVtbXk6ZHVtbXk=","email": "noemail@localhost"},"registry.ocp4.redhat.ren:5443": {"auth": "ZHVtbXk6ZHVtbXk=","email": "noemail@localhost"},"'${INSTALL_IMAGE_REGISTRY}'": {"auth": "'$( echo -n 'admin:shadowman' | openssl base64 )'","email": "noemail@localhost"}}}'
PULL_SECRET=$(cat /data/pull-secret.json)

NTP_SERVER=172.21.6.11
HELP_SERVER=172.21.6.11
# KVM_HOST=192.168.7.11
# API_VIP=192.168.77.99
# INGRESS_VIP=192.168.77.98
# CLUSTER_PROVISION_IP=192.168.7.103
# BOOTSTRAP_IP=192.168.7.12
MACHINE_NETWORK='172.21.6.0/24'

# 定义单节点集群的节点信息
SNO_CLUSTER_NAME=demo-gpu
SNO_BASE_DOMAIN=wzhlab.top

BOOTSTRAP_IP=172.21.6.22
MASTER_01_IP=172.21.6.23
MASTER_02_IP=172.21.6.24
MASTER_03_IP=172.21.6.25
WORKER_01_IP=172.21.6.26
WORKER_02_IP=172.21.6.27
WORKER_03_IP=172.21.6.28

BOOTSTRAP_IPv6=fd03::22
MASTER_01_IPv6=fd03::23
MASTER_02_IPv6=fd03::24
MASTER_03_IPv6=fd03::25
WORKER_01_IPv6=fd03::26
WORKER_02_IPv6=fd03::27
WORKER_03_IPv6=fd03::28

BOOTSTRAP_HOSTNAME=bootstrap-demo
MASTER_01_HOSTNAME=master-01-demo
MASTER_02_HOSTNAME=master-02-demo
MASTER_03_HOSTNAME=master-03-demo
WORKER_01_HOSTNAME=worker-01-demo
WORKER_02_HOSTNAME=worker-02-demo
WORKER_03_HOSTNAME=worker-03-demo

BOOTSTRAP_INTERFACE=enp1s0
MASTER_01_INTERFACE=enp1s0
MASTER_02_INTERFACE=enp1s0
MASTER_03_INTERFACE=enp1s0
WORKER_01_INTERFACE=enp1s0
WORKER_02_INTERFACE=enp1s0
WORKER_03_INTERFACE=enp1s0

MASTER_01_INTERFACE_MAC=52:54:00:12:A1:01
MASTER_02_INTERFACE_MAC=52:54:00:12:A1:02
MASTER_03_INTERFACE_MAC=52:54:00:12:A1:03
WORKER_01_INTERFACE_MAC=52:54:00:12:A1:04
WORKER_02_INTERFACE_MAC=52:54:00:12:A1:05
WORKER_03_INTERFACE_MAC=52:54:00:12:A1:06

BOOTSTRAP_DISK=/dev/vda
MASTER_01_DISK=/dev/vda
MASTER_02_DISK=/dev/vda
MASTER_03_DISK=/dev/vda
WORKER_01_DISK=/dev/vda
WORKER_02_DISK=/dev/vda
WORKER_03_DISK=/dev/vda

OCP_GW=172.21.6.254
OCP_NETMASK=255.255.255.0
OCP_NETMASK_S=24
OCP_DNS=172.21.6.11

OCP_GW_v6=fd03::11
OCP_NETMASK_v6=64

# echo ${SNO_IF_MAC} > /data/sno/sno.mac

mkdir -p ${BASE_DIR}/data/install
cd ${BASE_DIR}/data/install

/bin/rm -rf *.ign .openshift_install_state.json auth bootstrap manifests master*[0-9] worker*[0-9] *

cat << EOF > ${BASE_DIR}/data/install/install-config.yaml 
apiVersion: v1
baseDomain: $SNO_BASE_DOMAIN
compute:
- name: worker
  replicas: 0 
controlPlane:
  name: master
  replicas: 1
metadata:
  name: $SNO_CLUSTER_NAME
networking:
  # OVNKubernetes , OpenShiftSDN
  networkType: OVNKubernetes 
  clusterNetwork:
    - cidr: 10.132.0.0/14 
      hostPrefix: 23
    # - cidr: fd02::/48
    #   hostPrefix: 64
  machineNetwork:
    - cidr: $MACHINE_NETWORK
    # - cidr: 2001:DB8::/32
  serviceNetwork:
    - 172.22.0.0/16
    # - fd03::/112
platform: 
  none: {}
pullSecret: '${PULL_SECRET}'
sshKey: |
$( cat ${BASE_DIR}/.ssh/id_rsa.pub | sed 's/^/   /g' )
additionalTrustBundle: |
$( cat /etc/crts/redhat.ren.ca.crt | sed 's/^/   /g' )
imageContentSources:
- mirrors:
  - ${INSTALL_IMAGE_REGISTRY}/ocp4/openshift4
  source: quay.io/openshift-release-dev/ocp-release
- mirrors:
  - ${INSTALL_IMAGE_REGISTRY}/ocp4/openshift4
  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
EOF

cat << EOF > ${BASE_DIR}/data/install/agent-config.yaml
apiVersion: v1alpha1
kind: AgentConfig
metadata:
  name: $SNO_CLUSTER_NAME
rendezvousIP: $MASTER_01_IP
additionalNTPSources:
- $NTP_SERVER
hosts:
  - hostname: $MASTER_01_HOSTNAME
    role: master
    rootDeviceHints:
      deviceName: "$MASTER_01_DISK"
    interfaces:
      - name: $MASTER_01_INTERFACE
        macAddress: $MASTER_01_INTERFACE_MAC
    networkConfig:
      interfaces:
        - name: $MASTER_01_INTERFACE
          type: ethernet
          state: up
          mac-address: $MASTER_01_INTERFACE_MAC
          ipv4:
            enabled: true
            address:
              - ip: $MASTER_01_IP
                prefix-length: $OCP_NETMASK_S
            dhcp: false
      dns-resolver:
        config:
          server:
            - $OCP_DNS
      routes:
        config:
          - destination: 0.0.0.0/0
            next-hop-address: $OCP_GW
            next-hop-interface: $MASTER_01_INTERFACE
            table-id: 254        
EOF

/bin/cp -f ${BASE_DIR}/data/install/install-config.yaml ${BASE_DIR}/data/install/install-config.yaml.bak

openshift-install --dir=${BASE_DIR}/data/install agent create cluster-manifests

# you can change the system's registry config
# but do not need under normal situation
# sudo bash -c "/bin/cp -f mirror/registries.conf /etc/containers/registries.conf.d/; chmod +r /etc/containers/registries.conf.d/*"

# /bin/cp -f  /data/ocp4/ansible-helper/files/* ${BASE_DIR}/data/install/openshift/

sudo bash -c "cd /data/ocp4 ; bash image.registries.conf.sh quaylab.infra.wzhlab.top:5443 ;"

# we lost the way to customize the registry during install stage
# leave the configuration to day-2 operation.
# /bin/cp -f /data/ocp4/99-worker-container-registries.yaml ${BASE_DIR}/data/install/openshift
# /bin/cp -f /data/ocp4/99-master-container-registries.yaml ${BASE_DIR}/data/install/openshift

cd ${BASE_DIR}/data/install/

# openshift-install --dir=${BASE_DIR}/data/install create ignition-configs 

# there is additinal file cache
# 我们发现，除了iso，其他文件，就算提前下载了，他还是会重新下载。
mkdir -p ${HOME}/.cache/agent/{files_cache,image_cache}

# oc image extract -a /data/pull-secret.json --path /coreos/coreos-x86_64.iso:${HOME}/.cache/agent/image_cache --confirm quaylab.infra.wzhlab.top:5443/ocp4/openshift4:$BUILDNUMBER-x86_64-machine-os-images

# oc image extract -a /data/pull-secret.json --path /usr/lib64/libnmstate.so.*:${HOME}/.cache/agent/files_cache --confirm  quaylab.infra.wzhlab.top:5443/ocp4/openshift4:$BUILDNUMBER-x86_64-agent-installer-node-agent

# oc image extract -a /data/pull-secret.json --path /usr/bin/agent-tui:${HOME}/.cache/agent/files_cache --confirm  quaylab.infra.wzhlab.top:5443/ocp4/openshift4:$BUILDNUMBER-x86_64-agent-installer-node-agent

mkdir -p ~/.cache/agent/image_cache/
/bin/cp -f /data/ocp-$BUILDNUMBER/rhcos-live.x86_64.iso ~/.cache/agent/image_cache/coreos-x86_64.iso

openshift-install --dir=${BASE_DIR}/data/install agent create image --log-level=debug
# ......
# DEBUG Fetching image from OCP release (oc adm release info --image-for=machine-os-images --insecure=true --icsp-file=/tmp/icsp-file3636774741 quay.io/openshift-release-dev/ocp-release@sha256:96bf74ce789ccb22391deea98e0c5050c41b67cc17defbb38089d32226dba0b8)
# DEBUG The file was found in cache: /home/3node/.cache/agent/image_cache/coreos-x86_64.iso
# INFO Verifying cached file
# DEBUG extracting /coreos/coreos-x86_64.iso.sha256 to /tmp/cache1876698393, oc image extract --path /coreos/coreos-x86_64.iso.sha256:/tmp/cache1876698393 --confirm --icsp-file=/tmp/icsp-file455852761 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:052130abddf741195b6753888cf8a00757dedeb7010f7d4dcc4b842b5bc705f6
# ......

coreos-installer iso ignition show agent.x86_64.iso > ignition.ign

# HTTP_PATH=http://192.168.7.11:8080/ignition

source /data/ocp4/acm.fn.sh

# 我们会创建一个wzh用户，密码是redhat，这个可以在第一次启动的是，从console/ssh直接用用户名口令登录
# 方便排错和研究
VAR_PWD_HASH="$(python3 -c 'import crypt,getpass; print(crypt.crypt("redhat"))')"

cat ${BASE_DIR}/data/install/ignition.ign \
  | jq --arg VAR "$VAR_PWD_HASH" --arg VAR_SSH "$NODE_SSH_KEY" '.passwd.users += [{ "name": "wzh", "system": true, "passwordHash": $VAR , "sshAuthorizedKeys": [ $VAR_SSH ], "groups": [ "adm", "wheel", "sudo", "systemd-journal"  ] }]' \
  | jq '. += { "kernel_arguments" : { "should_exist" : [ "systemd.debug-shell=1" ] } }' \
  | jq -c . \
  > ${BASE_DIR}/data/install/ignition-iso.ign

coreos-installer iso ignition embed -f -i ignition-iso.ign agent.x86_64.iso

# VAR_IMAGE_VER=rhcos-410.86.202303200936-AnolisOS-0-live.x86_64.iso


```

# on kvm host ( 102 )

## cleanup

```bash

# clean all lv
# List all LVs and grep those starting with 'lv'
lv_list=$(lvdisplay -c | cut -d: -f1 | grep '.*/lv')

# Loop through each LV and remove it
for lv in $lv_list; do
  echo "Deleting $lv..."
  lvremove -f $lv
done

create_lv() {
    var_vg=$1
    var_pool=$2
    var_lv=$3
    var_size=$4
    var_action=$5
    lvremove -f $var_vg/$var_lv || true
    # lvcreate -y -L $var_size -n $var_lv $var_vg
    if [ "$var_action" == "recreate" ]; then
      lvcreate --type thin -n $var_lv -V $var_size --thinpool $var_vg/$var_pool
      wipefs --all --force /dev/$var_vg/$var_lv
    fi
}

# virsh destroy ocp4-acm-one-bootstrap
# virsh undefine ocp4-acm-one-bootstrap

# create_lv vgdata poolA lvacm-one-bootstrap 500G 
# create_lv vgdata poolA lvacm-one-bootstrap-data 500G 

virsh destroy ocp4-demo-gpu-master-01
virsh undefine ocp4-demo-gpu-master-01

create_lv vgdata poolA lv-demo-gpu-master-01 500G remove
# create_lv vgdata poolA lvacm-one-master-01-data 500G 

# virsh destroy ocp4-acm-one-master-02
# virsh undefine ocp4-acm-one-master-02

# create_lv vgdata poolA lvacm-one-master-02 500G 
# create_lv vgdata poolA lvacm-one-master-02-data 500G 

# virsh destroy ocp4-acm-one-master-03
# virsh undefine ocp4-acm-one-master-03

# create_lv vgdata poolA lvacm-one-master-03 500G 
# create_lv vgdata poolA lvacm-one-master-03-data 500G 

```

## begin

```bash

cat << EOF >> /etc/sysctl.d/99-wzh-sysctl.conf

vm.overcommit_memory = 1

EOF
sysctl --system

# 创建实验用虚拟网络

mkdir -p /data/kvm
cd /data/kvm

cat << 'EOF' > /data/kvm/bridge.sh
#!/usr/bin/env bash

PUB_CONN='eno1'
PUB_IP='172.21.6.103/24'
PUB_GW='172.21.6.254'
PUB_DNS='172.21.1.1'

nmcli con down "$PUB_CONN"
nmcli con delete "$PUB_CONN"
nmcli con down baremetal
nmcli con delete baremetal
# RHEL 8.1 appends the word "System" in front of the connection,delete in case it exists
nmcli con down "System $PUB_CONN"
nmcli con delete "System $PUB_CONN"
nmcli connection add ifname baremetal type bridge con-name baremetal ipv4.method 'manual' \
    ipv4.address "$PUB_IP" \
    ipv4.gateway "$PUB_GW" \
    ipv4.dns "$PUB_DNS"
    
nmcli con add type bridge-slave ifname "$PUB_CONN" master baremetal
nmcli con down "$PUB_CONN";pkill dhclient;dhclient baremetal
nmcli con up baremetal
EOF
bash /data/kvm/bridge.sh

nmcli con mod baremetal +ipv4.addresses "192.168.7.103/24"
nmcli con up baremetal

cat << EOF > /root/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null
EOF

pvcreate -y /dev/vdb
vgcreate vgdate /dev/vdb

# https://access.redhat.com/articles/766133
lvcreate -y -n poolA -L 350G vgdata
lvcreate -y -n poolA_meta -L 10G vgdata
lvconvert -y --thinpool vgdata/poolA --poolmetadata vgdata/poolA_meta

lvextend -l +100%FREE vgdata/poolA

mkdir -p /data/kvm/one/

scp -o Compression=no root@172.21.6.11:/home/3node/data/install/agent.x86_64.iso /data/kvm/one/

create_lv() {
    var_vg=$1
    var_pool=$2
    var_lv=$3
    var_size=$4
    var_action=$5
    lvremove -f $var_vg/$var_lv || true
    # lvcreate -y -L $var_size -n $var_lv $var_vg
    if [ "$var_action" == "recreate" ]; then
      lvcreate --type thin -n $var_lv -V $var_size --thinpool $var_vg/$var_pool
      wipefs --all --force /dev/$var_vg/$var_lv
    fi
}


SNO_MEM=198

virsh destroy ocp4-demo-gpu-master-01
virsh undefine ocp4-demo-gpu-master-01

create_lv vgdata poolA lv-demo-gpu-master-01 350G recreate
# create_lv vgdata poolA lvacm-one-master-01-data 500G recreate

virt-install --name=ocp4-demo-gpu-master-01 --vcpus=38 --ram=$(($SNO_MEM*1024)) \
  --cpu=host-model \
  --disk path=/dev/vgdata/lv-demo-gpu-master-01,device=disk,bus=virtio,format=raw \
  --os-variant rhel8.3 \
  --network bridge=baremetal,model=virtio,mac=52:54:00:12:A1:01 \
  --graphics vnc,port=59003 --noautoconsole \
  --boot menu=on --cdrom /data/kvm/one/agent.x86_64.iso

# is install stopped
# login to the vm
# check and change dns setting 
# under most situation, the dns setting is wrong
# then restart several systemd service, to continue the install
systemctl restart apply-host-config.service
systemctl restart agent
systemctl restart start-cluster-installation.service


```

# on helper to see result

for unkonwn reason, the vm will be shutdown, instead of reboot, you have to poweron it manually.

```bash
cd ${BASE_DIR}/data/install
export KUBECONFIG=${BASE_DIR}/data/install/auth/kubeconfig
echo "export KUBECONFIG=${BASE_DIR}/data/install/auth/kubeconfig" >> ~/.bashrc
# oc completion bash | sudo tee /etc/bash_completion.d/openshift > /dev/null


cd ${BASE_DIR}/data/install
openshift-install --dir=${BASE_DIR}/data/install agent wait-for bootstrap-complete --log-level=debug
# ......
# DEBUG RendezvousIP from the AgentConfig 172.21.6.23
# INFO Bootstrap Kube API Initialized
# INFO Bootstrap configMap status is complete
# INFO cluster bootstrap is complete

cd ${BASE_DIR}/data/install
openshift-install --dir=${BASE_DIR}/data/install agent wait-for install-complete --log-level=debug
# ......
# INFO Cluster is installed
# INFO Install complete!
# INFO To access the cluster as the system:admin user when using 'oc', run
# INFO     export KUBECONFIG=/home/3node/data/install/auth/kubeconfig
# INFO Access the OpenShift web-console here: https://console-openshift-console.apps.demo-gpu.wzhlab.top
# INFO Login to the console with user: "kubeadmin", and password: "BZ9yI-RNv4B-eH7Jm-vpScB"

```

# password login and oc config

```bash

# init setting for helper node
cat << EOF > ~/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null
EOF
chmod 600 ~/.ssh/config

# upgrade ssh to ed25519
# ssh-keygen -o -a 100 -t ed25519


# on helper, as user 3node
# for master node
cat > ${BASE_DIR}/data/install/crack.txt << 'EOF'

echo redhat | sudo passwd --stdin root

sudo sh -c 'echo "PasswordAuthentication yes" > /etc/ssh/sshd_config.d/20-wzh.conf '
sudo sh -c 'echo "PermitRootLogin yes" >> /etc/ssh/sshd_config.d/20-wzh.conf '
sudo sh -c 'echo "ClientAliveInterval 1800" >> /etc/ssh/sshd_config.d/20-wzh.conf '

sudo systemctl restart sshd

sudo sh -c 'echo "export KUBECONFIG=/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig" >> /root/.bashrc'

sudo sh -c 'echo "RET=\`oc config use-context system:admin\`" >> /root/.bashrc'

EOF

for i in 23
do
  ssh core@172.21.6.$i < ${BASE_DIR}/data/install/crack.txt
done

# for worker node
cat > ${BASE_DIR}/data/install/crack.worker.txt << 'EOF'

echo redhat | sudo passwd --stdin root

sudo sh -c 'echo "PasswordAuthentication yes" > /etc/ssh/sshd_config.d/20-wzh.conf '
sudo sh -c 'echo "PermitRootLogin yes" >> /etc/ssh/sshd_config.d/20-wzh.conf '
sudo sh -c 'echo "ClientAliveInterval 1800" >> /etc/ssh/sshd_config.d/20-wzh.conf '

sudo systemctl restart sshd

EOF

for i in 26
do
  ssh core@172.21.6.$i < ${BASE_DIR}/data/install/crack.worker.txt
done

```

## from other host

```bash
# https://unix.stackexchange.com/questions/230084/send-the-password-through-stdin-in-ssh-copy-id
dnf install -y sshpass

for i in 23 26
do
  sshpass -p 'redhat' ssh-copy-id root@172.21.6.$i
done

```

## poweroff

```bash

for i in 23 24 25
do
  ssh root@192.168.7.$i poweroff
done

```

## poweron

```bash

virsh start ocp4-acm-one-master-01

virsh start ocp4-acm-one-master-02

virsh start ocp4-acm-one-master-03

```


# back and merge kubeconfig

```bash

mkdir -p ~/.kube/bak/

var_date=$(date '+%Y-%m-%d-%H%M')

/bin/cp -f /data/install/auth/kubeconfig ~/.kube/bak/kubeconfig-$var_date
/bin/cp -f /data/install/auth/kubeadmin-password ~/.kube/bak/kubeadmin-password-$var_date

sed "s/admin/admin\/$SNO_CLUSTER_NAME/g" /data/install/auth/kubeconfig > /tmp/config.new

# https://medium.com/@jacobtomlinson/how-to-merge-kubernetes-kubectl-config-files-737b61bd517d
/bin/cp -f ~/.kube/config ~/.kube/config.bak && KUBECONFIG=~/.kube/config:/tmp/config.new kubectl config view --flatten > /tmp/config && /bin/mv -f /tmp/config ~/.kube/config

unset KUBECONFIG

```

# add worker node 
我们装好了single node，那么接下来，我们还可以给这个single node添加worker节点，让这个single node cluster变成一个单master的集群。
```bash

# first, lets stick ingress to master
oc label node master-01-demo  ocp-ingress-run="true"

oc patch ingresscontroller default -n openshift-ingress-operator --type=merge --patch='{"spec":{"nodePlacement":{"nodeSelector": {"matchLabels":{"ocp-ingress-run":"true"}}}}}'

# we are testing env, so we don't need ingress replicas.
oc patch --namespace=openshift-ingress-operator --patch='{"spec": {"replicas": 1}}' --type=merge ingresscontroller/default

oc get -n openshift-ingress-operator ingresscontroller/default -o yaml

# then we get worker's ignition file, and start worker node, add it to cluster

oc extract -n openshift-machine-api secret/worker-user-data --keys=userData --to=- > ${BASE_DIR}/data/install/worker.ign

# copy the ignition file to web server
sudo bash -c "/bin/cp -f ${BASE_DIR}/data/install/worker.ign /data/dnf ;"

# define boot parameter, and create iso
BOOT_ARG=" ip=$WORKER_01_IP::$OCP_GW:$OCP_NETMASK:$WORKER_01_HOSTNAME:$WORKER_01_INTERFACE:none nameserver=$OCP_DNS coreos.inst.install_dev=${WORKER_01_DISK##*/} coreos.inst.ignition_url=http://$HELP_SERVER:5000/worker.ign"

/bin/cp -f /data/ocp-$BUILDNUMBER/rhcos-live.x86_64.iso sno.iso

coreos-installer iso kargs modify -a "$BOOT_ARG" sno.iso

# go to kvm host ( 105 )
mkdir -p /data/kvm
scp -o Compression=no root@172.21.6.11:/home/3node/data/install/sno.iso /data/kvm/

virsh destroy ocp4-demo-gpu-worker01
virsh undefine ocp4-demo-gpu-worker01

create_lv() {
    var_vg=$1
    var_pool=$2
    var_lv=$3
    var_size=$4
    var_action=$5
    lvremove -f $var_vg/$var_lv || true
    # lvcreate -y -L $var_size -n $var_lv $var_vg
    if [ "$var_action" == "recreate" ]; then
      lvcreate --type thin -n $var_lv -V $var_size --thinpool $var_vg/$var_pool
      wipefs --all --force /dev/$var_vg/$var_lv
    fi
}

# create_lv vgdata poolA lvacmhub-worker01 500G recreate
# create_lv vgdata poolA lvacmhub-worker01-data 500G remove

wipefs --all --force /dev/nvme0n1

SNO_MEM=192

virt-install --name=ocp4-demo-gpu-worker01 \
  --vcpus=64,sockets=1,cores=64,threads=1 \
  --ram=$(($SNO_MEM*1024)) \
  --cpu=host-model \
  --cpuset 64-127 \
  --numatune 1 \
  --disk path=/dev/nvme0n1,device=disk,bus=virtio,format=raw \
  `# --disk path=/dev/vgdata/lvacmhub-data,device=disk,bus=virtio,format=raw` \
  --host-device pci_0000_81_00_0 \
  --host-device pci_0000_a1_00_0 \
  --host-device pci_0000_c1_00_0 \
  --host-device pci_0000_e1_00_0 \
  --check path_in_use=off \
  --os-variant rhel8.3 \
  --network bridge=br-int,model=virtio,mac=52:54:00:12:A1:04 \
  --graphics vnc,port=59004 \
  --noautoconsole \
  --boot menu=on --cdrom /data/kvm/sno.iso 

# after 2 boot up,
# go back to helper
oc get csr
oc get csr -ojson | jq -r '.items[] | select(.status == {} ) | .metadata.name' | xargs oc adm certificate approve

```

# setup proxy

```bash

# https://docs.openshift.com/container-platform/4.13/networking/enable-cluster-wide-proxy.html
printf -v allip '%s,' .{0..255}

cat << EOF > ${BASE_DIR}/data/install/proxy.yaml
spec:
  httpProxy: http://172.21.1.211:18801 
  httpsProxy: http://172.21.1.211:18801 
  noProxy: localhost,127.0.0.1,${allip%,},.wzhlab.top
  readinessEndpoints: []
#   - http://www.gstatic.com/generate_204
EOF

oc patch proxy/cluster --type merge --patch-file=${BASE_DIR}/data/install/proxy.yaml

# to rollback
# oc delete proxy/cluster

# oc edit proxy.config.openshift.io cluster

```

# setup htpasswd identity provider

- https://docs.openshift.com/container-platform/4.13/authentication/identity_providers/configuring-htpasswd-identity-provider.html

```bash

# init the htpasswd file
htpasswd -c -B -b ${BASE_DIR}/data/install/users.htpasswd admin redhat

# add additional user
htpasswd -B -b ${BASE_DIR}/data/install/users.htpasswd user01 redhat

# import the htpasswd file
oc create secret generic htpass-secret \
  --from-file=htpasswd=${BASE_DIR}/data/install/users.htpasswd \
  -n openshift-config 

cat << EOF > ${BASE_DIR}/data/install/oauth.yaml
spec:
  identityProviders:
  - name: htpasswd
    mappingMethod: claim 
    type: HTPasswd
    htpasswd:
      fileData:
        name: htpass-secret
EOF
oc patch oauth/cluster --type merge --patch-file=${BASE_DIR}/data/install/oauth.yaml

oc adm policy add-cluster-role-to-user cluster-admin admin

oc adm policy add-role-to-user admin user01 -n llm-demo

```

# setup local persistent volume

```bash

# create some static host path to use
# go to worker-01-demo
# 01,02,03,04,05,06,07,08,09,10
for i in $(seq -w 1 20); do
  mkdir -p "/var/wzh-local-pv/static/$i"
done
mkdir -p /var/wzh-local-pv/dynamic
chcon -Rt container_file_t /var/wzh-local-pv/

# on helper
cat << EOF > ${BASE_DIR}/data/install/local-sc.yaml
---
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-volume
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
EOF
oc create --save-config -f ${BASE_DIR}/data/install/local-sc.yaml

cat << EOF > ${BASE_DIR}/data/install/local-pv.yaml
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-01
spec:
  capacity:
    storage: 4500Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-volume
  local:
    path: /var/wzh-local-pv/static/01
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - worker-01-demo
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: local-pvc-demo
spec:
  accessModes:
  - ReadWriteOnce
  volumeMode: Filesystem 
  resources:
    requests:
      storage: 4500Gi 
  storageClassName: local-volume
EOF
oc create --save-config -n llm-demo -f ${BASE_DIR}/data/install/local-pv.yaml

# oc delete -n llm-demo -f ${BASE_DIR}/data/install/local-pv.yaml

# for all diretory, create them
var_i=0
for var_i in $(seq -w 1 20); do
  oc create --save-config -n llm-demo -f - << EOF
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-${var_i}
  labels:
    local-dir: local-dir-${var_i}
spec:
  capacity:
    storage: 4500Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-volume
  local:
    path: /var/wzh-local-pv/static/${var_i}
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - worker-01-demo
EOF
done

for var_i in $(seq -w 1 10); do
  oc create --save-config -n llm-demo -f - << EOF
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: local-pvc-${var_i}
spec:
  accessModes:
  - ReadWriteOnce
  volumeMode: Filesystem 
  resources:
    requests:
      storage: 4500Gi 
  storageClassName: local-volume
  selector:
    matchLabels:
      local-dir: local-dir-${var_i}
EOF

done

# then delete

var_i=0
for var_i in $(seq -w 1 10); do
  oc delete -n llm-demo -f - << EOF
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: local-pvc-${var_i}
spec:
  accessModes:
  - ReadWriteOnce
  volumeMode: Filesystem 
  resources:
    requests:
      storage: 4500Gi 
  storageClassName: local-volume
EOF
done

var_i=0
for var_i in $(seq -w 1 10); do
  oc delete -n llm-demo -f - << EOF
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-${var_i}
spec:
  capacity:
    storage: 4500Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-volume
  local:
    path: /var/wzh-local-pv/static/${var_i}
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - worker-01-demo
EOF
done


```

# setup hostpath provisioner

1. install OpenShift Virtualization

then: 

```bash

# create some static host path to use
# go to worker-01-demo
# 01,02,03,04,05,06,07,08,09,10
for i in $(seq -w 1 20); do
  mkdir -p "/var/wzh-local-pv/static/$i"
done
mkdir -p /var/wzh-local-pv/dynamic
chcon -Rt container_file_t /var/wzh-local-pv/

cat << EOF > ${BASE_DIR}/data/install/hostpath-provisioner.yaml
apiVersion: hostpathprovisioner.kubevirt.io/v1beta1
kind: HostPathProvisioner
metadata:
  name: hostpath-provisioner
spec:
  imagePullPolicy: IfNotPresent
  storagePools: 
  - name: wzh-local
    path: "/var/wzh-local-pv/dynamic" 
workload:
  nodeSelector:
    kubernetes.io/hostname: worker-01-demo
EOF
oc create --save-config -f ${BASE_DIR}/data/install/hostpath-provisioner.yaml

# oc delete -f ${BASE_DIR}/data/install/hostpath-provisioner.yaml


cat << EOF > ${BASE_DIR}/data/install/hostpath-storage-class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: hostpath-csi
  annotations:
    storageclass.kubernetes.io/is-default-class: 'true'
provisioner: kubevirt.io.hostpath-provisioner
reclaimPolicy: Delete 
volumeBindingMode: WaitForFirstConsumer 
parameters:
  storagePool: wzh-local
EOF
oc create --save-config -f ${BASE_DIR}/data/install/hostpath-storage-class.yaml

# oc delete -f ${BASE_DIR}/data/install/hostpath-storage-class.yaml

```

# setup storage for registry

- https://docs.openshift.com/container-platform/4.13/registry/configuring_registry_storage/configuring-registry-storage-baremetal.html#registry-change-management-state_configuring-registry-storage-baremetal
  

```bash

cat << EOF > ${BASE_DIR}/data/install/registry.yaml
spec:
  managementState: Managed
  replicas: 1
  storage:
    pvc:
      claim:
EOF
oc patch config.imageregistry.operator.openshift.io/cluster \
  --type=merge \
  --patch-file=${BASE_DIR}/data/install/registry.yaml

oc get config.imageregistry.operator.openshift.io/cluster -o json | jq .spec

```

虽然我们设置了image registry 副本数为1，但是由于一些问题，operator并不能顺利的减少pod数量，需要我们找到deployment, 然后手动降低为0，之后operator就能顺利应用设置了。

# create cert for https 

- https://letsencrypt.org/getting-started/

```bash
su - 3node

rm -rf ${BASE_DIR}/data/certbot
mkdir -p ${BASE_DIR}/data/certbot/{etc,lib}

podman run -it --rm --name certbot \
            -v "${BASE_DIR}/data/certbot/etc:/etc/letsencrypt":Z \
            -v "${BASE_DIR}/data/certbot/lib:/var/lib/letsencrypt":Z \
            docker.io/certbot/certbot certonly  -d "*.apps.demo-gpu.wzhlab.top" --manual --preferred-challenges dns-01  --server https://acme-v02.api.letsencrypt.org/directory

# ......
# Successfully received certificate.
# Certificate is saved at: /etc/letsencrypt/live/apps.demo-gpu.wzhlab.top/fullchain.pem
# Key is saved at:         /etc/letsencrypt/live/apps.demo-gpu.wzhlab.top/privkey.pem
# This certificate expires on 2024-05-03.
# These files will be updated when the certificate renews.
# ......

cd ${BASE_DIR}/data/certbot
cp ./etc/archive/*/fullchain1.pem wzhlab.top.crt
# cp ./etc/archive/*/cert1.pem wzhlab.top.crt
cp ./etc/archive/*/privkey1.pem wzhlab.top.key

# for ods knative service key
# 应该是 istio
oc create secret tls knative-serving-cert \
     --cert=wzhlab.top.crt \
     --key=wzhlab.top.key \
     -n istio-system

oc create secret tls wzh-ods-key \
     --cert=wzhlab.top.crt \
     --key=wzhlab.top.key \
     -n istio-system

# oc delete secret knative-serving-cert -n istio-system

# oc create secret tls wzh-ods-key \
#      --cert=wzhlab.top.crt \
#      --key=wzhlab.top.key \
#      -n knative-serving


# oc create secret tls wzh-ods-key \
#      --cert=wzhlab.top.crt \
#      --key=wzhlab.top.key \
#      -n  redhat-ods-operator

# for ods, change DataScienceCluster / rhods
oc get DataScienceCluster
# NAME    AGE
# rhods   31h

oc get DataScienceCluster/rhods -o json | jq .spec.components.kserve
# {
#   "devFlags": {},
#   "managementState": "Managed",
#   "serving": {
#     "ingressGateway": {
#       "certificate": {
#         "secretName": "knative-serving-cert",
#         "type": "SelfSigned"
#       }
#     },
#     "managementState": "Managed",
#     "name": "knative-serving"
#   }
# }

# change 
# secretName -> secretName: wzh-ods-key
# type       -> Provided 

```

## for router

```bash
oc create secret tls wzh-ingress-key \
     --cert=wzhlab.top.crt \
     --key=wzhlab.top.key \
     -n openshift-ingress

oc patch ingresscontroller.operator default \
     --type=merge -p \
     '{"spec":{"defaultCertificate": {"name": "wzh-ingress-key"}}}' \
     -n openshift-ingress-operator

```

# show logs for all pod under ns

```bash



```

# clean up finalizer

```bash

oc get kfdef -A -o=jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.metadata.namespace}{"\n"}{end}'
# rhods-nbc       redhat-ods-applications
# modelmesh-monitoring    redhat-ods-monitoring
# monitoring      redhat-ods-monitoring
# rhods-notebooks rhods-notebooks


oc get kfdef -A -o=jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.metadata.namespace}{"\n"}{end}' | xargs -n 2 bash -c 'oc patch kfdef $0 -n $1 -p "{\"metadata\":{\"finalizers\":null}}" --type=merge'

```

# end