> [!CAUTION] WIP
# rhdh for book

Red Hat has published a book on how to use RHDF, which provides a detailed description of the complete project workflow and the associated software template. You can get the book from the following link:

- https://github.com/developer-hub-books

You can also get the software template from the following link:

- https://github.com/developer-hub-books/rhdh-book1-templates

Based on the requirements of the local experimental environment and service projects, I have modified the corresponding software code. The links to the relevant software are as follows:

- https://github.com/nepdemo/rhdh-book1-templates/tree/wzh

We primarily focus on building the foundational platform of OpenShift and configuring the related experimental environment. Additionally, we have introduced a new component, JFrog, to demonstrate the extensibility of RHDH.

The following is the system architecture used for this document and the workflow between its various components:

![](dia/rhdh.book.arch.drawio.png)

# techdoc

```bash



```

# end

# gitlab tips abandoned

- https://www.redhat.com/en/blog/install-the-gitlab-operator-on-openshift

![](imgs/2024.08.rhdh.book.md/2024-08-12-19-56-49.png)

```bash
# it will installed in gitlab-system namespace.

cat << EOF > ${BASE_DIR}/data/install/gitlab-scc.yaml
apiVersion: security.openshift.io/v1
kind: SecurityContextConstraints
metadata:
  name: gitlab-nginx-ingress-scc
priority: null

# Host-related settings
allowHostDirVolumePlugin: false  # Disallow host directory volume plugins
allowHostIPC: false                # Disallow IPC namespace sharing with the host
allowHostNetwork: false            # Disallow host network sharing
allowHostPID: false                # Disallow host PID namespace sharing
allowHostPorts: false              # Disallow host port usage

# Privilege settings
allowPrivilegeEscalation: true     # Allow privilege escalation
allowPrivilegedContainer: false     # Disallow privileged containers

# Capabilities configuration
allowedCapabilities: null           # No additional capabilities allowed
defaultAddCapabilities:             # Default capabilities added
  - NET_BIND_SERVICE                # Allow binding to privileged ports

# File system group settings
fsGroup:
  type: MustRunAs                   # fsGroup must be set

# User and SELinux settings
runAsUser:
  type: MustRunAs                   # User must run as specified UID
  uid: 101                          # User ID (UID) to run as
seLinuxContext:
  type: MustRunAs                   # SELinux context must be set

# Additional configuration
groups: []                          # No specific groups required
readOnlyRootFilesystem: false       # Root filesystem is not read-only
requiredDropCapabilities:           # Capabilities that must be dropped
  - ALL                             # Drop all capabilities
EOF

oc create -f ${BASE_DIR}/data/install/gitlab-scc.yaml


cat << EOF > ${BASE_DIR}/data/install/gitlab-values.yaml
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  # Ensure this value matches 'spec.chart.values.global.ingress.class'
  # in the GitLab CR on the next step.
  name: gitlab-nginx
spec:
  controller: k8s.io/ingress-nginx
EOF

oc create -f ${BASE_DIR}/data/install/gitlab-values.yaml

```

![](imgs/2024.08.rhdh.book.md/2024-08-12-18-19-13.png)

![](imgs/2024.08.rhdh.book.md/2024-08-12-20-00-46.png)

<!-- ![](imgs/2024.08.rhdh.book.md/2024-08-12-20-22-49.png) -->
<img src="imgs/2024.08.rhdh.book.md/2024-08-12-20-22-49.png" style="height:400px;">


```bash
oc -n gitlab-system get ing
# NAME                        CLASS          HOSTS                                               ADDRESS   PORTS     AGE
# gitlab-kas                  gitlab-nginx   kas.gitlab-wzh.apps.demo-01-rhsys.wzhlab.top                  80, 443   3m34s
# gitlab-minio                gitlab-nginx   minio.gitlab-wzh.apps.demo-01-rhsys.wzhlab.top                80, 443   4m5s
# gitlab-registry             gitlab-nginx   registry.gitlab-wzh.apps.demo-01-rhsys.wzhlab.top             80, 443   3m34s
# gitlab-webservice-default   gitlab-nginx   gitlab.gitlab-wzh.apps.demo-01-rhsys.wzhlab.top               80, 443   3m33s
```

open browser with: 
`https://gitlab.gitlab-wzh.apps.demo-01-rhsys.wzhlab.top`


# gitops tips abandoned

How to deal with gitlab self-cert
- https://argo-cd.readthedocs.io/en/stable/user-guide/private-repositories/#self-signed-untrusted-tls-certificates

```bash

# get ingress/route url for argocd, using openshift oc command
ARGOCD_NS="demo-gitops"
ARGOCD_INSTANCE_NAME="argocd"
ARGOCD_URL=`oc get route $ARGOCD_INSTANCE_NAME -n $ARGOCD_NS -o jsonpath='{.spec.host}'`
echo $ARGOCD_URL

# get the cli
wget --no-check-certificate -O ~/.local/bin/argocd https://$ARGOCD_URL/download/argocd-linux-amd64
chmod +x ~/.local/bin/argocd

```

# book lab

```bash

oc logs -n demo-rhdh deployment/redhat-developer-hub | grep -i argocd

oc logs -n demo-gitops deployment/argocd-server

oc logs -n demo-gitops deployment/argocd-repo-server

# https://github.com/tektoncd/catalog/issues/177

oc adm policy who-can use clusterrole/system:image-builder
# Warning: the server doesn't have a resource type 'clusterrole/system:image-builder'
# resourceaccessreviewresponse.authorization.openshift.io/<unknown>

# Namespace: demo-gitops
# Verb:      use
# Resource:  clusterrole/system:image-builder

# Users:  admin
#         system:admin
#         system:serviceaccount:openshift-apiserver-operator:openshift-apiserver-operator
#         system:serviceaccount:openshift-apiserver:openshift-apiserver-sa
#         system:serviceaccount:openshift-authentication-operator:authentication-operator
#         system:serviceaccount:openshift-authentication:oauth-openshift
#         system:serviceaccount:openshift-cluster-storage-operator:cluster-storage-operator
#         system:serviceaccount:openshift-cluster-version:default
#         system:serviceaccount:openshift-config-operator:openshift-config-operator
#         system:serviceaccount:openshift-controller-manager-operator:openshift-controller-manager-operator
#         system:serviceaccount:openshift-etcd-operator:etcd-operator
#         system:serviceaccount:openshift-etcd:installer-sa
#         system:serviceaccount:openshift-kube-apiserver-operator:kube-apiserver-operator
#         system:serviceaccount:openshift-kube-apiserver:installer-sa
#         system:serviceaccount:openshift-kube-apiserver:localhost-recovery-client
#         system:serviceaccount:openshift-kube-controller-manager-operator:kube-controller-manager-operator
#         system:serviceaccount:openshift-kube-controller-manager:installer-sa
#         system:serviceaccount:openshift-kube-controller-manager:localhost-recovery-client
#         system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator
#         system:serviceaccount:openshift-kube-scheduler:installer-sa
#         system:serviceaccount:openshift-kube-scheduler:localhost-recovery-client
#         system:serviceaccount:openshift-kube-storage-version-migrator-operator:kube-storage-version-migrator-operator
#         system:serviceaccount:openshift-kube-storage-version-migrator:kube-storage-version-migrator-sa
#         system:serviceaccount:openshift-machine-config-operator:machine-config-operator
#         system:serviceaccount:openshift-network-operator:cluster-network-operator
#         system:serviceaccount:openshift-oauth-apiserver:oauth-apiserver-sa
#         system:serviceaccount:openshift-service-ca-operator:service-ca-operator
# Groups: system:cluster-admins
#         system:masters


# oc policy add-role-to-user system:image-builder system:serviceaccount:pushed:pusher

oc policy add-role-to-user system:image-builder system:serviceaccount:demo-play:pipeline

apiVersion: v1
kind: Pod
metadata:
  name: ocp-cli-pod
  namespace: demo-play
spec:
  serviceAccountName: builder
  containers:
  - name: ocp-cli-container
    image: quay.io/wangzheng422/qimgs:rocky9-test-2024.06.17.v01
    command: ["/bin/sh"]
    args: ["-c", "sleep infinity"]
    securityContext:
      privileged: true
    volumeMounts:
    - name: dockercfg
      mountPath: /root/wzh-docker
  volumes:
  - name: dockercfg
    secret:
      secretName: builder-dockercfg-7d2h9
  restartPolicy: Never

podman pull docker.io/busybox

podman pull --tls-verify=false --authfile /root/wzh-docker/.dockercfg  image-registry.openshift-image-registry.svc:5000/openshift/cli

podman image tag docker.io/busybox  image-registry.openshift-image-registry.svc:5000/demo-play/demo-01:latest

podman login --tls-verify=false --authfile /root/wzh-docker/.dockercfg image-registry.openshift-image-registry.svc:5000

podman login --tls-verify=false --authfile auth image-registry.openshift-image-registry.svc:5000


podman image push --tls-verify=false --authfile /root/wzh-docker/.dockercfg image-registry.openshift-image-registry.svc:5000/demo-play/demo-01:latest


oc policy add-role-to-group \
    system:image-puller system:serviceaccounts:demo-play \
    --namespace=openshift

oc policy add-role-to-user \
    system:image-puller system:serviceaccount:demo-play:builder \
    --namespace=openshift



从 Kubernetes dockercfg Secret 转换为 Podman Authfile

import json

def k8s_to_podman(k8s_file, podman_file):
    with open(k8s_file, 'r') as f:
        k8s_data = json.load(f)
    
    podman_data = {"auths": {}}
    for registry, creds in k8s_data.items():
        podman_data["auths"][registry] = {"auth": creds["auth"]}
    
    with open(podman_file, 'w') as f:
        json.dump(podman_data, f, indent=2)

# 使用示例
k8s_to_podman('k8s_dockercfg.json', 'podman_authfile.json')


从 Podman Authfile 转换为 Kubernetes dockercfg Secret

import json
import base64

def podman_to_k8s(podman_file, k8s_file):
    with open(podman_file, 'r') as f:
        podman_data = json.load(f)
    
    k8s_data = {}
    for registry, creds in podman_data["auths"].items():
        decoded_auth = base64.b64decode(creds["auth"]).decode('utf-8')
        username, password = decoded_auth.split(':')
        k8s_data[registry] = {
            "username": username,
            "password": password,
            "email": "user@example.com",  # 你可以根据需要修改这个字段
            "auth": creds["auth"]
        }
    
    with open(k8s_file, 'w') as f:
        json.dump(k8s_data, f, indent=2)

# 使用示例
podman_to_k8s('podman_authfile.json', 'k8s_dockercfg.json')


app.kubernetes.io/instance: sample


# This command is used to retrieve the logs of a specific application instance in the `demo-play` namespace. It filters the logs based on the label `app.kubernetes.io/instance=sample`.
oc logs -l app.kubernetes.io/instance=sample -n demo-play


```


# install jfrog

## on rhel using docker

- https://jfrog.com/help/r/jfrog-installation-setup-documentation/install-artifactory-single-node-with-docker-compose
- https://jfrog.com/community/download-artifactory-oss/

```bash

mkdir -p /data/jfrog/install

cd /data/jfrog

tar -xvf jfrog-artifactory-oss-7.90.8-compose.tar.gz

cd artifactory-oss-7.90.8 

bash ./config.sh
# ...
# Docker setup complete

# Installation directory: [/data/jfrog/install/] contains data and configurations.

# Use docker-compose commands to start the application. Once the application has started, it can be accessed at [http://fe80::250:56ff:fe8e:c1e4%ens192:8082]

# Examples:
# cd /data/jfrog/artifactory-pro-7.90.8


# start postgresql:    docker compose -p rt-postgres -f docker-compose-postgres.yaml up -d
# stop  postgresql:    docker compose -p rt-postgres -f docker-compose-postgres.yaml down
# start:               docker compose -p rt up -d
# stop:                docker compose -p rt down

# NOTE: The compose file uses several environment variables from the .env file. Remember to run from within the [/data/jfrog/artifactory-pro-7.90.8] folder

# Done




```

## on ocp using operator

```bash

oc adm policy add-scc-to-user anyuid -z artifactory-ha-operator -n openshift-operators

# oc adm policy remove-scc-from-user anyuid -z artifactory-ha-operator -n openshift-operators
```

## on rhel

```bash

# disable ipv6 totally
cat << EOF >> /etc/sysctl.d/99-sysctl.conf

net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.ens192.disable_ipv6 = 1
EOF

sudo sysctl -p

sudo systemctl restart NetworkManager

nmcli connection modify ens192 ipv6.method "ignore"
nmcli con up ens192

# begin the install
mkdir -p /data/jfrog/install

cd /data/jfrog

wget https://releases.jfrog.io/artifactory/artifactory-rpms/artifactory-rpms.repo -O jfrog-artifactory-rpms.repo;

sudo mv jfrog-artifactory-rpms.repo /etc/yum.repos.d/;

# sudo yum -y update

sudo yum install -y jfrog-artifactory-oss

# ...
# Installation directory was set to /opt/jfrog/artifactory
# You can find more information in the log directory /opt/jfrog/artifactory/var/log 
# System configuration templates can be found under /opt/jfrog/artifactory/var/etc
# Copy any configuration you want to modify from the template to /opt/jfrog/artifactory/var/etc/system.yaml

sed -i 's/#   allowNonPostgresql: false/  allowNonPostgresql: true/' /opt/jfrog/artifactory/var/etc/system.yaml 

systemctl restart artifactory.service

systemctl stop artifactory.service

systemctl status artifactory.service

# yum remove -y jfrog-artifactory-oss

# /bin/rm -rf /opt/jfrog/artifactory


# /bin/rm -f /opt/jfrog/artifactory/var/etc/security/master.key
# /bin/rm -f /opt/jfrog/artifactory/var/etc/security/join.key

# openssl rand -hex 32 > /opt/jfrog/artifactory/var/etc/security/master.key
# openssl rand -hex 32 > /opt/jfrog/artifactory/var/etc/security/join.key

cd /opt/jfrog/artifactory/var/log

# goto webui
# http://192.168.50.17:8081/ui/
# user: admin
# passwd: password


```

## rhel jcr

- https://jfrog.com/download-jfrog-container-registry/
- https://jfrog.com/help/r/jfrog-artifactory-documentation/set-up-jfrog-container-registry

```bash

wget https://releases.jfrog.io/artifactory/artifactory-rpms/artifactory-rpms.repo -O jfrog-artifactory-rpms.repo;
sudo mv jfrog-artifactory-rpms.repo /etc/yum.repos.d/;
sudo yum update -y
sudo yum install -y jfrog-artifactory-jcr

# active build-in memory db
sed -i 's/#   allowNonPostgresql: false/  allowNonPostgresql: true/' /opt/jfrog/artifactory/var/etc/system.yaml 

systemctl restart artifactory.service

# systemctl stop artifactory.service

systemctl status artifactory.service

# yum remove -y jfrog-artifactory-jcr

# /bin/rm -rf /opt/jfrog/artifactory
# /bin/rm -rf /var/opt/jfrog

cd /opt/jfrog/artifactory/var/log
tail -f /opt/jfrog/artifactory/var/log/console.log
# ...
# ###############################################################
# ###   All services started successfully in 53.258 seconds   ###
# ###############################################################
# 2024-08-26T02:55:55.750Z [jfac ] [INFO ] [30e54f2f793096ff] [CertificateFileHandlerBase:167] [c-default-executor-0] - Loading ca certificate from database.
# 2024-08-26T02:55:55.898Z [jfac ] [INFO ] [30e54f2f793096ff] [CertificateFileHandlerBase:200] [c-default-executor-0] - [ACCESS BOOTSTRAP] Saved new ca certificate at: /opt/jfrog/artifactory/var/etc/access/keys/ca.crt
# 2024-08-26T02:55:55.898Z [jfac ] [INFO ] [30e54f2f793096ff] [CertificateFileHandlerBase:167] [c-default-executor-0] - Loading root certificate from database.
# 2024-08-26T02:55:55.987Z [jfac ] [INFO ] [30e54f2f793096ff] [CertificateFileHandlerBase:200] [c-default-executor-0] - [ACCESS BOOTSTRAP] Saved new root certificate at: /opt/jfrog/artifactory/var/etc/access/keys/root.crt
# 2024-08-26T02:55:58.714Z [jfrou] [INFO ] [2ba450275ed96969] [external_topology.go:272      ] [main                ] [] - External Topology changed: dify state changed (jfac@01j669r1vbmrhn1rx61rm213nz HEALTHY,jffe@01j669r1vbmrhn1rx61rm213nz HEALTHY,jfmd@01j669r08xadf1y7mrzh7296vw HEALTHY,jfob@01j669r75rr6211mt43gq0091t HEALTHY,jfrt@01j669r75rr6211mt43gq0091t HEALTHY)
# 2024-08-26T02:55:58.717Z [jfrou] [INFO ] [2ba450275ed96969] [routing_handler.go:410        ] [main                ] [] - External topology is changed, updating traefik external routes
# 2024-08-26T02:56:01.951Z [jfrt ] [INFO ] [33a70b18e4758697] [.e.EventsLogCleanUpService:111] [e4758697|art-exec-22] - Starting cleanup of old events from event log
# 2024-08-26T02:56:01.958Z [jfrt ] [INFO ] [33a70b18e4758697] [.e.EventsLogCleanUpService:117] [e4758697|art-exec-22] - deleteFrom=1724640931725
# 2024-08-26T02:56:01.959Z [jfrt ] [INFO ] [33a70b18e4758697] [.e.EventsLogCleanUpService:124] [e4758697|art-exec-22] - eventsDeleteInterval=14400000
# 2024-08-26T02:56:02.010Z [jfrt ] [INFO ] [33a70b18e4758697] [.e.EventsLogCleanUpService:127] [e4758697|art-exec-22] - maxBoundary.timestamp=1724637331741
# 2024-08-26T02:56:02.010Z [jfrt ] [INFO ] [33a70b18e4758697] [.e.EventsLogCleanUpService:128] [e4758697|art-exec-22] - maxBoundary.deleteErrors=false
# 2024-08-26T02:56:02.010Z [jfrt ] [INFO ] [33a70b18e4758697] [.e.EventsLogCleanUpService:155] [e4758697|art-exec-22] - Cleanup of old events from event log finished
# 2024-08-26T02:56:02.014Z [jfrt ] [INFO ] [33a70b18e4758697] [onStatusStorageServiceImpl:111] [onitor-migration-job] - Inserting new migration status record 'MigrationStatus(identifier=event-table-repo-key-migration, started=1724640962014, finished=0, migrationInfoBlob=MarkerMigrationInfoBlob(serverId=))'
# 2024-08-26T02:56:02.035Z [jfrt ] [INFO ] [33a70b18e4758697] [entTableRepoKeyMigrationDao:86] [onitor-migration-job] - start execute events table repo key migration in the first time
# 2024-08-26T02:56:02.043Z [jfrt ] [INFO ] [33a70b18e4758697] [ntTableRepoKeyMigrationDao:134] [onitor-migration-job] - finish to add  index on node_events
# 2024-08-26T02:56:07.055Z [jfrt ] [INFO ] [33a70b18e4758697] [onStatusStorageServiceImpl:124] [onitor-migration-job] - Migration for 'event-table-repo-key-migration' has finished.
# 2024-08-26T02:56:07.061Z [jfrt ] [INFO ] [33a70b18e4758697] [ntTableRepoKeyMigrationDao:185] [onitor-migration-job] - successfully finished execute events table repo key migration


# goto webui
# http://192.168.50.17:8082/ui/
# user: admin
# passwd: password

# to accept the eula
curl -XPOST -vu admin:password http://192.168.50.17:8082/artifactory/ui/jcr/eula/accept

cat << EOF > /etc/containers/registries.conf.d/99-wzh.conf
[[registry]]
location = "192.168.50.17:8082"
insecure = true
EOF

podman login 192.168.50.17:8082/demo-01

podman tag docker.io/library/busybox 192.168.50.17:8082/demo-01/busybox:v1

podman push 192.168.50.17:8082/demo-01/busybox:v1

# for ocp, add inscure registry for jfrog
cat << EOF > ${BASE_DIR}/data/install/ocp-registry.yaml
spec:
  registrySources: 
    # this will write to /etc/containers/policy.json
    allowedRegistries:
    - docker.io
    - quay.io
    - registry.redhat.io
    - image-registry.openshift-image-registry.svc:5000
    - gcr.io
    - k8s.gcr.io
    - nvcr.io
    - registry.access.redhat.com
    - registry.connect.redhat.com
    - 192.168.50.17:8082
    - ghcr.io
    # this will write to /etc/containers/registries.conf
    insecureRegistries:
    - 192.168.50.17:8082
EOF

oc patch image.config.openshift.io/cluster --patch "$(cat ${BASE_DIR}/data/install/ocp-registry.yaml)" --type=merge


# add pull secret for the jfrog registry
# https://docs.openshift.com/container-platform/4.15/openshift_images/managing_images/using-image-pull-secrets.html#images-update-global-pull-secret_using-image-pull-secrets
oc registry login --registry="192.168.50.17:8082" \
  --auth-basic="admin:password" \
  --to=${BASE_DIR}/data/install/internal-registry-secret.json

# merge with existed
jq --argjson newAuth "`cat ${BASE_DIR}/data/install/internal-registry-secret.json`" '.auths += $newAuth.auths' "${BASE_DIR}/data/pull-secret.json" | jq -c . > "${BASE_DIR}/data/pull-secret.json.tmp"

oc set data secret/pull-secret -n openshift-config \
  --from-file=.dockerconfigjson=${BASE_DIR}/data/pull-secret.json.tmp

```

![](imgs/2024.08.rhdh.book.md/2024-08-26-12-20-08.png)

![](imgs/2024.08.rhdh.book.md/2024-08-26-12-52-06.png)

![](imgs/2024.08.rhdh.book.md/2024-08-26-21-33-51.png)


```bash

# jfrog using graphql to query metadata
# https://jfrog.com/help/r/jfrog-rest-apis/example-1-package-by-name-and-type
cat << EOF > /tmp/demo.json
{
  "query": "query { packages(filter: { name: \"*\", packageTypeIn: [NPM, DOCKER] }, first: 3, orderBy: { field: NAME, direction: DESC }) { edges { node { name description } } } }"
}
EOF

curl -H "Authorization: Bearer cmVmdGtuOjAxOjE3NTYxODE5Mjc6RUtVOGNFSHpuamsweUh6aXY4TFhqQzRtcHBh" -XPOST http://192.168.50.17:8082/metadata/api/v1/query -d @/tmp/demo.json

```

## on ocp

not working

```bash

# https://jfrog.com/help/r/jfrog-installation-setup-documentation/install-artifactory-single-node-on-openshift

export JFROG_NS="demo-jfrog"

oc new-project $JFROG_NS

helm repo add jfrog https://charts.jfrog.io -n $JFROG_NS

helm repo update -n $JFROG_NS

# Create a key
export MASTER_KEY=$(openssl rand -hex 32)
echo ${MASTER_KEY}

# Create a secret containing the key. The key in the secret must be named master-key
kubectl create secret generic my-masterkey-secret -n $JFROG_NS --from-literal=master-key=${MASTER_KEY}

# Create a key
export JOIN_KEY=$(openssl rand -hex 32)
echo ${JOIN_KEY}

# Create a secret containing the key. The key in the secret must be named join-key
kubectl create secret generic my-joinkey-secret -n $JFROG_NS --from-literal=join-key=${JOIN_KEY}

cat << EOF > ${BASE_DIR}/data/install/jfrog-values.yaml
artifactory:
  containerSecurityContext:
    enabled: false
  artifactory:
    podSecurityContext : false
  postgresql:
    securityContext:
      enabled: false
    containerSecurityContext:
      enabled: false
    podSecurityContext:
      enabled: false
  nginx:
    podSecurityContext:
      enabled: false
    containerSecurityContext:
      enabled: false
EOF

cd ${BASE_DIR}/data/install/

# do not use these in production system,
# only use in test environment
oc adm policy add-scc-to-user anyuid -z default -n $JFROG_NS
oc adm policy add-scc-to-user privileged -z default -n $JFROG_NS

helm upgrade --install artifactory --set artifactory.masterKey=${MASTER_KEY} --set artifactory.joinKey=${JOIN_KEY} --namespace $JFROG_NS jfrog/artifactory -f jfrog-values.yaml

Release "artifactory" does not exist. Installing it now.
W0822 14:12:10.747134  170436 warnings.go:70] spec.template.spec.containers[7].ports[0]: duplicate port definition with spec.template.spec.containers[0].ports[0]
NAME: artifactory
LAST DEPLOYED: Thu Aug 22 14:12:09 2024
NAMESPACE: demo-jfrog
STATUS: deployed
REVISION: 1
TEST SUITE: None

NOTES:
Congratulations. You have just deployed JFrog Artifactory!

1. Get the Artifactory URL by running these commands:

   NOTE: It may take a few minutes for the LoadBalancer IP to be available.
         You can watch the status of the service by running 'kubectl get svc --namespace demo-jfrog -w artifactory-artifactory-nginx'
   export SERVICE_IP=$(kubectl get svc --namespace demo-jfrog artifactory-artifactory-nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
   echo http://$SERVICE_IP/

2. Open Artifactory in your browser
   Default credential for Artifactory:
   user: admin
   password: password


```


# end