# openshift 4.11 acm with ztp disconnected static-ip, auto

<!-- 本文介绍，在openshift4.11上，装 multicluster engine for Kubernetes (MCE) 组件以后，如何通过zero touch provision的方式，来部署一个单节点openshift4.10的集群（SNO），在部署的过程中，我们模拟离线的网络环境，并且禁止DHCP，只用静态IP。 -->
This document describes how to deploy a single-node OpenShift 4.10 cluster (SNO) using zero-touch provisioning after installing the Multicluster Engine for Kubernetes (MCE) component on OpenShift 4.11. During the deployment process, we simulate an offline network environment, disable DHCP, and use only static IPs. 

<!-- ZTP(zero touch provision)模式之所以诱人，是因为他只需要baremetal的bmc信息，以及网卡的mac地址，就可以完成集群的部署。MCE会创建一个iso，并调用bmc的api，去挂载这个iso并启动。 -->
ZTP (zero touch provisioning) mode is appealing because it only needs the bare metal BMC information and the network card's MAC address to complete cluster deployment. MCE creates an ISO, calls the BMC's API to mount the ISO, and boots from it. 

<!-- 本次实验，使用一个全自动流程，就是让MCE创建iso，MCE会调用redfish用iso启动kvm。整个流程如下：
1. 在openshift4上安装MCE组件
2. 在MCE上配置cluster, infra env等配置。
3. MCE通过网络 redfish 协议启动kvm
4. kvm自动开始集群安装，但是由于kvm+redfish的限制，安装过程中的重启，需要手动停止kvm，配置由硬盘启动，然后再手动启动kvm。
5. 集群安装完成，保存集群登录信息 -->
This experiment utilizes a fully automated process: MCE creates an ISO, then invokes Redfish to boot a KVM using said ISO. The entire process is as follows:
1. Install MCE components on OpenShift 4.
2. Configure cluster and infrastructure environment settings within MCE.
3. MCE initiates the KVM via the Redfish protocol over the network.
4. The KVM automatically begins cluster installation. However, due to limitations with KVM and Redfish, reboots during installation require manually stopping the KVM, configuring it to boot from disk, and then manually restarting it.
5. Once cluster installation is complete, save the cluster login information. 

<!-- 本次实验的部署架构图： -->
Deployment architecture diagram of this experiment: 

![](../4.10/dia/4.10.bm.ipi.sno.static.ip.drawio.svg)

<!-- 本次实验的网络架构，和服务器,kvm部属架构，是依托之前的一个未完成的实验，[工厂模式](../4.10/4.10.factory.md)，虽然工厂模式实验的网络模型比较复杂，但是我们就不重复配置环境了。 -->
The network architecture and server/KVM deployment architecture for this experiment are based on a previous unfinished experiment on the [factory pattern](../4.10/4.10.factory.md). Although the network model of the factory pattern experiment is relatively complex, we will not repeat the environment configuration. 

<!-- 参考资料： -->
References: 
- https://github.com/jparrill/ztp-the-hard-way/blob/main/docs/connected-ZTP-flow-hub-deployment.md
- https://github.com/jparrill/ztp-the-hard-way/blob/main/docs/disconnected-ZTP-flow-hub-deployment.md

<!-- 视频讲解

[<kbd><img src="../4.10/imgs/20220412112651.png" width="600"></kbd>](https://www.bilibili.com/video/bv1F3411n7tT)

- [bilibili](https://www.bilibili.com/video/bv1F3411n7tT)
- [youtube](https://youtu.be/tX2iozE2Rn0) -->

<!-- # 静态变量和 kvm 配置 -->
# Static Variable and KVM Configuration

<!-- 根据 factory 的安装过程，我们弄了一个 3 node IPI 模式安装的 openshift， 是一个 ipi 的 compact cluster. -->
Following factory's installation process, we set up a 3-node OpenShift cluster using IPI mode. It is an IPI compact cluster. 

<!-- 以下的参数，是我们用这个装了MCE的集群后，通过ZTP创建出来的新集群的参数，新集群只有1个节点。 -->
The following parameters belong to a new single-node cluster that we created via ZTP after we deployed an MCE to this cluster. 

```bash
# on helper

# 做一些配置参数定义
# Define some configuration parameters. 
INSTALL_IMAGE_REGISTRY=quaylab.infra.wzhlab.top:8443
# PULL_SECRET='{"auths":{"registry.redhat.io": {"auth": "ZHVtbXk6ZHVtbXk=","email": "noemail@localhost"},"registry.ocp4.redhat.ren:5443": {"auth": "ZHVtbXk6ZHVtbXk=","email": "noemail@localhost"},"'${INSTALL_IMAGE_REGISTRY}'": {"auth": "'$( echo -n 'admin:redhatadmin' | openssl base64 )'","email": "noemail@localhost"}}}'
PULL_SECRET=$(cat /data/pull-secret.json)

ACM_DEMO_CLUSTER=edge01

SNO_BASE_DOMAIN=wzhlab.top
SNO_IP=192.168.12.33
SNO_GW=192.168.12.1
SNO_NETMAST=255.255.255.0
SNO_NETMAST_S=24
SNO_HOSTNAME=edge-master-01
SNO_IF=enp1s0
SNO_IF_MAC=52:54:00:20:a2:01
SNO_DNS=192.168.77.11
SNO_DISK=/dev/vda
SNO_CORE_PWD=redhat

```

<!-- # 部署MCE -->
# Deploy MCE 

<!-- 接下来，我们就部署MCE，我们用最简单的部署模式。 -->
Next, let's deploy MCE. We'll use the simplest deployment mode. 

```bash
# install operator Advanced Cluster Management for Kubernetes

# https://docs.openshift.com/container-platform/4.9/scalability_and_performance/ztp-deploying-disconnected.html#enabling-assisted-installer-service-on-bare-metal_ztp-deploying-disconnected
# https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.4/html/install/installing#installing-from-the-cli


cat << EOF > ${BASE_DIR}/data/install/acm.subscript.ns.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: multicluster-engine
EOF
oc create -f ${BASE_DIR}/data/install/acm.subscript.ns.yaml

cat << EOF > ${BASE_DIR}/data/install/acm.subscript.yaml
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name:  multicluster-engine-wzh
  namespace: multicluster-engine
spec:
  targetNamespaces:
    - multicluster-engine
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: multicluster-engine
  namespace: multicluster-engine
spec:
  sourceNamespace: openshift-marketplace
  source: redhat-operators
  channel: stable-2.1
  installPlanApproval: Automatic
  name: multicluster-engine
EOF
oc create -f ${BASE_DIR}/data/install/acm.subscript.yaml

# RHACM create the MultiClusterHub resource

cat << EOF > ${BASE_DIR}/data/install/acm.mch.mch.yaml
kind: MultiClusterEngine
apiVersion: multicluster.openshift.io/v1
metadata:
  name: multiclusterengine
  namespace: multicluster-engine
spec: {}
EOF
oc create -f ${BASE_DIR}/data/install/acm.mch.mch.yaml

```

<!-- 装好了是这样： -->
It looks like this after installation: 

![](../4.11/imgs/2023-01-13-17-52-41.png)

<!-- 我们可以通过webUI访问MCE：  -->
We can access MCE through the web UI. 

https://multicloud-console.apps.acm-demo-hub.redhat.ren/overview

![](../4.11/imgs/2023-01-13-17-53-56.png)

<!-- 我们可以看到MCE里面什么都是空的： -->
We can see that everything inside MCE is empty. 

![](../4.11/imgs/2023-01-13-17-55-22.png)

<!-- # 用ZTP模式部署一个SNO -->
# Deploy an SNO in ZTP mode. 

<!-- 有过部署assisted install service，并通过AIS来部署SNO的经验，那么通过MCE，用ZTP的模式来部署，就容易理解了，整个过程一样，都是配置MCE里面的assisted install service，然后创建一个iso出来，调用BMC API，来直接挂载iso，并启动主机。 -->
If you have experience deploying the assisted install service and using AIS to deploy SNO, then using MCE to deploy in ZTP mode will be easy to understand. The entire process is the same: configure the assisted install service in MCE, create an ISO, call the BMC API to directly mount the ISO, and boot the host. 

<!-- ## 命令行配置新集群 -->
## Command line configuration of a new cluster 

<!-- MCE 2.4 UI 是完全支持ZTP的，但是，我们现在的实验，是为了项目上能定制，所以有些配置要用命令行完成。 -->
The MCE 2.4 UI fully supports ZTP. However, our current experiment requires some configurations to be done via command line for project customization purposes. 

<!-- 红帽官方文档上，还有另外一个做法，通过定义配置文件，同时使用工具，创建本实验里面手动创建的各种yaml。 -->
Red Hat's official documentation offers an alternative approach: define configuration files and utilize tools to generate the various YAML files created manually in this experiment. 

<!-- 本文就是手动创建yaml，然后一步一步的做，更深入的理解以下ZTP的过程。 -->
This article focuses on manually creating YAML files and executing each step meticulously to gain a deeper understanding of the Zero Touch Provisioning (ZTP) process. 

```bash

# oc project open-cluster-management
oc project multicluster-engine

# # do not need, because now, it is acm 2.4.2
# # but it seems doesn't matter, if you enable it
# oc patch hiveconfig hive --type merge -p '{"spec":{"targetNamespace":"hive","logLevel":"debug","featureGates":{"custom":{"enabled":["AlphaAgentInstallStrategy"]},"featureSet":"Custom"}}}'

oc get hiveconfig hive -n multicluster-engine -o yaml
# ......
# spec: {}
# status:
#   aggregatorClientCAHash: b30ffa769079a2ac0e37e40172084089
#   conditions:
#   - lastProbeTime: "2023-01-13T09:10:10Z"
#     lastTransitionTime: "2023-01-13T09:10:10Z"
#     message: Hive is deployed successfully
#     reason: DeploymentSuccess
#     status: "True"
#     type: Ready
#   configApplied: true
#   observedGeneration: 1

oc patch provisioning provisioning-configuration --type merge -p '{"spec":{"watchAllNamespaces": true }}'

oc get provisioning provisioning-configuration -o yaml
# ......
# spec:
#   preProvisioningOSDownloadURLs: {}
#   provisioningMacAddresses:
#   - 52:54:00:20:a1:01
#   - 52:54:00:20:a1:02
#   - 52:54:00:20:a1:03
#   provisioningNetwork: Disabled
#   provisioningOSDownloadURL: http://192.168.77.11:8080/rhcos-openstack.x86_64.qcow2.gz?sha256=506bb66f8cb407c74061a8201f13e7b1edd44000d944be85eb7a4df7058dcb79
#   watchAllNamespaces: true
# ......

cat << EOF > ${BASE_DIR}/data/install/acm.ocp.release.yaml
apiVersion: hive.openshift.io/v1
kind: ClusterImageSet
metadata:
  name: openshift-v4.11.21
  namespace: multicluster-engine
spec:
  releaseImage: ${INSTALL_IMAGE_REGISTRY}/openshift/release-images:4.11.21-x86_64
EOF
oc create -f ${BASE_DIR}/data/install/acm.ocp.release.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.ocp.release.yaml

oc get ClusterImageSet
# NAME                 RELEASE
# openshift-v4.11.21   quaylab.infra.wzhlab.top:8443/openshift/release-images:4.11.21-x86_64

cat << EOF > ${BASE_DIR}/data/install/acm.cm.asc.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: assisted-service-config
  namespace: multicluster-engine
  labels:
    app: assisted-service
data:
  LOG_LEVEL: "debug"
EOF
oc create -f ${BASE_DIR}/data/install/acm.cm.asc.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.cm.asc.yaml

cat << EOF > ${BASE_DIR}/data/install/acm.secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: assisted-deployment-pull-secret
  namespace: multicluster-engine
stringData:
  .dockerconfigjson: '$PULL_SECRET'
EOF
oc create -f ${BASE_DIR}/data/install/acm.secret.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.secret.yaml

# oc get pod -A | grep metal3
# if the result is empty, then we will go in manual way
oc get pod -A | grep metal3
# openshift-machine-api                              metal3-8666f4cf4d-2bkfb                                           5/5     Running             0               5m21s
# openshift-machine-api                              metal3-image-cache-8jhtr                                          1/1     Running             0               74m
# openshift-machine-api                              metal3-image-cache-9jfs7                                          1/1     Running             0               74m
# openshift-machine-api                              metal3-image-cache-fl545                                          1/1     Running             0               74m
# openshift-machine-api                              metal3-image-customization-868d87999b-x2mnw                       1/1     Running             0               74m

openshift-install version
# openshift-install 4.11.21
# built from commit d3fb15afdbf1558344ea88a1e134c8e9a011440f
# release image quay.io/openshift-release-dev/ocp-release@sha256:860cc37824074671c4cf76e02d224d243e670d2298e6dab8923ee391fbd0ae1c
# release architecture amd64

openshift-install coreos print-stream-json | jq .architectures.x86_64.artifacts.metal.release -r
# 411.86.202210041459-0

VAR_COREOS_VERSION=`openshift-install coreos print-stream-json | jq .architectures.x86_64.artifacts.metal.release -r`

# the config of CA is important here.
# assisted service will not use cluster's CA config
cat << EOF > ${BASE_DIR}/data/install/acm.mirror.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hyper1-mirror-config
  namespace: multicluster-engine
  labels:
    app: assisted-service
data:
  ca-bundle.crt: |
$( cat /etc/crts/infra.wzhlab.top.crt | sed 's/^/    /g' )
  registries.conf: |
    unqualified-search-registries = ["registry.access.redhat.com", "docker.io"]

    [[registry]]
      prefix = ""
      location = "quay.io/openshift-release-dev/ocp-release"
      mirror-by-digest-only = true

      [[registry.mirror]]
        location = "${INSTALL_IMAGE_REGISTRY}/openshift/release-images"

    [[registry]]
      prefix = ""
      location = "quay.io/openshift-release-dev/ocp-v4.0-art-dev"
      mirror-by-digest-only = true

      [[registry.mirror]]
        location = "${INSTALL_IMAGE_REGISTRY}/openshift/release"

---
EOF
oc create -f ${BASE_DIR}/data/install/acm.mirror.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.mirror.yaml

cat << EOF > ${BASE_DIR}/data/install/acm.agentservicecofnig.yaml
apiVersion: agent-install.openshift.io/v1beta1
kind: AgentServiceConfig
metadata:
  name: agent
  namespace: multicluster-engine
  ### This is the annotation that injects modifications in the Assisted Service pod
  annotations:
    unsupported.agent-install.openshift.io/assisted-service-configmap: "assisted-service-config"
###
spec:
  databaseStorage:
    accessModes:
      - ReadWriteOnce
    resources:
      requests:
        storage: 40Gi
  filesystemStorage:
    accessModes:
      - ReadWriteOnce
    resources:
      requests:
        storage: 40Gi
  ### This is a ConfigMap that only will make sense on Disconnected environments
  mirrorRegistryRef:
    name: "hyper1-mirror-config"
  ###
  osImages:
    - openshiftVersion: "4.11"
      version: "$VAR_COREOS_VERSION"
      url: "http://192.168.77.11:8080/rhcos-live.x86_64.iso"
      rootFSUrl: "http://192.168.77.11:8080/rhcos-live-rootfs.x86_64.img"
      cpuArchitecture: x86_64
EOF
oc create -f ${BASE_DIR}/data/install/acm.agentservicecofnig.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.agentservicecofnig.yaml

# oc get pod -n multicluster-engine -o json | jq .items[].metadata.name -r | xargs -I DEMO oc logs -n multicluster-engine --prefix=true DEMO | grep 'failed to add release image '

# wait here to see all the status is True
oc get AgentServiceConfig/agent -n multicluster-engine -o yaml  
# ......
# status:
#   conditions:
#   - lastTransitionTime: "2023-01-13T01:38:25Z"
#     message: AgentServiceConfig reconcile completed without error.
#     reason: ReconcileSucceeded
#     status: "True"
#     type: ReconcileCompleted
#   - lastTransitionTime: "2023-01-13T01:40:25Z"
#     message: All the deployments managed by Infrastructure-operator are healthy.
#     reason: DeploymentSucceeded
#     status: "True"
#     type: DeploymentsHealthy

# logs in infrastructure-operator 

# stop here, and wait the assisted-service pod run into ok status
oc get pod -n multicluster-engine | grep assisted
# assisted-image-service-0                               1/1     Running   0               4m38s
# assisted-service-764cd98cf7-2r2db                      2/2     Running   1 (2m59s ago)   4m40s

# begin to create new cluster

oc create ns ${ACM_DEMO_CLUSTER}
oc project ${ACM_DEMO_CLUSTER}

cat << EOF > ${BASE_DIR}/data/install/acm.managed.secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: assisted-deployment-pull-secret
  namespace: ${ACM_DEMO_CLUSTER}
stringData:
  .dockerconfigjson: '$PULL_SECRET'
EOF
oc create -f ${BASE_DIR}/data/install/acm.managed.secret.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.managed.secret.yaml

cat << EOF > ${BASE_DIR}/data/install/acm.nmsc.yaml
apiVersion: agent-install.openshift.io/v1beta1
kind: NMStateConfig
metadata:
 name: ${ACM_DEMO_CLUSTER}
 namespace: ${ACM_DEMO_CLUSTER}
 labels:
   nmstate-conf-cluster-name: ${ACM_DEMO_CLUSTER}
spec:
 config:
   interfaces:
     - name: ${SNO_IF}
       type: ethernet
       state: up
       ipv4:
         enabled: true
         address:
           - ip: ${SNO_IP}
             prefix-length: ${SNO_NETMAST_S}
         dhcp: false
   dns-resolver:
     config:
       server:
         - ${SNO_DNS}
   routes:
     config:
       - destination: 0.0.0.0/0
         next-hop-address: ${SNO_GW}
         next-hop-interface: ${SNO_IF}
         table-id: 254
 interfaces:
   - name: "${SNO_IF}" 
     macAddress: ${SNO_IF_MAC}
EOF
oc create -f ${BASE_DIR}/data/install/acm.nmsc.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.nmsc.yaml

oc get NMStateConfig/${ACM_DEMO_CLUSTER} -n ${ACM_DEMO_CLUSTER}
# NAME     AGE
# edge01   3h30m

cat << EOF > ${BASE_DIR}/data/install/acm.clusterdeployment.yaml
apiVersion: hive.openshift.io/v1
kind: ClusterDeployment
metadata:
  name: ${ACM_DEMO_CLUSTER}
  namespace: ${ACM_DEMO_CLUSTER}
spec:
  baseDomain: ${SNO_BASE_DOMAIN}
  clusterName: ${ACM_DEMO_CLUSTER}
  controlPlaneConfig:
    servingCertificates: {}
  installed: false
  clusterInstallRef:
    group: extensions.hive.openshift.io
    kind: AgentClusterInstall
    name: ${ACM_DEMO_CLUSTER}
    version: v1beta1
  platform:
    agentBareMetal:
      agentSelector:
        matchLabels:
          cluster-name: "${ACM_DEMO_CLUSTER}"
  pullSecretRef:
    name: assisted-deployment-pull-secret
EOF
oc create -f ${BASE_DIR}/data/install/acm.clusterdeployment.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.clusterdeployment.yaml

oc get ClusterDeployment/${ACM_DEMO_CLUSTER} -n ${ACM_DEMO_CLUSTER} -o json | jq .status | head
# {
#   "conditions": [
#     {
#       "lastProbeTime": "2023-01-13T01:53:10Z",
#       "lastTransitionTime": "2023-01-13T01:53:10Z",
#       "message": "Platform credentials passed authentication check",
#       "reason": "PlatformAuthSuccess",
#       "status": "False",
#       "type": "AuthenticationFailure"
#     },

# oc create configmap registry-config --from-file=quaylab.infra.wzhlab.top..8443=/etc/crts/infra.wzhlab.top.crt -n openshift-config

# oc edit image.config.openshift.io cluster
# spec:
#   additionalTrustedCA:
#     name: registry-config

cat << EOF > ${BASE_DIR}/data/install/acm.agentclusterinstall.yaml
apiVersion: extensions.hive.openshift.io/v1beta1
kind: AgentClusterInstall
metadata:
  name: ${ACM_DEMO_CLUSTER}
  namespace: ${ACM_DEMO_CLUSTER}
  # Only include the annotation if using OVN, otherwise omit the annotation
#   annotations:
#     agent-install.openshift.io/install-config-overrides: '{"networking":{"networkType":"OVNKubernetes"}}'
spec:
  clusterDeploymentRef:
    name: ${ACM_DEMO_CLUSTER}
  imageSetRef:
    name: openshift-v4.11.21
  networking:
    clusterNetwork:
      - cidr: "10.128.0.0/14"
        hostPrefix: 23
    serviceNetwork:
      - "172.30.0.0/16"
    machineNetwork:
      - cidr: "192.168.12.0/24"
  provisionRequirements:
    controlPlaneAgents: 1
  sshPublicKey: "$(< ~/.ssh/id_rsa.pub)"
EOF
oc create -f ${BASE_DIR}/data/install/acm.agentclusterinstall.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.agentclusterinstall.yaml

# wait a moment, and this will be ok
oc get AgentClusterInstall/${ACM_DEMO_CLUSTER} -n ${ACM_DEMO_CLUSTER} -o json | jq .status | head
# {
#   "conditions": [
#     {
#       "lastProbeTime": "2023-01-13T03:04:50Z",
#       "lastTransitionTime": "2023-01-13T03:04:50Z",
#       "message": "SyncOK",
#       "reason": "SyncOK",
#       "status": "True",
#       "type": "SpecSynced"
#     },

cat << EOF > ${BASE_DIR}/data/install/acm.managedcluster.yaml
apiVersion: cluster.open-cluster-management.io/v1
kind: ManagedCluster
metadata:
  name: ${ACM_DEMO_CLUSTER}
spec:
  hubAcceptsClient: true
EOF
oc create -f ${BASE_DIR}/data/install/acm.managedcluster.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.managedcluster.yaml

oc get ManagedCluster/${ACM_DEMO_CLUSTER} -o yaml
# ......
# status:
#   capacity:
#     core_worker: "0"
#     socket_worker: "0"
#   conditions:
#   - lastTransitionTime: "2023-01-13T15:24:23Z"
#     message: Accepted by hub cluster admin
#     reason: HubClusterAdminAccepted
#     status: "True"
#     type: HubAcceptedManagedCluster
#   version: {}

# 我们是离线安装，所以要定制一下启动配置文件
# generate the ignition

cat << EOF > ${BASE_DIR}/data/sno/ign.base.json
{
  "ignition": {
    "version": "3.1.0"
  }
}
EOF

cat << EOF > ${BASE_DIR}/data/sno/install.images.bu
variant: openshift
version: 4.9.0
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 99-zzz-master-install-images
storage:
  files:
    - path: /etc/containers/registries.conf.d/base.registries.conf
      overwrite: true
      contents:
        inline: |
          unqualified-search-registries = ["registry.access.redhat.com", "docker.io"]
          short-name-mode = ""

          [[registry]]
            prefix = ""
            location = "quay.io/openshift-release-dev/ocp-release"
            mirror-by-digest-only = true

            [[registry.mirror]]
              location = "${INSTALL_IMAGE_REGISTRY}/openshift/release-images"

          [[registry]]
            prefix = ""
            location = "quay.io/openshift-release-dev/ocp-v4.0-art-dev"
            mirror-by-digest-only = true

            [[registry.mirror]]
              location = "${INSTALL_IMAGE_REGISTRY}/openshift/release"

EOF

cat << EOF > ${BASE_DIR}/data/sno/install.crts.bu
variant: openshift
version: 4.9.0
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 99-zzz-master-install-crts
storage:
  files:
    - path: /etc/pki/ca-trust/source/anchors/quaylab.crt
      overwrite: true
      contents:
        inline: |
$( cat /etc/crts/infra.wzhlab.top.crt | sed 's/^/          /g' )

EOF

mkdir -p ${BASE_DIR}/data/sno/disconnected/
# copy ntp related config
/bin/cp -f  /data/ocp4/ansible-helper/files/* ${BASE_DIR}/data/sno/disconnected/

# copy image registry proxy related config
# cd /data/ocp4
# bash image.registries.conf.sh nexus.infra.redhat.ren:8083
sudo bash -c "cd /data/ocp4 ; bash image.registries.conf.quay.sh quaylab.infra.wzhlab.top:8443 ;"

/bin/cp -f /data/ocp4/99-worker-container-registries.yaml ${BASE_DIR}/data/sno/disconnected/
/bin/cp -f /data/ocp4/99-master-container-registries.yaml ${BASE_DIR}/data/sno/disconnected/

cd ${BASE_DIR}/data/sno/
# load ignition file generation function
source /data/ocp4/acm.fn.sh


get_file_content_for_ignition "/opt/openshift/openshift/99-master-chrony-configuration.yaml" "${BASE_DIR}/data/sno/disconnected/99-master-chrony-configuration.yaml"
VAR_99_master_chrony=$RET_VAL
VAR_99_master_chrony_2=$RET_VAL_2

get_file_content_for_ignition "/opt/openshift/openshift/99-worker-chrony-configuration.yaml" "${BASE_DIR}/data/sno/disconnected/99-worker-chrony-configuration.yaml"
VAR_99_worker_chrony=$RET_VAL
VAR_99_worker_chrony_2=$RET_VAL_2

get_file_content_for_ignition "/opt/openshift/openshift/99-master-container-registries.yaml" "${BASE_DIR}/data/sno/disconnected/99-master-container-registries.yaml"
VAR_99_master_container_registries=$RET_VAL
VAR_99_master_container_registries_2=$RET_VAL_2

get_file_content_for_ignition "/opt/openshift/openshift/99-worker-container-registries.yaml" "${BASE_DIR}/data/sno/disconnected/99-worker-container-registries.yaml"
VAR_99_worker_container_registries=$RET_VAL
VAR_99_worker_container_registries_2=$RET_VAL_2

butane ${BASE_DIR}/data/sno/install.images.bu > ${BASE_DIR}/data/sno/disconnected/99-zzz-master-install-images.yaml
get_file_content_for_ignition "/opt/openshift/openshift/99-zzz-master-install-images.yaml" "${BASE_DIR}/data/sno/disconnected/99-zzz-master-install-images.yaml"
VAR_99_master_install_images=$RET_VAL
VAR_99_master_install_images_2=$RET_VAL_2

butane ${BASE_DIR}/data/sno/install.crts.bu > ${BASE_DIR}/data/sno/disconnected/99-zzz-master-install-crts.yaml
get_file_content_for_ignition "/opt/openshift/openshift/99-zzz-master-install-crts.yaml" "${BASE_DIR}/data/sno/disconnected/99-zzz-master-install-crts.yaml"
VAR_99_master_install_crts=$RET_VAL
VAR_99_master_install_crts_2=$RET_VAL_2

# https://access.redhat.com/solutions/6194821
# butane /data/sno/static.ip.bu | python3 -c 'import json, yaml, sys; print(json.dumps(yaml.load(sys.stdin)))'

# https://stackoverflow.com/questions/2854655/command-to-escape-a-string-in-bash
# VAR_PULL_SEC=`printf "%q" $(cat  /data/pull-secret.json)`

# https://access.redhat.com/solutions/221403
# VAR_PWD_HASH="$(openssl passwd -1 -salt 'openshift' 'redhat')"
VAR_PWD_HASH="$(python3 -c 'import crypt,getpass; print(crypt.crypt("redhat"))')"

tmppath=$(mktemp)
cat ${BASE_DIR}/data/sno/ign.base.json \
  | jq --arg VAR "$VAR_PWD_HASH" --arg VAR_SSH "$NODE_SSH_KEY" '.passwd.users += [{ "name": "wzh", "system": true, "passwordHash": $VAR , "sshAuthorizedKeys": [ $VAR_SSH ], "groups": [ "adm", "wheel", "sudo", "systemd-journal"  ] }]' \
  | jq --argjson VAR "$VAR_99_master_chrony" '.storage.files += [$VAR] ' \
  | jq --argjson VAR "$VAR_99_worker_chrony" '.storage.files += [$VAR] ' \
  | jq --argjson VAR "$VAR_99_master_container_registries" '.storage.files += [$VAR] ' \
  | jq --argjson VAR "$VAR_99_worker_container_registries" '.storage.files += [$VAR] ' \
  | jq --argjson VAR "$VAR_99_master_chrony_2" '.storage.files += [$VAR] ' \
  | jq --argjson VAR "$VAR_99_master_container_registries_2" '.storage.files += [$VAR] ' \
  | jq --argjson VAR "$VAR_99_master_install_images_2" '.storage.files += [$VAR] ' \
  | jq --argjson VAR "$VAR_99_master_install_crts_2" '.storage.files += [$VAR] ' \
  | jq -c . \
  > ${tmppath}
VAR_IGNITION=$(cat ${tmppath})
rm -f ${tmppath}


cat << EOF > ${BASE_DIR}/data/install/acm.infraenv.yaml
apiVersion: agent-install.openshift.io/v1beta1
kind: InfraEnv
metadata:
  name: ${ACM_DEMO_CLUSTER}
  namespace: ${ACM_DEMO_CLUSTER}
spec:
  additionalNTPSources:
    - 192.168.77.11
  clusterRef:
    name: ${ACM_DEMO_CLUSTER}
    namespace: ${ACM_DEMO_CLUSTER}
  sshAuthorizedKey: "$(< ~/.ssh/id_rsa.pub)"
  pullSecretRef:
    name: assisted-deployment-pull-secret
  ignitionConfigOverride: '${VAR_IGNITION}'
  nmStateConfigLabelSelector:
    matchLabels:
      nmstate-conf-cluster-name: ${ACM_DEMO_CLUSTER}
  # imageType: "full-iso"
EOF
oc create -f ${BASE_DIR}/data/install/acm.infraenv.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.infraenv.yaml

oc get infraenv/${ACM_DEMO_CLUSTER} -n ${ACM_DEMO_CLUSTER} -o json | jq .status
# {
#   "agentLabelSelector": {
#     "matchLabels": {
#       "infraenvs.agent-install.openshift.io": "edge01"
#     }
#   },
#   "bootArtifacts": {
#     "initrd": "https://assisted-image-service-multicluster-engine.apps.factory.wzhlab.top/images/c70485f3-0b12-437f-9efe-85b17f0c627f/pxe-initrd?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbmZyYV9lbnZfaWQiOiJjNzA0ODVmMy0wYjEyLTQzN2YtOWVmZS04NWIxN2YwYzYyN2YifQ.rrkRFxLVcMjEw16W3brxl_YCxHtJtUu-h0KMHcvj3DO701_ZPUM6cDg765Q02CviGSNcSTmu0ic5g06AkU0Zzg&arch=x86_64&version=4.11",
#     "ipxeScript": "https://assisted-service-multicluster-engine.apps.factory.wzhlab.top/api/assisted-install/v2/infra-envs/c70485f3-0b12-437f-9efe-85b17f0c627f/downloads/files?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbmZyYV9lbnZfaWQiOiJjNzA0ODVmMy0wYjEyLTQzN2YtOWVmZS04NWIxN2YwYzYyN2YifQ.3j_oKrmfOVQn85v2S3laLojUKaCTRqgkv_aSBPo-z_7k8-n2swb2m9aNT3uPr3CEstV4UVurkYwShtawFed0Cg&file_name=ipxe-script",
#     "kernel": "https://assisted-image-service-multicluster-engine.apps.factory.wzhlab.top/boot-artifacts/kernel?arch=x86_64&version=4.11",
#     "rootfs": "https://assisted-image-service-multicluster-engine.apps.factory.wzhlab.top/boot-artifacts/rootfs?arch=x86_64&version=4.11"
#   },
#   "conditions": [
#     {
#       "lastTransitionTime": "2023-01-13T03:15:17Z",
#       "message": "Image has been created",
#       "reason": "ImageCreated",
#       "status": "True",
#       "type": "ImageCreated"
#     }
#   ],
#   "createdTime": "2023-01-13T03:15:16Z",
#   "debugInfo": {
#     "eventsURL": "https://assisted-service-multicluster-engine.apps.factory.wzhlab.top/api/assisted-install/v2/events?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbmZyYV9lbnZfaWQiOiJjNzA0ODVmMy0wYjEyLTQzN2YtOWVmZS04NWIxN2YwYzYyN2YifQ.W_KCQgx4SwgbErK6eiyh7EmxPb9L8KKawXLOWPgBoPxVPH79QXq5wb-X5DT48b6qBlk3xk-F7MCT_bEG1f30Ww&infra_env_id=c70485f3-0b12-437f-9efe-85b17f0c627f"
#   },
#   "isoDownloadURL": "https://assisted-image-service-multicluster-engine.apps.factory.wzhlab.top/images/c70485f3-0b12-437f-9efe-85b17f0c627f?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbmZyYV9lbnZfaWQiOiJjNzA0ODVmMy0wYjEyLTQzN2YtOWVmZS04NWIxN2YwYzYyN2YifQ.4FqFWSqfYijmGGWAKopqHIiKghDZBZ2NAqTY1hmUhwNfTzuKlFLZ2pDZAevAxtmf7aN96-6UCeNewIfqoLzPVQ&arch=x86_64&type=minimal-iso&version=4.11"
# }


# VAR_ISO=`oc get infraenv ${ACM_DEMO_CLUSTER} -n ${ACM_DEMO_CLUSTER} -o jsonpath={.status.isoDownloadURL}`

# cd /data/install/
# wget --no-check-certificate -O acm.demo1.iso $VAR_ISO

oc get pod -A | grep metal3
# openshift-machine-api                              metal3-8666f4cf4d-2bkfb                                           5/5     Running     5               12h
# openshift-machine-api                              metal3-image-cache-8jhtr                                          1/1     Running     1               13h
# openshift-machine-api                              metal3-image-cache-9jfs7                                          1/1     Running     1               13h
# openshift-machine-api                              metal3-image-cache-fl545                                          1/1     Running     1               13h
# openshift-machine-api                              metal3-image-customization-868d87999b-x2mnw                       1/1     Running     1               13h

# cd /data/ocp4/
# cat << 'EOF' > redfish.sh
# #!/usr/bin/env bash

# curl -k -s https://192.168.7.1:8000/redfish/v1/Systems/ | jq -r '.Members[]."@odata.id"' >  list

# while read -r line; do
#     curl -k -s https://192.168.7.1:8000/$line | jq -j '.Id, " ", .Name, "\n" '
# done < list

# EOF
# bash redfish.sh > /data/install/vm.list
# cat /data/install/vm.list
# # 075b17f7-9be9-4576-8d72-2ddd99909e19 ocp4-acm-demo1-master0
# # c991312a-26de-438d-8c2d-6aa6cd586bca ocp4-master0
# # e70f66bc-7878-4617-811d-89cdaf62cc8c ocp4-Helper

# oc patch provisioning provisioning-configuration --type merge -p '{"spec":{"watchAllNamespaces": true}}'

cat << EOF > ${BASE_DIR}/data/install/acm.demo.secret.bmc.yaml
apiVersion: v1
kind: Secret
metadata:
  name: ${ACM_DEMO_CLUSTER}-bmc-master-01
  namespace: ${ACM_DEMO_CLUSTER}
data:
  password: $(echo password | base64)
  username: $(echo admin | base64)
type: Opaque
EOF
oc create -f ${BASE_DIR}/data/install/acm.demo.secret.bmc.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.demo.secret.bmc.yaml

cat << EOF > ${BASE_DIR}/data/install/acm.demo.bmh.master.yaml
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: ${ACM_DEMO_CLUSTER}-master-01
  namespace: ${ACM_DEMO_CLUSTER}
  labels:
    infraenvs.agent-install.openshift.io: "${ACM_DEMO_CLUSTER}"
  annotations:
    ## Disable the Introspection
    inspect.metal3.io: disabled
    ## Set Static Hostname
    bmac.agent-install.openshift.io/hostname: "${SNO_HOSTNAME}"
    ## Set Static Role
    bmac.agent-install.openshift.io/role: "master"
spec:
  online: true
  bmc:
    address: redfish-virtualmedia://192.168.77.101:8000/redfish/v1/Systems/$(cat /data/install/vm.list.* | grep ocp4-ipi-edge-master-01 | awk '{print $1}')
    credentialsName: ${ACM_DEMO_CLUSTER}-bmc-master-01
    disableCertificateVerification: true
  bootMACAddress: $(cat /data/install/mac.list.* | grep ocp4-ipi-edge-master-01 | awk '{print $2}')
  automatedCleaningMode: disabled
EOF
oc create -f ${BASE_DIR}/data/install/acm.demo.bmh.master.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.demo.bmh.master.yaml

oc get agent
# NAME                                   CLUSTER   APPROVED   ROLE     STAGE
# a176e428-fea7-43ff-95c7-a927514227ed   edge01    true       master   Rebooting

oc get agent/a176e428-fea7-43ff-95c7-a927514227ed -o yaml
# ......
# spec:
#   approved: true
#   clusterDeploymentName:
#     name: edge01
#     namespace: edge01
#   hostname: edge-master-01
#   role: master
# ......

```

<!-- 我们回到MCE的界面中，能从基础架构中，看到我们新创建的HOST了，能看到MCE正在通过redfish配置这个kvm -->
Back to the MCE interface, we can see the newly created host from the infrastructure, and we can see that MCE is configuring this KVM via Redfish. 

![](../4.11/imgs/2023-01-13-23-38-41.png)

<!-- 这个bare metal host其实是调用的openshift4平台上的服务创建的，所以从openshift4的console上也能看得到： -->
This bare metal host was actually created by calling a service on the Openshift 4 platform, so it is also visible from the Openshift 4 console. 

![](../4.11/imgs/2023-01-13-23-31-32.png)

<!-- 能从openshift4 console上看到这个bare metal host的详细信息： -->
View details of this bare metal host from the openshift4 console. 

![](../4.11/imgs/2023-01-13-23-32-56.png)

![](../4.11/imgs/2023-01-13-23-32-27.png)

<!-- 回到ACM的界面中，我们能看到安装正在继续： -->
Back in the ACM interface, we can see the installation is continuing. 

![](../4.11/imgs/2023-01-13-23-33-40.png)

<!-- 从ACM的cluster界面中，我们能看到安装的详细进展情况： -->
From ACM's cluster interface, we can see the detailed installation progress: 

![](../4.11/imgs/2023-01-13-23-34-09.png)

<!-- 但是安装的中途，提示我们需要动手操作一下。这是因为我们是用kvm模拟的物理机，并且模拟了一个redfish，这个redfish功能比较简单，在安装ocp的过程中，kvm会重启，但是远程挂载的光盘没有卸载，所以我们需要卸载掉这个光驱，然后继续安装： -->
However, during the installation process, we are prompted for manual intervention. This is because we are using KVM to simulate a physical machine with a simplified Redfish implementation. When KVM restarts during the OpenShift Container Platform (OCP) installation, the remotely mounted optical drive isn't unmounted. Therefore, we need to manually unmount the drive before continuing with the installation. 

![](../4.11/imgs/2023-01-13-20-19-01.png)

![](../4.11/imgs/2023-01-13-20-19-34.png)

<!-- 进入kvm的界面，调整一下启动顺序： -->
Enter the KVM interface and adjust the boot order. 

![](../4.10/imgs/20220408230353.png)  

<!-- 然后重启kvm，等待一段时间，infra env就安装完成了。 -->
Then restart KVM. After waiting for a while, the infra environment installation will be finished. 

![](../4.11/imgs/2023-01-13-20-22-49.png)

<!-- 不过，cluster还在继续安装，我们安心等待安装过程完成。 -->
However, the cluster installation is still in progress, we can wait patiently for it to complete. 

![](../4.11/imgs/2023-01-13-20-21-47.png) 

<!-- 终于完成啦。 -->
Finally finished. 

<!-- ## 安装完成 -->
## Installation Complete 

<!-- 装好了以后，我们在MCE里面就能看到如下景象：  -->
After installation, we can see the following in MCE: 

https://console-openshift-console.apps.factory.wzhlab.top/multicloud/infrastructure/clusters/managed

![](../4.11/imgs/2023-01-13-21-19-58.png)


<!-- 看cluster的详细信息，也正常了： -->
Checking the cluster details, it appears normal now. 

![](../4.11/imgs/2023-01-13-21-16-29.png)

<!-- ⚠️一定记得，下载kubeconfig文件，还有密码 -->
⚠️Checking the cluster details, it appears normal now. 

<!-- cluster的node tab也有内容了： -->
The cluster's node tab now also has content. 

![](../4.11/imgs/2023-01-13-21-52-50.png)

<!-- cluster的add-on也有了组件，这两个组件是干什么用的呢？作者暂时说不确切，以后查到资料再分析吧。 -->
The cluster's add-on now also has components. What are these two components for? The author is not sure at the moment and will analyze them after consulting relevant materials later. 

![](imgs/2023-01-13-21-53-25.png)

<!-- infra env也绿色状态了  -->
The infra environment is also green now. 

https://console-openshift-console.apps.factory.wzhlab.top/multicloud/infrastructure/environments

![](../4.11/imgs/2023-01-13-22-45-47.png)

<!-- 详细信息和原来一样： -->
Detailed information remains the same. 

![](../4.11/imgs/2023-01-13-22-46-13.png)

<!-- hosts tab 也完成了 -->
The hosts tab is also finished. 

![](../4.11/imgs/2023-01-13-22-46-41.png)

## post operation

<!-- 装完了，我们为了方便做实验，我们对集群节点做点配置。虽然减低了集群的安全性，但是做实验吗，无所谓了。 -->
Installation complete. To simplify our experiments, we'll adjust some cluster node configurations.  This reduces cluster security, but since it's for experiments, it doesn't matter. 

```bash

# on helper

# VAR_CLUSTER=edge01
# oc get secret/$VAR_CLUSTER-keypair -n $VAR_CLUSTER --template='{{index .data "id_rsa.key" | base64decode}}' > ${BASE_DIR}/data/install/edge.key

# chmod 600 ${BASE_DIR}/data/install/edge.key

# ssh -i ${BASE_DIR}/data/install/edge.key core@192.168.12.33

cat > ${BASE_DIR}/data/install/crack.txt << EOF

echo redhat | sudo passwd --stdin root

sudo sed -i "s|^PasswordAuthentication no$|PasswordAuthentication yes|g" /etc/ssh/sshd_config
sudo sed -i "s|^PermitRootLogin no$|PermitRootLogin yes|g" /etc/ssh/sshd_config
sudo sed -i "s|^#ClientAliveInterval 180$|ClientAliveInterval 1800|g" /etc/ssh/sshd_config

sudo systemctl restart sshd

sudo sh -c 'echo "export KUBECONFIG=/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig" >> /root/.bashrc'

sudo sh -c 'echo "RET=\\\`oc config use-context system:admin\\\`" >> /root/.bashrc'

EOF

for i in 33
do
  ssh core@192.168.12.$i < ${BASE_DIR}/data/install/crack.txt
done


for i in 33
do
  sshpass -p 'redhat' ssh-copy-id root@192.168.12.$i
done


ssh root@192.168.12.33

```

# check dhcp existed

<!-- 我们是静态IP安装，那么就要确认一下环境里面是不是真的 DHCP 给关了，检查的方法如下。 -->
If you are using a static IP address, please make sure that DHCP is disabled in your environment.  You can check this as follows: 

https://superuser.com/questions/750359/check-if-a-dhcp-server-existing-in-my-network-using-bash

```bash
dnf install nmap -y
nmap --script broadcast-dhcp6-discover -e enp1s0

```

# end

```bash

# revert the order
tac << EOF 
oc delete -f /data/install/acm.ocp.release.yaml
oc delete -f /data/install/acm.cm.asc.yaml
oc delete -f /data/install/acm.secret.yaml
oc delete -f /data/install/acm.mirror.yaml
oc delete -f /data/install/acm.agentservicecofnig.yaml
oc delete -f /data/install/acm.managed.secret.yaml
oc delete -f /data/install/acm.agentclusterinstall.yaml
oc delete -f /data/install/acm.nmsc.yaml
oc delete -f /data/install/acm.clusterdeployment.yaml
oc delete -f /data/install/acm.klusterletaddonconfig.yaml
oc delete -f /data/install/acm.managedcluster.yaml
oc delete -f /data/install/acm.infraenv.yaml
EOF
oc delete -f /data/install/acm.infraenv.yaml
oc delete -f /data/install/acm.managedcluster.yaml
oc delete -f /data/install/acm.klusterletaddonconfig.yaml
oc delete -f /data/install/acm.clusterdeployment.yaml
oc delete -f /data/install/acm.nmsc.yaml
oc delete -f /data/install/acm.agentclusterinstall.yaml
oc delete -f /data/install/acm.managed.secret.yaml
oc delete -f /data/install/acm.agentservicecofnig.yaml
oc delete -f /data/install/acm.mirror.yaml
oc delete -f /data/install/acm.secret.yaml
oc delete -f /data/install/acm.cm.asc.yaml
oc delete -f /data/install/acm.ocp.release.yaml


```
