# persistent log for pod

# using crio config

经过时间，发现crio里面，虽然有配置，说可以mount dir，但是发现挂载得只是那个目录得一个快照，容器里面往里面写得数据，不会持久化到host上面。

```bash

cat /usr/share/containers/mounts.conf
# /usr/share/rhel/secrets:/run/secrets

mkdir -p /var/wzh-local-log

chmod 755 /var/wzh-local-log

cat /usr/share/containers/mounts.conf > /etc/containers/mounts.conf

cat << EOF > /etc/containers/mounts.conf
/usr/share/rhel/secrets:/run/secrets
/var/wzh-local-log:/wzh-log
EOF


```

install cnv, and config a hostpath. We know the hostpath works for worker node, but it not work for master node.

```yaml
apiVersion: hostpathprovisioner.kubevirt.io/v1beta1
kind: HostPathProvisioner
metadata:
  name: hostpath-provisioner
spec:
  imagePullPolicy: IfNotPresent
  storagePools:
    - name: local
      path: /var/wzh-local-log
  workload:
    nodeSelector:
      kubernetes.io/os: linux

```

create a machine config, to update the crio config

```bash

export BASE_DIR=/home/dev-admin

mkdir -p ${BASE_DIR}/.local/bin

cd ${BASE_DIR}/.local/bin
wget --inet4-only -O butane-amd64 https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/butane/latest/butane-amd64

mv butane-amd64 butane
chmod +x butane


mkdir -p ${BASE_DIR}/data/install/openshift

cat << EOF > ${BASE_DIR}/data/install/98-master-crio-mount.bu
variant: openshift
version: 4.12.0
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 98-master-crio-mount
storage:
  files:
    - path: /etc/containers/mounts.conf
      mode: 0644
      overwrite: true
      contents:
        inline: |
          /usr/share/rhel/secrets:/run/secrets
          /var/wzh-local-log:/wzh-log
EOF

butane ${BASE_DIR}/data/install/98-master-crio-mount.bu -o ${BASE_DIR}/data/install/openshift/98-master-crio-mount.yaml

cat << EOF > ${BASE_DIR}/data/install/98-worker-crio-mount.bu
variant: openshift
version: 4.12.0
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 98-worker-crio-mount
storage:
  files:
    - path: /etc/containers/mounts.conf
      mode: 0644
      overwrite: true
      contents:
        inline: |
          /usr/share/rhel/secrets:/run/secrets
          /var/wzh-local-log:/wzh-log
EOF

butane ${BASE_DIR}/data/install/98-worker-crio-mount.bu -o ${BASE_DIR}/data/install/openshift/98-worker-crio-mount.yaml

oc apply -f ${BASE_DIR}/data/install/openshift/

```

# using hostpath with retain policy

install cnv, and config a hostpath. We know the hostpath works for worker node, but it not work for master node.

```yaml
apiVersion: hostpathprovisioner.kubevirt.io/v1beta1
kind: HostPathProvisioner
metadata:
  name: hostpath-provisioner
spec:
  imagePullPolicy: IfNotPresent
  storagePools:
    - name: local
      path: /var/wzh-local-log
  workload:
    nodeSelector:
      kubernetes.io/os: linux

```

then define a storage class

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-log-hostpath-csi
provisioner: kubevirt.io.hostpath-provisioner
reclaimPolicy: Retain 
volumeBindingMode: WaitForFirstConsumer 
parameters:
  storagePool: local
```

Now, we can start a demo app, and mount the log volumn, later, we will shutdown the deployment, and see the log files is still there.

```bash

export BASE_DIR=/home/dev-admin

mkdir -p ${BASE_DIR}/data/install/openshift

cat << EOF > ${BASE_DIR}/data/install/sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: pod-info-sa
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
subjects:
- kind: ServiceAccount
  name: pod-info-sa
  namespace: llm-demo
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
EOF

oc apply -f ${BASE_DIR}/data/install/sa.yaml -n llm-demo

# oc delete -f ${BASE_DIR}/data/install/sa.yaml -n llm-demo

cat << 'EOF' > ${BASE_DIR}/data/install/pod-info.sh
#!/bin/sh

touch pod.name.$POD_NAME.txt
touch pod.namespace.$NAMESPACE.txt

oc get pod $POD_NAME -n $NAMESPACE -o yaml > /mnt/pod-deploy.txt 2>&1

oc describe pod $POD_NAME -n $NAMESPACE  > /mnt/pod-describe.txt 2>&1

# oc get pod $POD_NAME -n $NAMESPACE -o jsonpath='{.spec.containers[*].name}' > /mnt/pod-containers.txt 2>&1

for var_container in $(oc get pod $POD_NAME -n $NAMESPACE -o jsonpath='{.spec.containers[*].name}'); do 
  mkdir -p /mnt/$var_container
done
EOF

oc create configmap wzh-script-configmap --from-file=${BASE_DIR}/data/install/pod-info.sh -n llm-demo

# oc delete configmap wzh-script-configmap -n llm-demo

oc describe sa pod-info-sa -n llm-demo | grep Tokens | awk '{print $2}'
# pod-info-sa-token-cxn8c

VAR_TOKEN=$(oc describe sa pod-info-sa -n llm-demo | grep Tokens | awk '{print $2}')

# create the demo pod
cat << EOF > ${BASE_DIR}/data/install/demo.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: wzh-demo-service
spec:
  ports:
    - name: service-port
      port: 80
      protocol: TCP
      targetPort: 8080
  selector:
    app: pod-description-writer

---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: wzh-demo
  annotations:
    haproxy.router.openshift.io/timeout: 2m
spec:
  to:
    kind: Service
    name: wzh-demo-service
  port:
    targetPort: service-port

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pod-description-writer
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pod-description-writer
  template:
    metadata:
      labels:
        app: pod-description-writer
    spec:
      serviceAccountName: pod-info-sa
      automountServiceAccountToken: false
      volumes:
      - name: pod-description
        ephemeral:
          volumeClaimTemplate:
            metadata:
              labels:
                type: my-frontend-volume
            spec:
              accessModes: [ "ReadWriteOnce" ]
              storageClassName: "local-log-hostpath-csi"
              resources:
                requests:
                  # storage size does not matter for hostpath, becuase it will use all of the disk free space.
                  # but it must be set to actual required size for other storage class
                  storage: 1Gi
      - name: service-account-token
        secret:
          secretName: $VAR_TOKEN
      - name: wzh-script-volume
        configMap:
          name: wzh-script-configmap
      initContainers:
      - name: write-pod-description
        image: registry.redhat.io/openshift4/ose-cli:v4.15
        volumeMounts:
        - name: pod-description
          mountPath: /mnt
        - name: service-account-token
          mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          readOnly: true
        - name: wzh-script-volume
          mountPath: /wzh-scripts
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        command: ['bash', '/wzh-scripts/pod-info.sh']
      containers:
      - name: my-app-heap-dump
        image: quay.io/wangzheng422/qimgs:simple-java-http-server-threads-2024.05.30.v04
        volumeMounts:
        - name: pod-description
          subPath: my-app-heap-dump
          mountPath: /wzh-log/
EOF

oc apply -f ${BASE_DIR}/data/install/demo.yaml -n llm-demo

# oc delete -f ${BASE_DIR}/data/install/demo.yaml -n llm-demo

# get the route of the demo app, and extract the url from the route resource
oc get route wzh-demo -n llm-demo -o jsonpath='{.spec.host}{"\n"}'
# wzh-demo-llm-demo.apps.cluster-gtmcf.sandbox301.opentlc.com

VAR_ROUTE=$(oc get route wzh-demo -n llm-demo -o jsonpath='{.spec.host}{"\n"}')

# cause the java app to dump
curl -k http://${VAR_ROUTE}/dumpheap

# cause the java app to crash
curl -k http://${VAR_ROUTE}/crashapi

```

## clean up pv

After pod exits, the PV used for log is in release status, this is because the reclaim policy is set to retain, we need to manually mark the PV to delete.

```bash

oc get pv | grep local-log-hostpath-csi | grep Released | awk '{print $1}' | \
  xargs -I {} oc patch pv {} -p '{"spec":{"persistentVolumeReclaimPolicy":"Delete"}}'


```

# end