# persistent log for pod with multiple log files

Customer wants to use k8s/container platform/openshift, just as vm platform. They want to have a persistent log directory for each pod, and the log files should be kept after the pod is deleted. But k8s by default, only support STDOUT, STDERR for log, and the log files will be deleted after the pod is deleted.

The key problem here is that container only support 2 log streams, but app, like java app, may have multiple log files, and the log files are generated by the app itself, like gc.log, access.log, and so on. So, we need to find a way to keep the log files.

For customer moving from vm to container, they are familiar with file based log collecting system, like fluend, and most of them already have deploy such platform. So we will create a solution, that can work with the existing log collecting system. This means, we will write log to local file system, and the log files will be collected by the log collecting system.

From openshift admin view, the solution architecture is like this:

![](./dia/4.14.log.dir.pod.drawio.png)

From customer developer team view, and operation/log analyze team view, the solution architecture is like this:

![](./dia/4.14.log.dir.pod.custome.drawio.png)

# using hostpath with retain policy

install cnv, and config a hostpath. We know the hostpath works for worker node, but it not work for master node.

```yaml
apiVersion: hostpathprovisioner.kubevirt.io/v1beta1
kind: HostPathProvisioner
metadata:
  name: hostpath-provisioner
spec:
  imagePullPolicy: IfNotPresent
  storagePools:
    - name: local
      path: /var/wzh-local-log
  workload:
    nodeSelector:
      kubernetes.io/os: linux

```

then define a storage class

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-log-hostpath-csi
provisioner: kubevirt.io.hostpath-provisioner
reclaimPolicy: Retain 
volumeBindingMode: WaitForFirstConsumer 
parameters:
  storagePool: local

```

Now, we can start a demo app, and mount the log volumn, later, we will shutdown the deployment, and see the log files is still there.

The source code of the demo app is here:
- https://github.com/wangzheng422/simple-java-http-server/blob/threads/src/main/java/com/example/httpservice/MyController.java

```bash

export BASE_DIR=/home/dev-admin

mkdir -p ${BASE_DIR}/data/install/openshift

# we need a sa, that can read pod information
# the default sa, do have such privilege
cat << EOF > ${BASE_DIR}/data/install/sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: pod-info-sa
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
subjects:
- kind: ServiceAccount
  name: pod-info-sa
  namespace: llm-demo
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
EOF

oc apply -f ${BASE_DIR}/data/install/sa.yaml -n llm-demo

# oc delete -f ${BASE_DIR}/data/install/sa.yaml -n llm-demo

# we use a startup script to get pod information
# and store it to log dir, so admin can know the pod information
cat << 'EOF' > ${BASE_DIR}/data/install/pod-info.sh
#!/bin/sh

touch /mnt/pod.name.$POD_NAME
touch /mnt/pod.namespace.$NAMESPACE

oc get pod $POD_NAME -n $NAMESPACE -o yaml > /mnt/pod-deploy.txt 2>&1

oc describe pod $POD_NAME -n $NAMESPACE  > /mnt/pod-describe.txt 2>&1

# oc get pod $POD_NAME -n $NAMESPACE -o jsonpath='{.spec.containers[*].name}' > /mnt/pod-containers.txt 2>&1

# create a directory for each container
for var_container in $(oc get pod $POD_NAME -n $NAMESPACE -o jsonpath='{.spec.containers[*].name}'); do 
  mkdir -p /mnt/$var_container
done
EOF

# inject the script into a configmap
oc create configmap wzh-script-configmap --from-file=${BASE_DIR}/data/install/pod-info.sh -n llm-demo

# oc delete configmap wzh-script-configmap -n llm-demo

oc describe sa pod-info-sa -n llm-demo | grep Tokens | awk '{print $2}'
# pod-info-sa-token-cxn8c

VAR_TOKEN=$(oc describe sa pod-info-sa -n llm-demo | grep Tokens | awk '{print $2}')

# create the demo pod
cat << EOF > ${BASE_DIR}/data/install/demo.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: wzh-demo-service
spec:
  ports:
    - name: service-port
      port: 80
      protocol: TCP
      targetPort: 8080
  selector:
    app: pod-description-writer

---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: wzh-demo
  annotations:
    haproxy.router.openshift.io/timeout: 2m
spec:
  to:
    kind: Service
    name: wzh-demo-service
  port:
    targetPort: service-port

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pod-description-writer
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pod-description-writer
  template:
    metadata:
      labels:
        app: pod-description-writer
    spec:
      serviceAccountName: pod-info-sa
      # do not auto mount sa token to container, for security reason
      automountServiceAccountToken: false
      volumes:
      - name: pod-description
        ephemeral:
          volumeClaimTemplate:
            metadata:
              labels:
                type: my-frontend-volume
            spec:
              accessModes: [ "ReadWriteOnce" ]
              storageClassName: "local-log-hostpath-csi"
              resources:
                requests:
                  # storage size does not matter for hostpath, becuase it will use all of the disk free space.
                  # but it must be set to actual required size for other storage class
                  storage: 1Gi
      - name: service-account-token
        secret:
          secretName: $VAR_TOKEN
      - name: wzh-script-volume
        configMap:
          name: wzh-script-configmap
      initContainers:
      - name: write-pod-description
        image: registry.redhat.io/openshift4/ose-cli:v4.15
        volumeMounts:
        - name: pod-description
          mountPath: /mnt
        - name: service-account-token
          mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          readOnly: true
        - name: wzh-script-volume
          mountPath: /wzh-scripts
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        command: ['bash', '/wzh-scripts/pod-info.sh']
      containers:
      - name: my-app-heap-dump
        image: quay.io/wangzheng422/qimgs:simple-java-http-server-threads-2024.05.30.v04
        volumeMounts:
        - name: pod-description
          # subpath should support var, but we can not get container name
          # so we use a fixed name
          subPath: my-app-heap-dump
          mountPath: /wzh-log/
EOF

oc apply -f ${BASE_DIR}/data/install/demo.yaml -n llm-demo

# oc delete -f ${BASE_DIR}/data/install/demo.yaml -n llm-demo

# get the route of the demo app, and extract the url from the route resource
oc get route wzh-demo -n llm-demo -o jsonpath='{.spec.host}{"\n"}'
# wzh-demo-llm-demo.apps.cluster-gtmcf.sandbox301.opentlc.com

VAR_ROUTE=$(oc get route wzh-demo -n llm-demo -o jsonpath='{.spec.host}{"\n"}')

# cause the java app to dump
curl -k http://${VAR_ROUTE}/dumpheap

# cause the java app to crash
curl -k http://${VAR_ROUTE}/crashapi

# cause the java app to dump again
curl -k http://${VAR_ROUTE}/dumpheap

# check the log dir from pod
oc exec -it $(oc get pod -n llm-demo | grep pod-description-writer | awk '{print $1}') -n llm-demo -- ls -l /wzh-log/
# Defaulted container "my-app-heap-dump" out of: my-app-heap-dump, write-pod-description (init)
# total 52196
# -rw-------. 1 1000880000 1000880000 26725084 May 30 16:49 heap-dump_2024-05-30_16-49-47.hprof
# -rw-------. 1 1000880000 1000880000 26719723 May 30 16:50 heap-dump_2024-05-30_16-50-09.hprof


# check the log dir from host
oc get pod -o wide -n llm-demo
# NAME                                      READY   STATUS    RESTARTS      AGE   IP            NODE                                         NOMINATED NODE   READINESS GATES
# pod-description-writer-76df449b56-kxqfr   1/1     Running   1 (35s ago)   62s   10.128.2.32   ip-10-0-248-240.us-east-2.compute.internal   <none>           <none>

oc debug node/ip-10-0-248-240.us-east-2.compute.internal -- ls -R /host/var/wzh-local-log
# Temporary namespace openshift-debug-blrnb is created for debugging node...
# Starting pod/ip-10-0-248-240us-east-2computeinternal-debug ...
# To use host binaries, run `chroot /host`
# /host/var/wzh-local-log:
# csi

# /host/var/wzh-local-log/csi:
# pvc-5f48184b-2b3b-4101-815c-2d93894b139f

# /host/var/wzh-local-log/csi/pvc-5f48184b-2b3b-4101-815c-2d93894b139f:
# my-app-heap-dump
# pod-deploy.txt
# pod-describe.txt
# pod.name.pod-description-writer-76df449b56-kxqfr
# pod.namespace.llm-demo

# /host/var/wzh-local-log/csi/pvc-5f48184b-2b3b-4101-815c-2d93894b139f/my-app-heap-dump:
# heap-dump_2024-05-30_16-49-47.hprof
# heap-dump_2024-05-30_16-50-09.hprof

# Removing debug pod ...
# Temporary namespace openshift-debug-blrnb was removed.


```

## clean up pv

After pod exits, the PV used for log is in release status, this is because the reclaim policy is set to retain, we need to manually mark the PV to delete.

```bash

oc get pv | grep local-log-hostpath-csi | grep Released | awk '{print $1}' | \
  xargs -I {} oc patch pv {} -p '{"spec":{"persistentVolumeReclaimPolicy":"Delete"}}'

```

# limitation & recommand

1. enough space on local disk, especially the /var
2. additional disk partition for /var/wzh-local-log
3. consider using pv/pvc for storagePools

# collect raw data files

For java heap dump, it is binary raw data, and log collect tools like fluentd, will not work with it, as it depends on tail and parser on the log file. We need to use other tools to collect the raw data files.

What we plan to do, is just create additional container, mount the bin data dir in RO mode, monitor the dir for bin data file changed and upload the file to remote sftp server.

In the bin-log collecting script, we use sftp command to upload file, and we use inotifywait to monitor the file changes. We expect there is sftp server running, and we can use sftp command to upload file to it, the sftp server should be enhanced with security, only allow upload, not download, and no ssh shell provided. [Here is a sftp pod demo, for your reference](../4.12/2024.06.sftp.pod.md). 

Now, the overall solution architecture is like this:

![](./dia/4.14.log.dir.pod.collect.files.drawio.png)

Please note, we only want to collect bin data file, which will create once, and not modified after created. For log file, we can use fluentd to collect it.

```bash

export BASE_DIR=${HOME}
mkdir -p ${BASE_DIR}/data/install/openshift

# we need a sa, that can read pod information
# the default sa, do have such privilege
cat << EOF > ${BASE_DIR}/data/install/sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: pod-info-sa
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
subjects:
- kind: ServiceAccount
  name: pod-info-sa
  namespace: llm-demo
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
EOF

oc apply -f ${BASE_DIR}/data/install/sa.yaml -n llm-demo

# oc delete -f ${BASE_DIR}/data/install/sa.yaml -n llm-demo

# we use a startup script to get pod information
# and store it to log dir, so admin can know the pod information
cat << 'EOF' > ${BASE_DIR}/data/install/pod-info.sh
#!/bin/sh

touch /mnt/pod.name.$POD_NAME
touch /mnt/pod.namespace.$NAMESPACE

oc get pod $POD_NAME -n $NAMESPACE -o yaml > /mnt/pod-deploy.txt 2>&1

oc describe pod $POD_NAME -n $NAMESPACE  > /mnt/pod-describe.txt 2>&1

# oc get pod $POD_NAME -n $NAMESPACE -o jsonpath='{.spec.containers[*].name}' > /mnt/pod-containers.txt 2>&1

# create a directory for each container
for var_container in $(oc get pod $POD_NAME -n $NAMESPACE -o jsonpath='{.spec.containers[*].name}'); do 
  mkdir -p /mnt/$var_container
done
EOF

# we use a startup script to get pod information
# and store it to log dir, so admin can know the pod information
cat << 'EOF' > ${BASE_DIR}/data/install/log-collect.sh
#!/bin/bash

# Define the local directory to monitor and the remote server directory
LOCAL_DIR="$ENV_LOCAL_DIR"
REMOTE_DIR="$ENV_REMOTE_DIR"

# Get the SFTP username and password from environment variables
SFTP_USER="$ENV_SFTP_USER"
SFTP_PASSWORD="$ENV_SFTP_PASSWORD"
SFTP_HOST="$ENV_SFTP_HOST" # Replace with your actual remote server host


# Create a temporary batch file to check and create the remote directory
INIT_BATCH_FILE=$(mktemp)
echo "mkdir $REMOTE_DIR" > $INIT_BATCH_FILE

# Use sshpass with sftp to automatically login and execute the batch file to ensure the remote directory exists
sshpass -p "$SFTP_PASSWORD" sftp -oBatchMode=no -b $INIT_BATCH_FILE -o StrictHostKeyChecking=no -oUserKnownHostsFile=/tmp/known_hosts $SFTP_USER@$SFTP_HOST

# Remove the temporary batch file for directory initialization
rm $INIT_BATCH_FILE

# Loop indefinitely
while true; do
  # Use inotifywait to monitor the directory for any change events and read the output
  inotifywait -e modify -e create -e move --format '%w%f' $LOCAL_DIR | while read FILE; do
    # Check if the file exists before attempting to sync, as it might have been deleted
    if [ -e "$FILE" ]; then
      # Create a temporary batch file for sftp commands
      BATCH_FILE=$(mktemp)
      echo "put $FILE $REMOTE_DIR" > $BATCH_FILE
      
      # Use sshpass with sftp to automatically login using the provided username and password
      # and execute the batch file
      sshpass -p"$SFTP_PASSWORD" sftp -oBatchMode=no -b $BATCH_FILE -o StrictHostKeyChecking=no -oUserKnownHostsFile=/tmp/known_hosts $SFTP_USER@$SFTP_HOST
      
      # Remove the temporary batch file
      rm $BATCH_FILE
    fi
  done
done
EOF

oc delete configmap wzh-script-configmap -n llm-demo
# inject the script into a configmap
oc create configmap wzh-script-configmap \
  --from-file=${BASE_DIR}/data/install/pod-info.sh \
  --from-file=${BASE_DIR}/data/install/log-collect.sh \
  -n llm-demo

# oc delete configmap wzh-script-configmap -n llm-demo

oc describe sa pod-info-sa -n llm-demo | grep Tokens | awk '{print $2}'
# pod-info-sa-token-cxn8c

VAR_TOKEN=$(oc describe sa pod-info-sa -n llm-demo | grep Tokens | awk '{print $2}')

# create the demo pod
cat << EOF > ${BASE_DIR}/data/install/demo.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: wzh-demo-service
spec:
  ports:
    - name: service-port
      port: 80
      protocol: TCP
      targetPort: 8080
  selector:
    app: pod-description-writer

---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: wzh-demo
spec:
  to:
    kind: Service
    name: wzh-demo-service
  port:
    targetPort: service-port

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pod-description-writer
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pod-description-writer
  template:
    metadata:
      labels:
        app: pod-description-writer
    spec:
      serviceAccountName: pod-info-sa
      # do not auto mount sa token to container, for security reason
      automountServiceAccountToken: false
      volumes:
      - name: pod-local-log
        ephemeral:
          volumeClaimTemplate:
            metadata:
              labels:
                type: my-frontend-volume
            spec:
              accessModes: [ "ReadWriteOnce" ]
              storageClassName: "local-log-hostpath-csi"
              resources:
                requests:
                  # storage size does not matter for hostpath, becuase it will use all of the disk free space.
                  # but it must be set to actual required size for other storage class
                  storage: 1Gi
      - name: service-account-token
        secret:
          secretName: $VAR_TOKEN
      - name: wzh-script-volume
        configMap:
          name: wzh-script-configmap
      initContainers:
      - name: write-pod-description
        image: registry.redhat.io/openshift4/ose-cli:v4.15
        volumeMounts:
        - name: pod-local-log
          mountPath: /mnt
        - name: service-account-token
          mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          readOnly: true
        - name: wzh-script-volume
          mountPath: /wzh-scripts
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        command: ['bash', '/wzh-scripts/pod-info.sh']
      containers:
      - name: my-app-heap-dump
        image: quay.io/wangzheng422/qimgs:simple-java-http-server-threads-2024.05.30.v04
        volumeMounts:
        - name: pod-local-log
          # subpath should support variable, but we can not get container name
          # so we use a fixed name
          subPath: my-app-heap-dump
          mountPath: /wzh-log/
      - name: log-collector-for-app-one
        # the image used here is not optimized for rpms installed, it is just for demo.
        image: quay.io/wangzheng422/qimgs:rocky9-test-2024.06.17.v01
        command: ['bash', '/wzh-scripts/log-collect.sh']
        volumeMounts:
        - name: wzh-script-volume
          mountPath: /wzh-scripts
        - name: pod-local-log
          subPath: my-app-heap-dump
          mountPath: /wzh-log/
          readOnly: true
        env:
        - name: POD_UID
          valueFrom:
            fieldRef:
              fieldPath: metadata.uid
        - name: ENV_LOCAL_DIR
          value: /wzh-log/
        - name: ENV_REMOTE_DIR
          value: upload/\$(POD_UID)
        - name: ENV_SFTP_USER
          value: foo
        - name: ENV_SFTP_PASSWORD
          value: pass
        - name: ENV_SFTP_HOST
          value: sftp-service
EOF

oc apply -f ${BASE_DIR}/data/install/demo.yaml -n llm-demo

# oc delete -f ${BASE_DIR}/data/install/demo.yaml -n llm-demo

# get the route of the demo app, and extract the url from the route resource
oc get route wzh-demo -n llm-demo -o jsonpath='{.spec.host}{"\n"}'
# wzh-demo-llm-demo.apps.cluster-gtmcf.sandbox301.opentlc.com

VAR_ROUTE=$(oc get route wzh-demo -n llm-demo -o jsonpath='{.spec.host}{"\n"}')

# cause the java app to dump
curl -k http://${VAR_ROUTE}/dumpheap

# cause the java app to crash
curl -k http://${VAR_ROUTE}/crashapi

# cause the java app to dump again
curl -k http://${VAR_ROUTE}/dumpheap



# check on sftp server, it is a pod in our case, 
# we can see the uploaded files
oc exec -it sftp-pod -- ls -lR /home/foo/upload
/home/foo/upload:
total 24
d-wx-wx---+ 2 foo users 4096 Jun 17 08:49 492f2920-79ff-423f-ba3f-d6c80721fa88

/home/foo/upload/492f2920-79ff-423f-ba3f-d6c80721fa88:
total 52328
--w-------. 1 foo users 26767535 Jun 17 08:49 heap-dump_2024-06-17_08-49-01.hprof
--w-------. 1 foo users 26808556 Jun 17 08:49 heap-dump_2024-06-17_08-49-56.hprof


```

# trigger java heap dump based on pod memory usage

Customer also needs an automatically way, to trigger the java heap dump, when the java app run out of the pod memory. We will use liveness pod of the container, and run a script to check the memory usage, and trigger the java heap dump.

If you have multiple liveness check logic, you have to combine them into one script.

```bash


export BASE_DIR=${HOME}
mkdir -p ${BASE_DIR}/data/install/openshift

# we need a sa, that can read pod information
# the default sa, do have such privilege
cat << EOF > ${BASE_DIR}/data/install/sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: pod-info-sa
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
subjects:
- kind: ServiceAccount
  name: pod-info-sa
  namespace: llm-demo
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
EOF

oc apply -f ${BASE_DIR}/data/install/sa.yaml -n llm-demo

# oc delete -f ${BASE_DIR}/data/install/sa.yaml -n llm-demo

# we use a startup script to get pod information
# and store it to log dir, so admin can know the pod information
cat << 'EOF' > ${BASE_DIR}/data/install/pod-info.sh
#!/bin/sh

touch /mnt/pod.name.$POD_NAME
touch /mnt/pod.namespace.$NAMESPACE

oc get pod $POD_NAME -n $NAMESPACE -o yaml > /mnt/pod-deploy.txt 2>&1

oc describe pod $POD_NAME -n $NAMESPACE  > /mnt/pod-describe.txt 2>&1

# oc get pod $POD_NAME -n $NAMESPACE -o jsonpath='{.spec.containers[*].name}' > /mnt/pod-containers.txt 2>&1

# create a directory for each container
for var_container in $(oc get pod $POD_NAME -n $NAMESPACE -o jsonpath='{.spec.containers[*].name}'); do 
  mkdir -p /mnt/$var_container
done
EOF

# we use a startup script to get pod information
# and store it to log dir, so admin can know the pod information
cat << 'EOF' > ${BASE_DIR}/data/install/log-collect.sh
#!/bin/bash

# Define the local directory to monitor and the remote server directory
LOCAL_DIR="$ENV_LOCAL_DIR"
REMOTE_DIR="$ENV_REMOTE_DIR"

# Get the SFTP username and password from environment variables
SFTP_USER="$ENV_SFTP_USER"
SFTP_PASSWORD="$ENV_SFTP_PASSWORD"
SFTP_HOST="$ENV_SFTP_HOST" # Replace with your actual remote server host


# Create a temporary batch file to check and create the remote directory
INIT_BATCH_FILE=$(mktemp)
echo "mkdir $REMOTE_DIR" > $INIT_BATCH_FILE

# Use sshpass with sftp to automatically login and execute the batch file to ensure the remote directory exists
sshpass -p "$SFTP_PASSWORD" sftp -oBatchMode=no -b $INIT_BATCH_FILE -o StrictHostKeyChecking=no -oUserKnownHostsFile=/tmp/known_hosts $SFTP_USER@$SFTP_HOST

# Remove the temporary batch file for directory initialization
rm $INIT_BATCH_FILE

# Loop indefinitely
while true; do
  # Use inotifywait to monitor the directory for any change events and read the output
  inotifywait -e modify -e create -e move --format '%w%f' $LOCAL_DIR | while read FILE; do
    # Check if the file exists before attempting to sync, as it might have been deleted
    if [ -e "$FILE" ]; then
      # Create a temporary batch file for sftp commands
      BATCH_FILE=$(mktemp)
      echo "put $FILE $REMOTE_DIR" > $BATCH_FILE
      
      # Use sshpass with sftp to automatically login using the provided username and password
      # and execute the batch file
      sshpass -p"$SFTP_PASSWORD" sftp -oBatchMode=no -b $BATCH_FILE -o StrictHostKeyChecking=no -oUserKnownHostsFile=/tmp/known_hosts $SFTP_USER@$SFTP_HOST
      
      # Remove the temporary batch file
      rm $BATCH_FILE
    fi
  done
done
EOF

# we use a script to run in liveness check
# go check the memory usage, if it is over the limit, trigger the java heap dump
cat << 'EOF' > ${BASE_DIR}/data/install/liveness-check.sh
#!/bin/bash
# Set the memory threshold to trigger a heap dump
MEMORY_THRESHOLD=90 # in percentage

# Get the total memory limit from the cgroup
TOTAL_MEMORY=$(cat /sys/fs/cgroup/memory/memory.limit_in_bytes)

# Calculate the threshold in bytes
THRESHOLD_BYTES=$((TOTAL_MEMORY * MEMORY_THRESHOLD / 100))

# Get the current memory usage
CURRENT_MEMORY=$(cat /sys/fs/cgroup/memory/memory.usage_in_bytes)

# Check if the current memory usage exceeds the threshold
if [ "$CURRENT_MEMORY" -ge "$THRESHOLD_BYTES" ]; then
  # Get the Java process ID
  JAVA_PID=$(pgrep -f java)

  # Trigger a heap dump
  jcmd $JAVA_PID GC.heap_dump /wzh-log/heapdump.hprof

  # Optionally, copy the heap dump to a persistent storage or another pod
  # kubectl cp <namespace>/<pod-name>:/path/to/heapdump.hprof /local/path

  # Exit with an error code to restart the pod
  # exit 1
fi

# Exit with a success code
exit 0
EOF

oc delete configmap wzh-script-configmap -n llm-demo
# inject the script into a configmap
oc create configmap wzh-script-configmap \
  --from-file=${BASE_DIR}/data/install/pod-info.sh \
  --from-file=${BASE_DIR}/data/install/log-collect.sh \
  --from-file=${BASE_DIR}/data/install/liveness-check.sh \
  -n llm-demo

# oc delete configmap wzh-script-configmap -n llm-demo

oc describe sa pod-info-sa -n llm-demo | grep Tokens | awk '{print $2}'
# pod-info-sa-token-cxn8c

VAR_TOKEN=$(oc describe sa pod-info-sa -n llm-demo | grep Tokens | awk '{print $2}')

# create the demo pod
cat << EOF > ${BASE_DIR}/data/install/demo.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: wzh-demo-service
spec:
  ports:
    - name: service-port
      port: 80
      protocol: TCP
      targetPort: 8080
  selector:
    app: pod-description-writer

---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: wzh-demo
spec:
  to:
    kind: Service
    name: wzh-demo-service
  port:
    targetPort: service-port

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pod-description-writer
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pod-description-writer
  template:
    metadata:
      labels:
        app: pod-description-writer
    spec:
      serviceAccountName: pod-info-sa
      # do not auto mount sa token to container, for security reason
      automountServiceAccountToken: false
      volumes:
      - name: pod-local-log
        ephemeral:
          volumeClaimTemplate:
            metadata:
              labels:
                type: my-frontend-volume
            spec:
              accessModes: [ "ReadWriteOnce" ]
              storageClassName: "local-log-hostpath-csi"
              resources:
                requests:
                  # storage size does not matter for hostpath, becuase it will use all of the disk free space.
                  # but it must be set to actual required size for other storage class
                  storage: 1Gi
      - name: service-account-token
        secret:
          secretName: $VAR_TOKEN
      - name: wzh-script-volume
        configMap:
          name: wzh-script-configmap
      initContainers:
      - name: write-pod-description
        image: registry.redhat.io/openshift4/ose-cli:v4.15
        volumeMounts:
        - name: pod-local-log
          mountPath: /mnt
        - name: service-account-token
          mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          readOnly: true
        - name: wzh-script-volume
          mountPath: /wzh-scripts
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        command: ['bash', '/wzh-scripts/pod-info.sh']
      containers:
      - name: my-app-heap-dump
        image: quay.io/wangzheng422/qimgs:simple-java-http-server-threads-2024.05.30.v04
        livenessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - /wzh-scripts/liveness-check.sh
          initialDelaySeconds: 60
          periodSeconds: 300        
        volumeMounts:
        - name: pod-local-log
          # subpath should support variable, but we can not get container name
          # so we use a fixed name
          subPath: my-app-heap-dump
          mountPath: /wzh-log/
        - name: wzh-script-volume
          subPath: liveness-check.sh
          mountPath: /wzh-scripts/liveness-check.sh      
      - name: log-collector-for-app-one
        # the image used here is not optimized for rpms installed, it is just for demo.
        image: quay.io/wangzheng422/qimgs:rocky9-test-2024.06.17.v01
        command: ['bash', '/wzh-scripts/log-collect.sh']
        volumeMounts:
        - name: wzh-script-volume
          mountPath: /wzh-scripts
        - name: pod-local-log
          subPath: my-app-heap-dump
          mountPath: /wzh-log/
          readOnly: true
        env:
        - name: POD_UID
          valueFrom:
            fieldRef:
              fieldPath: metadata.uid
        - name: ENV_LOCAL_DIR
          value: /wzh-log/
        - name: ENV_REMOTE_DIR
          value: upload/\$(POD_UID)
        - name: ENV_SFTP_USER
          value: foo
        - name: ENV_SFTP_PASSWORD
          value: pass
        - name: ENV_SFTP_HOST
          value: sftp-service
EOF

oc apply -f ${BASE_DIR}/data/install/demo.yaml -n llm-demo

# oc delete -f ${BASE_DIR}/data/install/demo.yaml -n llm-demo

# get the route of the demo app, and extract the url from the route resource
oc get route wzh-demo -n llm-demo -o jsonpath='{.spec.host}{"\n"}'
# wzh-demo-llm-demo.apps.cluster-gtmcf.sandbox301.opentlc.com

VAR_ROUTE=$(oc get route wzh-demo -n llm-demo -o jsonpath='{.spec.host}{"\n"}')

# cause the java app to dump
curl -k http://${VAR_ROUTE}/dumpheap

# cause the java app to crash
curl -k http://${VAR_ROUTE}/crashapi

# cause the java app to dump again
curl -k http://${VAR_ROUTE}/dumpheap



# check on sftp server, it is a pod in our case, 
# we can see the uploaded files
oc exec -it sftp-pod -- ls -lR /home/foo/upload
/home/foo/upload:
total 24
d-wx-wx---+ 2 foo users 4096 Jun 17 08:49 492f2920-79ff-423f-ba3f-d6c80721fa88

/home/foo/upload/492f2920-79ff-423f-ba3f-d6c80721fa88:
total 52328
--w-------. 1 foo users 26767535 Jun 17 08:49 heap-dump_2024-06-17_08-49-01.hprof
--w-------. 1 foo users 26808556 Jun 17 08:49 heap-dump_2024-06-17_08-49-56.hprof


```

# end