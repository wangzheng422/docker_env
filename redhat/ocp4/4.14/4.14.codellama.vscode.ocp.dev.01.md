> [!CAUTION] 
> RETIRED
# (RETIRED) 在 openshift4 上部署 codellama 服务，实现 vscode 代码自动补全

本文继续上一篇文章，我们在 openshift4 上部署 codellama 服务，实现 vscode 代码自动补全和LLM会话。

# 实验准备

openshift机器需要部署
- hostpath provisioner
- nvidia gpu operator
- openshift odf

# 镜像制作

## S3 for codellama 13B

```bash
# on 105
mkdir -p /data/workspace/s3.codellama
cd /data/workspace/s3.codellama

rsync -P -ar /data/huggingface/CodeLlama-13b-Instruct-hf /data/workspace/s3.codellama/

rm -f /data/workspace/s3.codellama/CodeLlama-13b-Instruct-hf/*.safetensors

cd /data/workspace/s3.codellama

cat << 'EOF' > Dockerfile
FROM quay.io/cloudservices/minio:RELEASE.2021-06-17T00-10-46Z.hotfix.35a0912ff as minio-examples

EXPOSE 9000

ARG MODEL_DIR=/data1/models

USER root

RUN useradd -u 1000 -g 0 modelmesh
RUN mkdir -p ${MODEL_DIR}
RUN chown -R 1000:0 /data1 && \
    chgrp -R 0 /data1 && \
    chmod -R g=u /data1

COPY --chown=1000:0 CodeLlama-13b-Instruct-hf ${MODEL_DIR}/CodeLlama-13b-Instruct-hf

USER 1000

EOF

podman build -t quaylab.infra.wzhlab.top:7443/wangzheng422/qimgs:minio-codellama-13-instruct-hf -f Dockerfile .

podman push quaylab.infra.wzhlab.top:7443/wangzheng422/qimgs:minio-codellama-13-instruct-hf

```

### deploy on ocp

```bash

oc new-project llm-demo
oc label --overwrite ns llm-demo \
   pod-security.kubernetes.io/enforce=privileged

# on helper
S3_NAME='codellama'
S3_NS='llm-demo'
S3_IMAGE='quaylab.infra.wzhlab.top:7443/wangzheng422/qimgs:minio-codellama-13-instruct-hf'

cat << EOF > ${BASE_DIR}/data/install/s3-codellama.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: minio-${S3_NAME}
spec:
  ports:
    - name: minio-client-port
      port: 9000
      protocol: TCP
      targetPort: 9000
  selector:
    app: minio-${S3_NAME}

---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: s3-${S3_NAME}
spec:
  to:
    kind: Service
    name: minio-${S3_NAME}
  port:
    targetPort: 9000

---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: minio-${S3_NAME}
  name: minio-${S3_NAME}
spec:
  containers:
    - args:
        - server
        - /data1
      env:
        - name: MINIO_ACCESS_KEY
          value:  admin
        - name: MINIO_SECRET_KEY
          value: password
      image: ${S3_IMAGE}
      imagePullPolicy: IfNotPresent
      name: minio
      nodeSelector:
        kubernetes.io/hostname: "worker-01-demo"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
            drop:
            - ALL
        runAsNonRoot: true
        seccompProfile:
            type: RuntimeDefault

---
apiVersion: v1
kind: Secret
metadata:
  annotations:
    serving.kserve.io/s3-endpoint: minio-${S3_NAME}.${S3_NS}.svc:9000 # replace with your s3 endpoint e.g minio-service.kubeflow:9000
    serving.kserve.io/s3-usehttps: "0" # by default 1, if testing with minio you can set to 0
    serving.kserve.io/s3-region: "us-east-2"
    serving.kserve.io/s3-useanoncredential: "false" # omitting this is the same as false, if true will ignore provided credential and use anonymous credentials
  name: storage-config-${S3_NAME}
stringData:
  "AWS_ACCESS_KEY_ID": "admin"
  "AWS_SECRET_ACCESS_KEY": "password"

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: sa-${S3_NAME}
secrets:
- name: storage-config-${S3_NAME}

EOF

oc create -n llm-demo -f ${BASE_DIR}/data/install/s3-codellama.yaml

# oc delete -n llm-demo -f ${BASE_DIR}/data/install/s3-codellama.yaml

# open in browser to check
# http://s3-codellama-llm-demo.apps.demo-gpu.wzhlab.top/minio/models/
# http://s3-codellama-llm-demo.apps.demo-gpu.wzhlab.top/minio/models/CodeLlama-13b-Instruct-hf/


```

## S3 for tabby 13B

```bash
# on 105
cd /data/workspace/tabby

cat << 'EOF' > Dockerfile
FROM quay.io/cloudservices/minio:RELEASE.2021-06-17T00-10-46Z.hotfix.35a0912ff as minio-examples

EXPOSE 9000

ARG MODEL_DIR=/data1/

USER root

RUN useradd -u 1000 -g 0 modelmesh
RUN mkdir -p ${MODEL_DIR}
RUN chown -R 1000:0 /data1 && \
    chgrp -R 0 /data1 && \
    chmod -R g=u /data1

COPY --chown=1000:0 models ${MODEL_DIR}/models

USER 1000

EOF

podman build -t quaylab.infra.wzhlab.top:7443/wangzheng422/qimgs:minio-tabby -f Dockerfile .

podman push quaylab.infra.wzhlab.top:7443/wangzheng422/qimgs:minio-tabby

```

### deploy on ocp

```bash

# on helper
S3_NAME='tabby'
S3_NS='llm-demo'
S3_IMAGE='quaylab.infra.wzhlab.top:7443/wangzheng422/qimgs:minio-tabby'

cat << EOF > ${BASE_DIR}/data/install/s3-tabby.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: minio-${S3_NAME}
spec:
  ports:
    - name: minio-client-port
      port: 9000
      protocol: TCP
      targetPort: 9000
  selector:
    app: minio-${S3_NAME}

---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: s3-${S3_NAME}
spec:
  to:
    kind: Service
    name: minio-${S3_NAME}
  port:
    targetPort: 9000

---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: minio-${S3_NAME}
  name: minio-${S3_NAME}
spec:
  containers:
    - args:
        - server
        - /data1
      env:
        - name: MINIO_ACCESS_KEY
          value:  admin
        - name: MINIO_SECRET_KEY
          value: password
      image: ${S3_IMAGE}
      imagePullPolicy: IfNotPresent
      name: minio
      nodeSelector:
        kubernetes.io/hostname: "worker-01-demo"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
            drop:
            - ALL
        runAsNonRoot: true
        seccompProfile:
            type: RuntimeDefault

---
apiVersion: v1
kind: Secret
metadata:
  annotations:
    serving.kserve.io/s3-endpoint: minio-${S3_NAME}.${S3_NS}.svc:9000 # replace with your s3 endpoint e.g minio-service.kubeflow:9000
    serving.kserve.io/s3-usehttps: "0" # by default 1, if testing with minio you can set to 0
    serving.kserve.io/s3-region: "us-east-2"
    serving.kserve.io/s3-useanoncredential: "false" # omitting this is the same as false, if true will ignore provided credential and use anonymous credentials
  name: storage-config-${S3_NAME}
stringData:
  "AWS_ACCESS_KEY_ID": "admin"
  "AWS_SECRET_ACCESS_KEY": "password"

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: sa-${S3_NAME}
secrets:
- name: storage-config-${S3_NAME}

EOF

oc create -n llm-demo -f ${BASE_DIR}/data/install/s3-tabby.yaml

# oc delete -n llm-demo -f ${BASE_DIR}/data/install/s3-tabby.yaml

# open in browser to check
# http://s3-tabby-llm-demo.apps.demo-gpu.wzhlab.top/minio/models/
# http://s3-tabby-llm-demo.apps.demo-gpu.wzhlab.top/minio/models/TabbyML/

```

## image for fastchat

我们需要一个容器镜像，来承载fastchat的python 模块，来运行应用

```bash

# on vultr
mkdir -p /data/fastchat
cd /data/fastchat

cat << EOF > Dockerfile
FROM docker.io/nvidia/cuda:12.3.1-devel-rockylinux9

RUN dnf install -y python3 python3-pip

RUN python3 -m pip install --upgrade pip setuptools wheel

RUN pip3 install -U "fschat[model_worker,webui]"

EOF

podman build --squash -t quay.io/wangzheng422/qimgs:cuda-12.3.1-fastchat -f Dockerfile .

podman push quay.io/wangzheng422/qimgs:cuda-12.3.1-fastchat

# on helper
oc image mirror quay.io/wangzheng422/qimgs:cuda-12.3.1-fastchat quaylab.infra.wzhlab.top:7443/wangzheng422/qimgs:cuda-12.3.1-fastchat

```

## image for simple openai service

# 部署 codellama 服务

```bash

# on helper
S3_NAME='codellama'
S3_NS='llm-demo'
S3_IMAGE='quaylab.infra.wzhlab.top:7443/wangzheng422/qimgs:cuda-12.3.1-fastchat'

cat << EOF > ${BASE_DIR}/data/install/codellama.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: fastchat-controller
spec:
  ports:
    - name: fastchat-controller-port
      port: 21001
      protocol: TCP
      targetPort: 21001
  selector:
    app: fastchat-controller

---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: fastchat-controller
  name: fastchat-controller
spec:
  containers:
      - name: fastchat-controller
        command: [ "/bin/sh","-c","--" ]
        args: [" cd /root; python3 -m fastchat.serve.controller --host 0.0.0.0 --port 21001 "]
        image: ${S3_IMAGE}
        imagePullPolicy: IfNotPresent
        nodeSelector:
            kubernetes.io/hostname: "worker-01-demo"
        securityContext:
            privileged: true

---
apiVersion: v1
kind: Service
metadata:
  name: fastchat-api
spec:
  ports:
    - name: fastchat-api-port
      port: 8000
      protocol: TCP
      targetPort: 8000
  selector:
    app: fastchat-api

---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: fastchat-api
spec:
  to:
    kind: Service
    name: fastchat-api
  port:
    targetPort: 8000

---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: fastchat-api
  name: fastchat-api
spec:
  containers:
      - name: fastchat-api
        command: [ "/bin/sh","-c","--" ]
        args: [" cd /root; python3 -m fastchat.serve.openai_api_server  --controller-address fastchat-controller:21001 --host 0.0.0.0 --port 8000 "]
        image: ${S3_IMAGE}
        imagePullPolicy: IfNotPresent
        nodeSelector:
            kubernetes.io/hostname: "worker-01-demo"
        securityContext:
            privileged: true

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fastchat-worker
  labels:
    app: fastchat-worker
spec:
  replicas: 1
  selector:
    matchLabels:
      app: fastchat-worker
  template:
    metadata:
      labels:
        app: fastchat-worker
    spec:
      containers:
      - name: fastchat-worker
        command: [ "/bin/sh","-c","--" ]
        # args: [" cd /root; python3 -m fastchat.serve.openai_api_server  --controller-address fastchat-controller:21001 --host 0.0.0.0 --port 8000 "]
        args: [" tail -f /dev/null "] # just keep the container running
        image: ${S3_IMAGE}
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            nvidia.com/gpu:
        # volumeMounts:
        #   - mountPath: /hugepages
        #     name: hugepage
        #   - name: lib-modules
        #     mountPath: /lib/modules
EOF

oc create -n llm-demo -f ${BASE_DIR}/data/install/codellama.yaml

# oc delete -n llm-demo -f ${BASE_DIR}/data/install/codellama.yaml

```

# 部署 zabby 服务

# vscode 配置和使用