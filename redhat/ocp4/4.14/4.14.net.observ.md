# openshift 4.14 Network Observability

openshift 4.14 introduce eBPF as agent for netoberv, we try it out, and see how it works. We focus on egress IP senario, and want to see RTT value from backend service outside of cluster.

- https://docs.openshift.com/container-platform/4.14/observability/network_observability/installing-operators.html

here is the architecture of this lab:

![](dia/4.14.netobserv.arch.drawio.svg)

# install loki

RTT value need loki as backend, we install loki first.

- [Installing the Loki Operator](https://docs.openshift.com/container-platform/4.14/observability/network_observability/installing-operators.html#network-observability-loki-installation_network_observability)


## install a minio as backend

we need s3 storage, we will use minio as backend, and use local disk as storage.

deploy a minio for testing only, not for production. becuase official minio will enable https, it will bring so many trouble into app integration, we use a old version minio.

```bash

# oc new-project llm-demo
# oc label --overwrite ns llm-demo \
#    pod-security.kubernetes.io/enforce=privileged

oc new-project netobserv

# on helper
S3_NAME='netobserv'
S3_NS='netobserv'
S3_IMAGE='quay.io/cloudservices/minio:RELEASE.2021-06-17T00-10-46Z.hotfix.35a0912ff'

cat << EOF > ${BASE_DIR}/data/install/s3-codellama.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: minio-${S3_NAME}
spec:
  ports:
    - name: minio-client-port
      port: 9000
      protocol: TCP
      targetPort: 9000
  selector:
    app: minio-${S3_NAME}

---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: s3-${S3_NAME}
spec:
  to:
    kind: Service
    name: minio-${S3_NAME}
  port:
    targetPort: 9000

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: minio-${S3_NAME}-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 200Gi
  storageClassName: hostpath-csi

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: minio-${S3_NAME}
  labels:
    app: minio-${S3_NAME}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: minio-${S3_NAME}
  template:
    metadata:
      labels:
        app: minio-${S3_NAME}
    spec:
      containers:
        - args:
            - server
            - /data1
          env:
            - name: MINIO_ACCESS_KEY
              value:  admin
            - name: MINIO_SECRET_KEY
              value: redhatocp
          image: ${S3_IMAGE}
          imagePullPolicy: IfNotPresent
          name: minio
          nodeSelector:
            kubernetes.io/hostname: "worker-01-demo"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
                drop:
                - ALL
            runAsNonRoot: true
            seccompProfile:
                type: RuntimeDefault
          volumeMounts:
            - mountPath: "/data1"
              name: data
      volumes:
        - name: data 
          persistentVolumeClaim:
            claimName: minio-${S3_NAME}-pvc

EOF

oc create -n netobserv -f ${BASE_DIR}/data/install/s3-codellama.yaml

# oc delete -n netobserv -f ${BASE_DIR}/data/install/s3-codellama.yaml

# open in browser to check, and create bucket 'demo'
# http://s3-netobserv-netobserv.apps.demo-gpu.wzhlab.top/


```

## install loki operator

we have a s3 storage, and we will install loki operator.

![](imgs/2024-04-04-00-15-56.png)

```bash

# oc new-project netobserv

cat << EOF > ${BASE_DIR}/data/install/loki-netobserv.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: loki-s3 
stringData:
  access_key_id: admin
  access_key_secret: redhatocp
  bucketnames: demo
  endpoint: http://minio-netobserv.netobserv.svc.cluster.local:9000
  # region: eu-central-1

---
apiVersion: loki.grafana.com/v1
kind: LokiStack
metadata:
  name: loki
spec:
  size: 1x.demo
  storage:
    schemas:
    - version: v12
      effectiveDate: '2022-06-01'
    secret:
      name: loki-s3
      type: s3
  storageClassName: hostpath-csi
  tenants:
    mode: openshift-network
    openshift:
        adminGroups: 
        - cluster-admin

EOF

oc create --save-config -n netobserv -f ${BASE_DIR}/data/install/loki-netobserv.yaml

# to delete

# oc delete -n netobserv -f ${BASE_DIR}/data/install/loki-netobserv.yaml

# oc get pvc -n netobserv | grep loki- | awk '{print $1}' | xargs oc delete -n netobserv pvc

# run below, if reinstall
oc adm groups new cluster-admin

oc adm groups add-users cluster-admin admin

oc adm policy add-cluster-role-to-group cluster-admin cluster-admin

```

# install net observ

we will install net observ operator, the installation is simple, just follow the official document. If you use eBPF agent, there seems have bugs with the installation steps, it is better to restart nodes to make the ebpf agent function well.

![](imgs/2024-04-04-00-24-27.png)

![](imgs/2024-04-04-00-45-16.png)

![](imgs/2024-04-04-00-48-50.png)

## RTT tracing

enable rtt tracing, following official document.

- https://docs.openshift.com/container-platform/4.12/observability/network_observability/observing-network-traffic.html#network-observability-RTT_nw-observe-network-traffic

![](imgs/2024-04-07-13-49-51.png)

or if you want to change yaml directly

```yaml
apiVersion: flows.netobserv.io/v1beta2
kind: FlowCollector
metadata:
  name: cluster
spec:
  namespace: netobserv
  deploymentModel: Direct
  agent:
    type: eBPF
    ebpf:
      features:
       - FlowRTT 
```

# try it out

## RRT

we can see RRT based on each flow. At this point, we did not introduct network latency on backend service, so the RTT is very low.

<!-- ![](imgs/2024-04-07-16-11-33.png) -->

![](imgs/2024-04-08-15-36-50.png)

## deploy egress IP

next, we will deploy egress IP on worker-02, and make traffic from worker-01 to worker-02, and then backend-service, and see the RTT value.

```bash

# label a node to host egress ip
oc label nodes worker-02-demo k8s.ovn.org/egress-assignable="" 

# label a namespace with env
oc label ns llm-demo env=egress-demo


# create a egress ip
cat << EOF > ${BASE_DIR}/data/install/egressip.yaml
apiVersion: k8s.ovn.org/v1
kind: EgressIP
metadata:
  name: egressips-prod
spec:
  egressIPs:
  - 172.21.6.22
  namespaceSelector:
    matchLabels:
      env: egress-demo
EOF

oc create --save-config -f ${BASE_DIR}/data/install/egressip.yaml

# oc delete -f ${BASE_DIR}/data/install/egressip.yaml

oc get egressip -o json | jq -r '.items[] | [.status.items[].egressIP, .status.items[].node] | @tsv'
# 172.21.6.22     worker-02-demo

```

## make traffic and see result

then, we create backend http service, and introduce network latency by 1s.

```bash

# on backend host, 172.21.6.8
# create a web service
python3 -m http.server 13000
# ...
# 172.21.6.22 - - [08/Apr/2024 12:22:39] "GET / HTTP/1.1" 200 -
# 172.21.6.22 - - [08/Apr/2024 12:22:42] "GET / HTTP/1.1" 200 -
# 172.21.6.22 - - [08/Apr/2024 12:22:45] "GET / HTTP/1.1" 200 -
# ...

# using tc to traffic control / delay 1s
dnf install -y kernel-modules-extra # to install netem module
dnf install -y /usr/sbin/tc

iptables -A OUTPUT -t mangle -p tcp --sport 13000 -j MARK --set-mark 13000

sudo tc qdisc del dev ens192 root
sudo tc qdisc add dev ens192 root handle 1: htb
sudo tc class add dev ens192 parent 1: classid 1:1 htb rate 100mbit
sudo tc filter add dev ens192 protocol ip parent 1:0 prio 1 handle 13000 fw flowid 1:1
sudo tc qdisc add dev ens192 parent 1:1 handle 10: netem delay 1s

sudo tc qdisc show dev ens192
# qdisc htb 1: root refcnt 3 r2q 10 default 0 direct_packets_stat 453 direct_qlen 1000
# qdisc netem 10: parent 1:1 limit 1000 delay 1s

```

and then, we create testing pod, and curl from the pod to backend service

```bash

# go back to helper
# create a dummy pod
cat << EOF > ${BASE_DIR}/data/install/demo1.yaml
---
kind: Pod
apiVersion: v1
metadata:
  name: wzh-demo-pod
spec:
  nodeSelector:
    kubernetes.io/hostname: 'worker-01-demo'
  restartPolicy: Always
  containers:
    - name: demo1
      image: >- 
        quay.io/wangzheng422/qimgs:rocky9-test
      env:
        - name: key
          value: value
      command: [ "/bin/bash", "-c", "--" ]
      args: [ "tail -f /dev/null" ]
      # imagePullPolicy: Always
EOF

oc apply -n llm-demo -f demo1.yaml

oc exec -n llm-demo wzh-demo-pod -it -- bash
# in the container terminal
while true; do curl http://172.21.6.8:13000 && sleep 1; done;

```

and you get the result like this, you can see the RTT become 1s.:

![](imgs/2024-04-08-12-00-50.png)

after setting the columnes, we can see something interesting.

![](imgs/2024-04-08-12-03-32.png)

as we can see 4 mac address, lets check what they are:
- 0A:58:0A:85:00:01 -> RouterToSwitch, on worker-01
- 0A:58:0A:85:00:2B -> pod's ip, on worker-01
- 0A:58:64:58:00:03 -> RouterToTransitSwitch, on worker-01
- 0A:58:64:58:00:04 -> TransitSwitchToRouter, on worker-02


```bash

VAR_POD=`oc get pod -n openshift-ovn-kubernetes -o wide | grep worker-01-demo | awk '{print $1}'`

oc exec -it ${VAR_POD} -c ovn-controller -n openshift-ovn-kubernetes -- ovn-nbctl show | grep -i 0A:58:0A:85:00:01 -A 10 -B 10
# router 032109ba-320f-4790-a3e6-44f305fa2397 (ovn_cluster_router)
#     port rtoj-ovn_cluster_router
#         mac: "0a:58:64:40:00:01"
#         networks: ["100.64.0.1/16"]
#     port rtos-worker-01-demo
#         mac: "0a:58:0a:85:00:01"
#         networks: ["10.133.0.1/23"]
#         gateway chassis: [163c3827-a827-450e-931c-65eaab2d67d8]
#     port rtots-worker-01-demo
#         mac: "0a:58:64:58:00:03"
#         networks: ["100.88.0.3/16"]


oc exec -it ${VAR_POD} -c ovn-controller -n openshift-ovn-kubernetes -- ovn-nbctl show | grep -i 0A:58:0A:85:00:2B -A 10 -B 20
# switch 2ae42bdd-9dde-43d6-9c1d-46b77298aebf (worker-01-demo)
#     port llm-demo_wzh-demo-pod
#         addresses: ["0a:58:0a:85:00:2b 10.133.0.43"]

oc exec -it ${VAR_POD} -c ovn-controller -n openshift-ovn-kubernetes -- ovn-nbctl show | grep -i 0A:58:64:58:00:03 -A 10 -B 20
# router 032109ba-320f-4790-a3e6-44f305fa2397 (ovn_cluster_router)
#     port rtoj-ovn_cluster_router
#         mac: "0a:58:64:40:00:01"
#         networks: ["100.64.0.1/16"]
#     port rtos-worker-01-demo
#         mac: "0a:58:0a:85:00:01"
#         networks: ["10.133.0.1/23"]
#         gateway chassis: [163c3827-a827-450e-931c-65eaab2d67d8]
#     port rtots-worker-01-demo
#         mac: "0a:58:64:58:00:03"
#         networks: ["100.88.0.3/16"]

oc exec -it ${VAR_POD} -c ovn-controller -n openshift-ovn-kubernetes -- ovn-nbctl show | grep -i 0A:58:64:58:00:04 -A 10 -B 20
# switch ceda4a6b-cac9-49d8-bc7c-7a419e7c51bd (transit_switch)
#     port tstor-worker-01-demo
#         type: router
#         router-port: rtots-worker-01-demo
#     port tstor-worker-02-demo
#         type: remote
#         addresses: ["0a:58:64:58:00:04 100.88.0.4/16"]
#     port tstor-master-01-demo
#         type: remote
#         addresses: ["0a:58:64:58:00:02 100.88.0.2/16"]

oc exec -it ${VAR_POD} -c ovn-controller -n openshift-ovn-kubernetes -- ovn-nbctl list Logical_Router_Port | grep -i 0A:58:0A:85:00:01 -A 10 -B 10
# _uuid               : 53b1cec8-ff5a-4fba-896d-da3bdbea07c1
# enabled             : []
# external_ids        : {}
# gateway_chassis     : [4b714c8d-1508-4a28-ab25-c8df85457fbb]
# ha_chassis_group    : []
# ipv6_prefix         : []
# ipv6_ra_configs     : {}
# mac                 : "0a:58:0a:85:00:01"
# name                : rtos-worker-01-demo
# networks            : ["10.133.0.1/23"]
# options             : {}
# peer                : []
# status              : {hosting-chassis="163c3827-a827-450e-931c-65eaab2d67d8"}


oc exec -it ${VAR_POD} -c ovn-controller -n openshift-ovn-kubernetes -- ovn-nbctl list Logical_Switch_Port | grep -i 0A:58:0A:85:00:2B -A 10 -B 10
# _uuid               : 641b3330-2216-4a6a-abbc-ecea5c888adc
# addresses           : ["0a:58:0a:85:00:2b 10.133.0.43"]
# dhcpv4_options      : []
# dhcpv6_options      : []
# dynamic_addresses   : []
# enabled             : []
# external_ids        : {namespace=llm-demo, pod="true"}
# ha_chassis_group    : []
# mirror_rules        : []
# name                : llm-demo_wzh-demo-pod
# options             : {iface-id-ver="20b12566-0596-4b45-8ed6-9bd4ea68c649", requested-chassis=worker-01-demo}
# parent_name         : []
# port_security       : ["0a:58:0a:85:00:2b 10.133.0.43"]
# tag                 : []
# tag_request         : []
# type                : ""
# up                  : true


oc exec -it ${VAR_POD} -c ovn-controller -n openshift-ovn-kubernetes -- ovn-nbctl list Logical_Router_Port | grep -i 0A:58:64:58:00:03 -A 10 -B 10
# _uuid               : 5e750f8b-0951-4b5f-b415-a6f870dfc3a3
# enabled             : []
# external_ids        : {}
# gateway_chassis     : []
# ha_chassis_group    : []
# ipv6_prefix         : []
# ipv6_ra_configs     : {}
# mac                 : "0a:58:64:58:00:03"
# name                : rtots-worker-01-demo
# networks            : ["100.88.0.3/16"]
# options             : {mcast_flood="true"}
# peer                : []
# status              : {}

oc exec -it ${VAR_POD} -c ovn-controller -n openshift-ovn-kubernetes -- ovn-nbctl list Logical_Switch_Port | grep -i 0A:58:64:58:00:04 -A 10 -B 10
# _uuid               : 225dc4bd-6fe0-4a49-a61f-ab20576fb159
# addresses           : ["0a:58:64:58:00:04 100.88.0.4/16"]
# dhcpv4_options      : []
# dhcpv6_options      : []
# dynamic_addresses   : []
# enabled             : []
# external_ids        : {node=worker-02-demo}
# ha_chassis_group    : []
# mirror_rules        : []
# name                : tstor-worker-02-demo
# options             : {requested-tnl-key="4"}
# parent_name         : []

oc exec -it ${VAR_POD} -c ovn-controller -n openshift-ovn-kubernetes -- ovn-nbctl list Logical_Router | grep name
# name                : GR_worker-01-demo
# name                : ovn_cluster_router

oc exec -it ${VAR_POD} -c ovn-controller -n openshift-ovn-kubernetes -- ovn-nbctl lr-list
# f462fab4-9a08-4dec-ab6b-2576c31908e2 (GR_worker-01-demo)
# 032109ba-320f-4790-a3e6-44f305fa2397 (ovn_cluster_router)

oc exec -it ${VAR_POD} -c ovn-controller -n openshift-ovn-kubernetes -- ovn-nbctl lr-nat-list GR_worker-01-demo
# TYPE             GATEWAY_PORT          EXTERNAL_IP        EXTERNAL_PORT    LOGICAL_IP          EXTERNAL_MAC         LOGICAL_PORT
# snat                                   172.21.6.26                         10.133.0.3
# snat                                   172.21.6.26                         10.133.0.6
# snat                                   172.21.6.26                         10.133.0.103
# snat                                   172.21.6.26                         10.133.0.5
# snat                                   172.21.6.26                         10.133.0.10
# snat                                   172.21.6.26                         10.133.0.20
# snat                                   172.21.6.26                         10.133.0.96
# snat                                   172.21.6.26                         10.133.0.7
# snat                                   172.21.6.26                         10.133.0.11
# snat                                   172.21.6.26                         10.133.0.89
# snat                                   172.21.6.26                         10.133.0.9
# snat                                   172.21.6.26                         10.133.0.91
# snat                                   172.21.6.26                         10.133.0.4
# snat                                   172.21.6.26                         10.133.0.87
# snat                                   172.21.6.26                         10.133.0.92
# snat                                   172.21.6.26                         10.133.0.95
# snat                                   172.21.6.26                         10.133.0.83
# snat                                   172.21.6.26                         10.133.0.12
# snat                                   172.21.6.26                         10.133.0.8
# snat                                   172.21.6.26                         10.133.0.118
# snat                                   172.21.6.26                         10.133.0.86
# snat                                   172.21.6.26                         100.64.0.3
# snat                                   172.21.6.26                         10.133.0.88
# snat                                   172.21.6.26                         10.133.0.13
# snat                                   172.21.6.26                         10.133.0.43
# snat                                   172.21.6.26                         10.133.0.90
# snat                                   172.21.6.26                         10.133.0.93

oc exec -it ${VAR_POD} -c ovn-controller -n openshift-ovn-kubernetes -- ovn-sbctl dump-flows | grep 172.21.6.22
# no result


# search on worker-02
VAR_POD=`oc get pod -n openshift-ovn-kubernetes -o wide | grep worker-02-demo | awk '{print $1}'`


oc exec -it ${VAR_POD} -c ovn-controller -n openshift-ovn-kubernetes -- ovn-nbctl lr-list
# 373ab1b4-7948-4370-a5ab-17f9f3d4b742 (GR_worker-02-demo)
# b23e792d-a79a-47b5-a4c3-c2716ffdb55b (ovn_cluster_router)


oc exec -it ${VAR_POD} -c ovn-controller -n openshift-ovn-kubernetes -- ovn-nbctl lr-nat-list GR_worker-02-demo
# TYPE             GATEWAY_PORT          EXTERNAL_IP        EXTERNAL_PORT    LOGICAL_IP          EXTERNAL_MAC         LOGICAL_PORT
# snat                                   172.21.6.22                         10.133.0.43
# snat                                   172.21.6.27                         10.134.0.5
# snat                                   172.21.6.27                         10.134.0.12
# snat                                   172.21.6.27                         100.64.0.4
# snat                                   172.21.6.27                         10.134.0.4
# snat                                   172.21.6.27                         10.134.0.7
# snat                                   172.21.6.27                         10.134.0.3
# snat                                   172.21.6.27                         10.134.0.6
# snat                                   172.21.6.27                         10.134.0.8

# check mac address again
oc exec -it ${VAR_POD} -c ovn-controller -n openshift-ovn-kubernetes -- ovn-nbctl show | grep -i 0A:58:0A:85:00:01 -A 10 -B 10
# no result

oc exec -it ${VAR_POD} -c ovn-controller -n openshift-ovn-kubernetes -- ovn-nbctl show | grep -i 0A:58:0A:85:00:2B -A 10 -B 20
# no result

oc exec -it ${VAR_POD} -c ovn-controller -n openshift-ovn-kubernetes -- ovn-nbctl show | grep -i 0A:58:64:58:00:03 -A 10 -B 20
# switch 858be777-d262-4db8-9c02-7ed86d85d8a6 (transit_switch)
#     port tstor-worker-02-demo
#         type: router
#         router-port: rtots-worker-02-demo
#     port tstor-worker-01-demo
#         type: remote
#         addresses: ["0a:58:64:58:00:03 100.88.0.3/16"]
#     port tstor-master-01-demo
#         type: remote
#         addresses: ["0a:58:64:58:00:02 100.88.0.2/16"]

oc exec -it ${VAR_POD} -c ovn-controller -n openshift-ovn-kubernetes -- ovn-nbctl show | grep -i 0A:58:64:58:00:04 -A 10 -B 20
# router b23e792d-a79a-47b5-a4c3-c2716ffdb55b (ovn_cluster_router)
#     port rtots-worker-02-demo
#         mac: "0a:58:64:58:00:04"
#         networks: ["100.88.0.4/16"]
#     port rtos-worker-02-demo
#         mac: "0a:58:0a:86:00:01"
#         networks: ["10.134.0.1/23"]
#         gateway chassis: [0e25df85-8df7-451a-bc4e-e59173042ccd]
#     port rtoj-ovn_cluster_router
#         mac: "0a:58:64:40:00:01"
#         networks: ["100.64.0.1/16"]


oc exec -it ${VAR_POD} -c ovn-controller -n openshift-ovn-kubernetes -- ovn-sbctl dump-flows | grep 172.21.6.22
  # table=3 (lr_in_ip_input     ), priority=90   , match=(arp.op == 1 && arp.tpa == 172.21.6.22), action=(eth.dst = eth.src; eth.src = xreg0[0..47]; arp.op = 2; /* ARP reply */ arp.tha = arp.sha; arp.sha = xreg0[0..47]; arp.tpa <-> arp.spa; outport = inport; flags.loopback = 1; output;)
  # table=4 (lr_in_unsnat       ), priority=90   , match=(ip && ip4.dst == 172.21.6.22), action=(ct_snat;)
  # table=3 (lr_out_snat        ), priority=33   , match=(ip && ip4.src == 10.133.0.43 && (!ct.trk || !ct.rpl)), action=(ct_snat(172.21.6.22);)
  # table=27(ls_in_l2_lkup      ), priority=80   , match=(flags[1] == 0 && arp.op == 1 && arp.tpa == 172.21.6.22), action=(clone {outport = "etor-GR_worker-02-demo"; output; }; outport = "_MC_flood_l2"; output;)
  # table=27(ls_in_l2_lkup      ), priority=80   , match=(flags[1] == 0 && arp.op == 1 && arp.tpa == 172.21.6.22), action=(outport = "jtor-GR_worker-02-demo"; output;)


```

Here is the ovn logical network topology:

![](imgs/2024-04-08-13-42-36.png)

based on our case, lets draw the network topology:

![](dia/4.14.netobserv.case.drawio.svg)

<!-- ![](imgs/2024-04-07-20-24-21.png)


![](imgs/2024-04-07-20-25-10.png)


![](imgs/2024-04-07-20-26-30.png)


![](imgs/2024-04-07-20-27-05.png)


![](imgs/2024-04-07-20-27-36.png) -->


# Principles

## ebpf src

- https://github.com/netobserv/netobserv-ebpf-agent/blob/main/bpf/flows.c

Look at the source code, these 2 sentences are to register tc bpf hooks

```c
SEC("tc_ingress")
int ingress_flow_parse(struct __sk_buff *skb) {
    return flow_monitor(skb, INGRESS);
}

SEC("tc_egress")
int egress_flow_parse(struct __sk_buff *skb) {
    return flow_monitor(skb, EGRESS);
}
```

# tips

1. sometimes, the netobserv-epf- OOM, and console do not work. restart node to fix.

## ovn short word

- https://github.com/openshift/ovn-kubernetes/blob/master/go-controller/pkg/types/const.go

```go
  JoinSwitchPrefix             = "join_"
  ExternalSwitchPrefix         = "ext_"
  GWRouterPrefix               = "GR_"
  GWRouterLocalLBPostfix       = "_local"
  RouterToSwitchPrefix         = "rtos-"
  InterPrefix                  = "inter-"
  HybridSubnetPrefix           = "hybrid-subnet-"
  SwitchToRouterPrefix         = "stor-"
  JoinSwitchToGWRouterPrefix   = "jtor-"
  GWRouterToJoinSwitchPrefix   = "rtoj-"
  DistRouterToJoinSwitchPrefix = "dtoj-"
  JoinSwitchToDistRouterPrefix = "jtod-"
  EXTSwitchToGWRouterPrefix    = "etor-"
  GWRouterToExtSwitchPrefix    = "rtoe-"
  EgressGWSwitchPrefix         = "exgw-"

  TransitSwitchToRouterPrefix = "tstor-"
  RouterToTransitSwitchPrefix = "rtots-"
```

## search container on interface name

```bash

ip a | grep ace0354dc9ca1cb -A 5
# 67: ace0354dc9ca1cb@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1400 qdisc noqueue master ovs-system state UP group default qlen 1000
#     link/ether 3e:3a:b4:70:28:c0 brd ff:ff:ff:ff:ff:ff link-netns 301bf28d-f04c-46a6-b34c-800b34ce4c76
#     inet6 fe80::3c3a:b4ff:fe70:28c0/64 scope link
#        valid_lft forever preferred_lft forever

# The namespace ID we're looking for
namespace_id="562f3b52-881a-4057-b8fd-f17ceaf9096b"

# Get all pod IDs
pod_ids=$(crictl pods -q)

# Loop through each pod ID
for pod_id in $pod_ids; do
    # Inspect the pod and get the network namespace
    network_ns=$(crictl inspectp $pod_id | jq -r '.info.runtimeSpec.linux.namespaces[] | select(.type=="network") | .path')

    # Check if the network namespace contains the namespace ID
    if [[ $network_ns == *"$namespace_id"* ]]; then
        echo "Pod $pod_id is in the namespace $namespace_id"
        crictl pods --id $pod_id
    fi
done


```

## ovn cmd nbctl

```bash

oc exec -it ${VAR_POD} -c ovn-controller -n openshift-ovn-kubernetes -- ovn-nbctl -h
ovn-nbctl: OVN northbound DB management utility
usage: ovn-nbctl [OPTIONS] COMMAND [ARG...]

General commands:
  init                      initialize the database
  show                      print overview of database contents
  show SWITCH               print overview of database contents for SWITCH
  show ROUTER               print overview of database contents for ROUTER

Logical switch commands:
  ls-add [SWITCH]           create a logical switch named SWITCH
  ls-del SWITCH             delete SWITCH and all its ports
  ls-list                   print the names of all logical switches

ACL commands:
  [--type={switch | port-group}] [--log] [--severity=SEVERITY] [--name=NAME] [--may-exist]
  acl-add {SWITCH | PORTGROUP} DIRECTION PRIORITY MATCH ACTION
                            add an ACL to SWITCH/PORTGROUP
  [--type={switch | port-group}]
  acl-del {SWITCH | PORTGROUP} [DIRECTION [PRIORITY MATCH]]
                            remove ACLs from SWITCH/PORTGROUP
  [--type={switch | port-group}]
  acl-list {SWITCH | PORTGROUP}
                            print ACLs for SWITCH

QoS commands:
  qos-add SWITCH DIRECTION PRIORITY MATCH [rate=RATE [burst=BURST]] [dscp=DSCP]
                            add an QoS rule to SWITCH
  qos-del SWITCH [{DIRECTION | UUID} [PRIORITY MATCH]]
                            remove QoS rules from SWITCH
  qos-list SWITCH           print QoS rules for SWITCH

Mirror commands:
  mirror-add NAME TYPE [INDEX] FILTER {IP | MIRROR-ID}
                            add a mirror with given name
                            specify TYPE 'gre', 'erspan', or 'local'
                            specify the tunnel INDEX value
                                (indicates key if GRE
                                 erpsan_idx if ERSPAN)
                            specify FILTER for mirroring selection
                                'to-lport' / 'from-lport' / 'both'
                            specify Sink / Destination i.e. Remote IP, or a
                                local interface with external-ids:mirror-id
                                matching MIRROR-ID
  mirror-del [NAME]         remove mirrors
  mirror-list               print mirrors

Meter commands:
  [--fair]
  meter-add NAME ACTION RATE UNIT [BURST]
                            add a meter
  meter-del [NAME]          remove meters
  meter-list                print meters

Logical switch port commands:
  lsp-add SWITCH PORT       add logical port PORT on SWITCH
  lsp-add SWITCH PORT PARENT TAG
                            add logical port PORT on SWITCH with PARENT
                            on TAG
  lsp-del PORT              delete PORT from its attached switch
  lsp-list SWITCH           print the names of all logical ports on SWITCH
  lsp-get-parent PORT       get the parent of PORT if set
  lsp-get-tag PORT          get the PORT's tag if set
  lsp-set-addresses PORT [ADDRESS]...
                            set MAC or MAC+IP addresses for PORT.
  lsp-get-addresses PORT    get a list of MAC or MAC+IP addresses on PORT
  lsp-set-port-security PORT [ADDRS]...
                            set port security addresses for PORT.
  lsp-get-port-security PORT    get PORT's port security addresses
  lsp-get-up PORT           get state of PORT ('up' or 'down')
  lsp-set-enabled PORT STATE
                            set administrative state PORT
                            ('enabled' or 'disabled')
  lsp-get-enabled PORT      get administrative state PORT
                            ('enabled' or 'disabled')
  lsp-set-type PORT TYPE    set the type for PORT
  lsp-get-type PORT         get the type for PORT
  lsp-set-options PORT KEY=VALUE [KEY=VALUE]...
                            set options related to the type of PORT
  lsp-get-options PORT      get the type specific options for PORT
  lsp-set-dhcpv4-options PORT [DHCP_OPTIONS_UUID]
                            set dhcpv4 options for PORT
  lsp-get-dhcpv4-options PORT  get the dhcpv4 options for PORT
  lsp-set-dhcpv6-options PORT [DHCP_OPTIONS_UUID]
                            set dhcpv6 options for PORT
  lsp-get-dhcpv6-options PORT  get the dhcpv6 options for PORT
  lsp-get-ls PORT           get the logical switch which the port belongs to
  lsp-attach-mirror PORT MIRROR   attach source PORT to MIRROR
  lsp-detach-mirror PORT MIRROR   detach source PORT from MIRROR

Forwarding group commands:
  [--liveness]
  fwd-group-add GROUP SWITCH VIP VMAC PORTS...
                            add a forwarding group on SWITCH
  fwd-group-del GROUP       delete a forwarding group
  fwd-group-list [SWITCH]   print forwarding groups

Logical router commands:
  lr-add [ROUTER]           create a logical router named ROUTER
  lr-del ROUTER             delete ROUTER and all its ports
  lr-list                   print the names of all logical routers

Logical router port commands:
  lrp-add ROUTER PORT MAC NETWORK... [peer=PEER]
                            add logical port PORT on ROUTER
  lrp-set-gateway-chassis PORT CHASSIS [PRIORITY]
                            set gateway chassis for port PORT
  lrp-set-options PORT KEY=VALUE [KEY=VALUE]...
                            set router port options
  lrp-del-gateway-chassis PORT CHASSIS
                            delete gateway chassis from port PORT
  lrp-get-gateway-chassis PORT
                            print the names of all gateway chassis on PORT
                            with PRIORITY
  lrp-del PORT              delete PORT from its attached router
  lrp-list ROUTER           print the names of all ports on ROUTER
  lrp-set-enabled PORT STATE
                            set administrative state PORT
                            ('enabled' or 'disabled')
  lrp-get-enabled PORT      get administrative state PORT
                            ('enabled' or 'disabled')
  lrp-set-redirect-type PORT TYPE
                            set whether redirected packet to gateway chassis
                            of PORT will be encapsulated or not
                            ('overlay' or 'bridged')
  lrp-get-redirect-type PORT
                            get whether redirected packet to gateway chassis
                            of PORT will be encapsulated or not
                            ('overlay' or 'bridged')

Route commands:
  [--policy=POLICY]
  [--ecmp]
  [--ecmp-symmetric-reply]
  [--route-table=ROUTE_TABLE]
  lr-route-add ROUTER PREFIX NEXTHOP [PORT]
                            add a route to ROUTER
  [--policy=POLICY]
  [--route-table=ROUTE_TABLE]
  lr-route-del ROUTER [PREFIX [NEXTHOP [PORT]]]
                            remove routes from ROUTER
  [--route-table=ROUTE_TABLE]
  lr-route-list ROUTER      print routes for ROUTER

Policy commands:
  lr-policy-add ROUTER PRIORITY MATCH ACTION [NEXTHOP,[NEXTHOP,...]] [OPTIONS KEY=VALUE ...]
                            add a policy to router
  lr-policy-del ROUTER [{PRIORITY | UUID} [MATCH]]
                            remove policies from ROUTER
  lr-policy-list ROUTER     print policies for ROUTER


NAT commands:
  [--stateless]
  [--portrange]
  [--add-route]
  [--gateway-port=GATEWAY_PORT]
  lr-nat-add ROUTER TYPE EXTERNAL_IP LOGICAL_IP [LOGICAL_PORT EXTERNAL_MAC]
                            [EXTERNAL_PORT_RANGE]
                            add a NAT to ROUTER
  lr-nat-del ROUTER [TYPE [IP] [GATEWAY_PORT]]
                            remove NATs from ROUTER
  lr-nat-list ROUTER        print NATs for ROUTER

LB commands:
  lb-add LB VIP[:PORT] IP[:PORT]... [PROTOCOL]
                            create a load-balancer or add a VIP to an
                            existing load balancer
  lb-del LB [VIP]           remove a load-balancer or just the VIP from
                            the load balancer
  lb-list [LB]              print load-balancers
  lr-lb-add ROUTER LB       add a load-balancer to ROUTER
  lr-lb-del ROUTER [LB]     remove load-balancers from ROUTER
  lr-lb-list ROUTER         print load-balancers
  ls-lb-add SWITCH LB       add a load-balancer to SWITCH
  ls-lb-del SWITCH [LB]     remove load-balancers from SWITCH
  ls-lb-list SWITCH         print load-balancers

DHCP Options commands:
  dhcp-options-create CIDR [EXTERNAL_IDS]
                           create a DHCP options row with CIDR
  dhcp-options-del DHCP_OPTIONS_UUID
                           delete DHCP_OPTIONS_UUID
  dhcp-options-list
                           lists the DHCP_Options rows
  dhcp-options-set-options DHCP_OPTIONS_UUID  KEY=VALUE [KEY=VALUE]...
                           set DHCP options for DHCP_OPTIONS_UUID
  dhcp-options-get-options DHCO_OPTIONS_UUID
                           displays the DHCP options for DHCP_OPTIONS_UUID

Connection commands:
  get-connection             print the connections
  del-connection             delete the connections
  [--inactivity-probe=MSECS]
  set-connection TARGET...   set the list of connections to TARGET...

SSL commands:
  get-ssl                     print the SSL configuration
  del-ssl                     delete the SSL configuration
  set-ssl PRIV-KEY CERT CA-CERT [SSL-PROTOS [SSL-CIPHERS]] set the SSL configuration
Port group commands:
  pg-add PG [PORTS]           Create port group PG with optional PORTS
  pg-set-ports PG PORTS       Set PORTS on port group PG
  pg-del PG                   Delete port group PG
HA chassis group commands:
  ha-chassis-group-add GRP  Create an HA chassis group GRP
  ha-chassis-group-del GRP  Delete the HA chassis group GRP
  ha-chassis-group-list     List the HA chassis groups
  ha-chassis-group-add-chassis GRP CHASSIS PRIORITY Adds an HAchassis with mandatory PRIORITY to the HA chassis group GRP
  ha-chassis-group-remove-chassis GRP CHASSIS Removes the HA chassisCHASSIS from the HA chassis group GRP

Control Plane Protection Policy commands:
  copp-add NAME PROTO METER
                            Add a copp policy for PROTO packets on NAME
                            CoPP policy based on an existing METER.
  copp-del NAME [PROTO]
                            Delete the copp policy for PROTO packets for
                            NAME copp. If PROTO is not specified, delete all
                            copp policies defined for NAME.
  copp-list NAME
                            List all copp policies defined for control
                            protocols NAME.
  ls-copp-add NAME SWITCH
                            Add a NAME copp policy on SWITCH logical switch.
  lr-copp-add NAME ROUTER
                            Add a NAME copp policy on ROUTER logical router.

MAC_Binding commands:
  static-mac-binding-add LOGICAL_PORT IP MAC
                                    Add a Static_MAC_Binding entry
  static-mac-binding-del LOGICAL_PORT IP
                                    Delete Static_MAC_Binding entry
  static-mac-binding-list           List all Static_MAC_Binding entries

Database commands:
  list TBL [REC]              list RECord (or all records) in TBL
  find TBL CONDITION...       list records satisfying CONDITION in TBL
  get TBL REC COL[:KEY]       print values of COLumns in RECord in TBL
  set TBL REC COL[:KEY]=VALUE set COLumn values in RECord in TBL
  add TBL REC COL [KEY=]VALUE add (KEY=)VALUE to COLumn in RECord in TBL
  remove TBL REC COL [KEY=]VALUE  remove (KEY=)VALUE from COLumn
  clear TBL REC COL           clear values from COLumn in RECord in TBL
  create TBL COL[:KEY]=VALUE  create and initialize new record
  destroy TBL REC             delete RECord from TBL
  wait-until TBL REC [COL[:KEY]=VALUE]  wait until condition is true
Potentially unsafe database commands require --force option.
Database commands may reference a row in each table in the following ways:
  ACL:
    by UUID
    by "name"
  Address_Set:
    by UUID
    by "name"
  BFD:
    by UUID
  Chassis_Template_Var:
    by UUID
    by "chassis"
  Connection:
    by UUID
    by "target"
  Copp:
    by UUID
  DHCP_Options:
    by UUID
    via "dhcpv4_options" of Logical_Switch_Port with matching "name"
    via "dhcpv4_options" of Logical_Switch_Port with matching "external_ids:neutron:port_name"
    via "dhcpv6_options" of Logical_Switch_Port with matching "name"
    via "dhcpv6_options" of Logical_Switch_Port with matching "external_ids:neutron:port_name"
  DNS:
    by UUID
  Forwarding_Group:
    by UUID
    by "name"
  Gateway_Chassis:
    by UUID
  HA_Chassis:
    by UUID
    by "chassis_name"
  HA_Chassis_Group:
    by UUID
    by "name"
  Load_Balancer:
    by UUID
    by "name"
  Load_Balancer_Group:
    by UUID
    by "name"
  Load_Balancer_Health_Check:
    by UUID
    by "vip"
  Logical_Router:
    by UUID
    by "name"
    by "external_ids:neutron:router_name"
  Logical_Router_Policy:
    by UUID
  Logical_Router_Port:
    by UUID
    by "name"
  Logical_Router_Static_Route:
    by UUID
  Logical_Switch:
    by UUID
    by "name"
    by "external_ids:neutron:network_name"
  Logical_Switch_Port:
    by UUID
    by "name"
    by "external_ids:neutron:port_name"
  Meter:
    by UUID
    by "name"
  Meter_Band:
    by UUID
  Mirror:
    by UUID
  NAT:
    by UUID
    by "external_ip"
  NB_Global:
    by UUID
    as "."
  Port_Group:
    by UUID
    by "name"
  QoS:
    by UUID
  SSL:
    by UUID
    as "."
  Static_MAC_Binding:
    by UUID

Synchronization command (use with --wait=sb|hv):
  sync                     wait even for earlier changes to take effect

Options:
  --db=DATABASE               connect to DATABASE
                              (default: unix:/var/run/ovn/ovnnb_db.sock)
  --no-wait, --wait=none      do not wait for OVN reconfiguration (default)
  --no-leader-only            accept any cluster member, not just the leader
  --no-shuffle-remotes        do not shuffle the order of remotes
  --wait=sb                   wait for southbound database update
  --wait=hv                   wait for all chassis to catch up
  --print-wait-time           print time spent on waiting
  -t, --timeout=SECS          wait at most SECS seconds
  --dry-run                   do not commit changes to database
  --oneline                   print exactly one line of output per command

Output formatting options:
  -f, --format=FORMAT         set output formatting to FORMAT
                              ("table", "html", "csv", or "json")
  -d, --data=FORMAT           set table cell output formatting to
                              FORMAT ("string", "bare", or "json")
  --no-headings               omit table heading row
  --pretty                    pretty-print JSON in output
  --bare                      equivalent to "--format=list --data=bare --no-headings"

Daemon options:
  --detach                run in background as daemon
  --monitor               creates a process to monitor this daemon
  --user=username[:group] changes the effective daemon user:group
  --no-chdir              do not chdir to '/'
  --pidfile[=FILE]        create pidfile (default: /var/run/openvswitch/ovn-nbctl.pid)
  --overwrite-pidfile     with --pidfile, start even if already running

Logging options:
  -vSPEC, --verbose=SPEC   set logging levels
  -v, --verbose            set maximum verbosity level
  --log-file[=FILE]        enable logging to specified FILE
                           (default: /var/log/openvswitch/ovn-nbctl.log)
  --syslog-method=(libc|unix:file|udp:ip:port)
                           specify how to send messages to syslog daemon
  --syslog-target=HOST:PORT  also send syslog msgs to HOST:PORT via UDP
  --no-syslog             equivalent to --verbose=nbctl:syslog:warn

Other options:
  -h, --help                  display this help message
  -V, --version               display version information

Active database connection methods:
  tcp:HOST:PORT           PORT at remote HOST
  ssl:HOST:PORT           SSL PORT at remote HOST
  unix:FILE               Unix domain socket named FILE
Passive database connection methods:
  ptcp:PORT[:IP]          listen to TCP PORT on IP
  pssl:PORT[:IP]          listen for SSL on PORT on IP
  punix:FILE              listen on Unix domain socket FILE
PKI configuration (required to use SSL):
  -p, --private-key=FILE  file with private key
  -c, --certificate=FILE  file with certificate for private key
  -C, --ca-cert=FILE      file with peer CA certificate
  --bootstrap-ca-cert=FILE  file with peer CA certificate to read or create
SSL options:
  --ssl-protocols=PROTOS  list of SSL protocols to enable
  --ssl-ciphers=CIPHERS   list of SSL ciphers to enable


```


# end