# openshift 4.14 baremetal scenario Install nginx operator and use

In OpenShift, there are other options that can be used as ingress besides the default router/haproxy. For example, the OpenShift-certified nginx operator, we'll try it out today.

reference：
- https://www.nginx.com/blog/getting-started-nginx-ingress-operator-red-hat-openshift/
- https://kubernetes.github.io/ingress-nginx/deploy/baremetal/

However, it is necessary to clarify that operators certified by OpenShift are not operators that the Red Hat product team can support. Red Hat products can only support products that they have developed and tested themselves. Corresponding to operators, this means operators released by the Red Hat channel.

![](imgs/2024-03-09-20-33-57.png)

If it is an operator released by a certified channel, Red Hat's official strategy is to conduct different levels of verification with its partners to prove that the operator can be deployed and run smoothly. However, if you go a step further, if a problem occurs when the operator itself runs, then The partner needs to deal with it.

Corresponding to our nginx operator, it also has 2 versions, one is oss, which is the open source community version, and the other is plus, which is the F5 paid version. If there is a problem with the nginx operator itself during the use of nginx operator, then the customer of the oss version of nginx can only find the community for help, if it is the plus version, you need to find F5 for support.

In a nutshell, a certified operator is only certified to be deployable.

So before deploying a third party solution like certified operator, make sure that you understand Red Hat's support policy and plan ahead.

Video tutorial:

[<kbd><img src="imgs/2024-03-10-00-23-27.png" width="600"></kbd>](https://youtu.be/gacwUmCGIlI)

- [youtube](https://youtu.be/gacwUmCGIlI)

# nginx operator

Let's first install the nginx operator. There's official documentation, so we just follow the instructions.

- https://github.com/nginxinc/nginx-ingress-helm-operator
- https://github.com/nginxinc/nginx-ingress-helm-operator/blob/main/docs/openshift-installation.md

![](imgs/2024-03-08-14-03-20.png)


```bash

# You can use the operator hub to install the nginx operator.
# Then add permissions(seems not necessary, try it later, see if there is a configuration when creating controller instance, create rbac)

# https://github.com/nginxinc/nginx-ingress-helm-operator/blob/main/docs/openshift-installation.md
wget -O nginx.scc.yaml https://raw.githubusercontent.com/nginxinc/nginx-ingress-helm-operator/v2.1.2/resources/scc.yaml

oc apply -f nginx.scc.yaml
# securitycontextconstraints.security.openshift.io/nginx-ingress-admin created

```
Then create an ingress instance.

![](imgs/2024-03-08-14-54-38.png)

<!-- 到此，我们分析一下我们都创建了什么，接下来我们要干什么 -->With that said, let's summarize what we've built and what we plan to do next

```bash

oc get NginxIngress -A
# NAMESPACE       NAME                  AGE
# nginx-ingress   nginxingress-sample   56m

# First, let's take a look at the pods created
oc get pod -n nginx-ingress
# NAME                                                            READY   STATUS    RESTARTS   AGE
# nginx-ingress-operator-controller-manager-6d8cbd487d-4rp6m      2/2     Running   0          166m
# nginxingress-sample-nginx-ingress-controller-5ddd56f675-7g5k7   1/1     Running   0          53m

oc get pod/nginxingress-sample-nginx-ingress-controller-5ddd56f675-7g5k7 -o json -n nginx-ingress | jq .spec | grep -i host

# The result is there is no any configuration information about the host
# Nginx attracted the traffic on itself in a very special way.

oc get svc -n nginx-ingress
# NAME                                                        TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
# nginx-ingress-operator-controller-manager-metrics-service   ClusterIP      172.22.43.63    <none>        8443/TCP                     18h
# nginxingress-sample-nginx-ingress-controller                LoadBalancer   172.22.145.65   <pending>     80:32758/TCP,443:31566/TCP   55m

```

<!-- 到这里，我们知道了，openshift/k8s 上面的nginx operator，需要依赖外部的load balancer来导入流量，如果是云服务的话，会自动对接，但是我们是baremetal的场景，那么就必须搞一个metalLB来充当这个load balancer了 -->Here we know that the nginx operator on openshift/k8s requires an external load balancer to import traffic. If it is a cloud service, it will be automatically connected, but if we are in a baremetal scenario, then we must use a metalLB as the load balancer.

- https://github.com/kubernetes/ingress-nginx/blob/main/docs/deploy/baremetal.md

![](imgs/2024-03-08-16-02-14.png)

# metalLB

<!-- openshift 官方文档就有metalLB的手册，照着做就可以啦。 -->The official openshift documentation has a manual for metalLB, just follow the instructions.

- https://docs.openshift.com/container-platform/4.14/networking/metallb/about-metallb.html

![](imgs/2024-03-08-16-05-50.png)

<!-- 安装完毕以后，我们需要激活 MetalLB  -->After the installation is complete, we need to activate MetalLB

![](imgs/2024-03-08-17-14-36.png)

<!-- 然后我们配置一个地址空间 -->Then we set up an address space

![](imgs/2024-03-08-17-18-59.png)

<!-- 这个地址空间，我们不用自动分配，因为我们希望nginx使用固定ip地址，后面方便我们配置dns. -->This address space, we don't need to automatically allocate, because we hope that nginx uses a fixed ip address, which will make it convenient for us to configure dns later.

![](imgs/2024-03-08-17-22-10.png)

<!-- 然后配置二层的地址广播，注意，在这里，我们有2个可选项，一个是bgp，一个是L2，bgp需要上游路由器支持，一般企业场景不太方便，我们一般用L2，但是这要保证各个节点之间二层可达。 -->Then configure Layer 2 address broadcast. Note that here, we have two options: BGP and L2. BGP requires support from the upstream router, which is generally not convenient in an enterprise scenario. We generally use L2, but this requires Layer 2 reachability between each node.

![](imgs/2024-03-08-17-30-38.png)

<!-- 配置界面自动把之前的ip address pool给带出来了。 -->The configuration interface automatically brings up the previous IP address pool.

![](imgs/2024-03-08-17-31-13.png)

# nginx operator, continue

<!-- 我们配置的metalLB是不能自动分配IP地址的，这是为了方便我们后面的dns配置，作者认为，在实际生产场景，应该也是不自动分配为好。 -->The metalLB we configured cannot automatically allocate IP addresses. This is for the convenience of our subsequent dns configuration. The author believes that in actual production scenarios, it should not be automatically allocated.

<!-- 我们更新一下nginx的配置，让nginx向metalLB申请一个IP地址。 -->Let's update the nginx configuration such that nginx requests an IP address from metalLB.

![](imgs/2024-03-08-17-40-11.png)

![](imgs/2024-03-08-17-40-39.png)

<!-- 然后，我们就能看到nginx service对应了一个固定的对外IP地址啦。 -->Then, we can see that the nginx service corresponds to a fixed external IP address.

![](imgs/2024-03-09-20-44-21.png)


## Application deployment

<!-- 好了，我们接下来就配置一个应用，看看效果吧。 -->OK, let's configure an application and see the result.

<!-- 注意，我们创建的ingress标准了ingress class，因为openshift里面默认有一个ingress的实现，是router/haproxy，所以我们需要告诉ingress用nginx来创建。 -->Note that the ingress we created specifies the ingress class, because OpenShift has a default ingress implementation, router/haproxy, so we need to tell ingress to use nginx to create it.

```bash

cat << EOF > ${BASE_DIR}/data/install/nginx-ingress.yaml
---
kind: Deployment
apiVersion: apps/v1
metadata:
  name: web-demo
  namespace: llm-demo
  labels:
    app: web-demo
    app.kubernetes.io/component: web-demo
    app.kubernetes.io/instance: web-demo
    app.kubernetes.io/name: web-demo
    app.kubernetes.io/part-of: web-demo
    app.openshift.io/runtime: nodejs
    app.openshift.io/runtime-namespace: llm-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: web-demo
  template:
    metadata:
      labels:
        app: web-demo
        deployment: web-demo
    spec:
      containers:
        - name: web-demo
          image: quay.io/wangzheng422/qimgs:web-demo-v01
          ports:
            - containerPort: 8080
              protocol: TCP

---
kind: Service
apiVersion: v1
metadata:
  name: web-demo
  namespace: llm-demo
  labels:
    app: web-demo
    app.kubernetes.io/component: web-demo
    app.kubernetes.io/instance: web-demo
    app.kubernetes.io/name: web-demo
    app.kubernetes.io/part-of: web-demo
    app.openshift.io/runtime-version: web-demo-v01
spec:
  ports:
    - name: 8080-tcp
      protocol: TCP
      port: 8080
      targetPort: 8080
  selector:
    app: web-demo
    deployment: web-demo

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-demo-wzh
spec:
  rules:
  - host: nginx-demo.wzhlab.top
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-demo
            port:
              number: 8080
  ingressClassName: nginx
EOF

oc create -f ${BASE_DIR}/data/install/nginx-ingress.yaml -n llm-demo

# oc delete -f ${BASE_DIR}/data/install/nginx-ingress.yaml -n llm-demo

```

goto to see the result:

http://nginx-demo.wzhlab.top/

![](imgs/2024-03-08-20-56-42.png)

<!-- 我们还能从界面上，看到一个新配置的ingress -->We can see on the interface that a new configuration has been made.

![](imgs/2024-03-09-20-43-49.png)

## Principle Analysis

<!-- 我们分析一下metalLB的原理，以及和nginx互动的关系。 -->Let's analyze the principles of MetalLB and its interaction with Nginx.

<!-- 这里是逻辑关系图，看上去，网络包传递的路径比较复杂，先访问metalLB的对外IP，然后找到ovn loandbalancer代表的k8s service，然后访问service后面的nginx，再从nginx访问后端app pod。 -->Here is a logical relationship diagram. It seems that the path of network packet transmission is complicated. First, access the external IP of metalLB, then find the k8s service represented by ovn loadbalancer, then access nginx behind the service, and then access the backend app pod from nginx.

![](./dia/4.14.nginx.drawio.svg)

<!-- 实际上，网络包只是经过了ovs/ovn的flow进行nat转换，然后就到了nginx pod上面，之后再访问后端app pod，所以传输路径并不复杂。 -->In fact, the network packet only passed through the ovs/ovn flow for NAT conversion, and then went to the nginx pod, and then visited the backend app pod, so the transmission path is not complicated.

![](./dia/4.14.nginx.ovn.drawio.svg)

<!-- 根据ovs官网的说法，ovs的hook是在iptables之前起作用的。 -->According to the ovs website, the ovs's hook takes effect before iptables.

- https://docs.openvswitch.org/en/latest/faq/issues/

<!-- 以下，我们就一步一步的在系统里面查看一下，相关的信息，验证我们的说法。 -->Lets go through the system step by step below to check the relevant information and verify what we have said.

```bash
oc get svc -n nginx-ingress
# NAME                                                        TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
# nginx-ingress-operator-controller-manager-metrics-service   ClusterIP      172.22.43.63    <none>        8443/TCP                     20h
# nginxingress-sample-nginx-ingress-controller                LoadBalancer   172.22.145.65   172.21.6.41   80:32758/TCP,443:31566/TCP   166m

# Let's look into the metalLB principle. It was found that there are rules in the nft, and DNAT was made

iptables -L -v -n -t nat
# Chain PREROUTING (policy ACCEPT 0 packets, 0 bytes)
#  pkts bytes target     prot opt in     out     source               destination
#  2037  125K OVN-KUBE-ETP  all  --  *      *       0.0.0.0/0            0.0.0.0/0
#  2037  125K OVN-KUBE-EXTERNALIP  all  --  *      *       0.0.0.0/0            0.0.0.0/0
#  2037  125K OVN-KUBE-NODEPORT  all  --  *      *       0.0.0.0/0            0.0.0.0/0

# Chain INPUT (policy ACCEPT 0 packets, 0 bytes)
#  pkts bytes target     prot opt in     out     source               destination

# Chain OUTPUT (policy ACCEPT 0 packets, 0 bytes)
#  pkts bytes target     prot opt in     out     source               destination
# 32647 1974K OVN-KUBE-EXTERNALIP  all  --  *      *       0.0.0.0/0            0.0.0.0/0
# 32647 1974K OVN-KUBE-NODEPORT  all  --  *      *       0.0.0.0/0            0.0.0.0/0
# 32647 1974K OVN-KUBE-ITP  all  --  *      *       0.0.0.0/0            0.0.0.0/0

# Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes)
#  pkts bytes target     prot opt in     out     source               destination
# 32647 1974K OVN-KUBE-EGRESS-IP-MULTI-NIC  all  --  *      *       0.0.0.0/0            0.0.0.0/0
# 32647 1974K OVN-KUBE-EGRESS-SVC  all  --  *      *       0.0.0.0/0            0.0.0.0/0
# 30580 1835K OVN-KUBE-SNAT-MGMTPORT  all  --  *      ovn-k8s-mp0  0.0.0.0/0            0.0.0.0/0

# Chain KUBE-KUBELET-CANARY (0 references)
#  pkts bytes target     prot opt in     out     source               destination

# Chain OVN-KUBE-EGRESS-IP-MULTI-NIC (1 references)
#  pkts bytes target     prot opt in     out     source               destination

# Chain OVN-KUBE-EGRESS-SVC (1 references)
#  pkts bytes target     prot opt in     out     source               destination
#     0     0 RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0            mark match 0x3f0 /* DoNotSNAT */

# Chain OVN-KUBE-ETP (1 references)
#  pkts bytes target     prot opt in     out     source               destination
#     0     0 DNAT       tcp  --  *      *       0.0.0.0/0            172.21.6.41          tcp dpt:443 to:169.254.169.3:31566
#     0     0 DNAT       tcp  --  *      *       0.0.0.0/0            172.21.6.41          tcp dpt:80 to:169.254.169.3:32758

# Chain OVN-KUBE-EXTERNALIP (2 references)
#  pkts bytes target     prot opt in     out     source               destination
#     0     0 DNAT       tcp  --  *      *       0.0.0.0/0            172.21.6.41          tcp dpt:443 to:172.22.145.65:443
#     0     0 DNAT       tcp  --  *      *       0.0.0.0/0            172.21.6.41          tcp dpt:80 to:172.22.145.65:80

# Chain OVN-KUBE-ITP (1 references)
#  pkts bytes target     prot opt in     out     source               destination

# Chain OVN-KUBE-NODEPORT (2 references)
#  pkts bytes target     prot opt in     out     source               destination
#     0     0 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL tcp dpt:31566 to:172.22.145.65:443
#     0     0 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL tcp dpt:32758 to:172.22.145.65:80

# Chain OVN-KUBE-SNAT-MGMTPORT (1 references)
#  pkts bytes target     prot opt in     out     source               destination
#     0     0 RETURN     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:31566
#     0     0 RETURN     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:32758
# 30580 1835K SNAT       all  --  *      ovn-k8s-mp0  0.0.0.0/0            0.0.0.0/0            /* OVN SNAT to Management Port */ to:10.133.0.2

# On worker/master, watch the routing table
# The routing table shows that the physical machine network segment 172.21* is connected to the ovs/ovn br-ex.
# The service IP subnet 172.22.* is also used for ovns/ovn br-ex.
ip r
# default via 172.21.6.254 dev br-ex proto static metric 48
# 10.132.0.0/23 dev ovn-k8s-mp0 proto kernel scope link src 10.132.0.2
# 10.132.0.0/14 via 10.132.0.1 dev ovn-k8s-mp0
# 169.254.169.0/29 dev br-ex proto kernel scope link src 169.254.169.2
# 169.254.169.1 dev br-ex src 172.21.6.23
# 169.254.169.3 via 10.132.0.1 dev ovn-k8s-mp0
# 172.21.6.0/24 dev br-ex proto kernel scope link src 172.21.6.23 metric 48
# 172.22.0.0/16 via 169.254.169.4 dev br-ex mtu 1400


# After the NFT, the system has entered the OVN, where the service is defined and the pod is started.
oc get pod -n openshift-ovn-kubernetes -o wide
# NAME                                     READY   STATUS    RESTARTS   AGE   IP            NODE             NOMINATED NODE   READINESS GATES
# ovnkube-control-plane-7648b89f9c-xzk6c   2/2     Running   9          12d   172.21.6.23   master-01-demo   <none>           <none>
# ovnkube-node-4wgzd                       8/8     Running   32         12d   172.21.6.26   worker-01-demo   <none>           <none>
# ovnkube-node-n5q6b                       8/8     Running   39         12d   172.21.6.23   master-01-demo   <none>           <none>

VAR_POD='ovnkube-node-4wgzd'

oc exec -it ${VAR_POD} -c ovn-controller -n openshift-ovn-kubernetes -- ovn-nbctl list load-balancer | grep 172.22.145.65 -B 8
# _uuid               : 38b40eee-be68-496f-8fa0-1728709b97bc
# external_ids        : {"k8s.ovn.org/kind"=Service, "k8s.ovn.org/owner"="nginx-ingress/nginxingress-sample-nginx-ingress-controller"}
# health_check        : []
# ip_port_mappings    : {}
# name                : "Service_nginx-ingress/nginxingress-sample-nginx-ingress-controller_TCP_cluster"
# options             : {event="false", hairpin_snat_ip="169.254.169.5 fd69::5", neighbor_responder=none, reject="true", skip_snat="false"}
# protocol            : tcp
# selection_fields    : []
# vips                : {"172.22.145.65:443"="10.133.0.21:443", "172.22.145.65:80"="10.133.0.21:80"}

oc exec -it ${VAR_POD} -c ovn-controller -n openshift-ovn-kubernetes -- ovn-sbctl dump-flows | grep 172.21.6.41
  # table=5 (lr_in_defrag       ), priority=100  , match=(ip && ip4.dst == 172.21.6.41), action=(ct_dnat;)
  # table=7 (lr_in_dnat         ), priority=120  , match=(ct.new && !ct.rel && ip4 && ip4.dst == 172.21.6.41 && tcp && tcp.dst == 443), action=(flags.skip_snat_for_lb = 1; ct_lb_mark(backends=10.133.0.21:443; skip_snat);)
  # table=7 (lr_in_dnat         ), priority=120  , match=(ct.new && !ct.rel && ip4 && ip4.dst == 172.21.6.41 && tcp && tcp.dst == 80), action=(flags.skip_snat_for_lb = 1; ct_lb_mark(backends=10.133.0.21:80; skip_snat);)
  # table=6 (ls_in_pre_stateful ), priority=120  , match=(reg0[2] == 1 && ip4.dst == 172.21.6.41 && tcp.dst == 443), action=(reg1 = 172.21.6.41; reg2[0..15] = 443; ct_lb_mark;)
  # table=6 (ls_in_pre_stateful ), priority=120  , match=(reg0[2] == 1 && ip4.dst == 172.21.6.41 && tcp.dst == 80), action=(reg1 = 172.21.6.41; reg2[0..15] = 80; ct_lb_mark;)
  # table=13(ls_in_lb           ), priority=120  , match=(ct.new && ip4.dst == 172.21.6.41 && tcp.dst == 443), action=(reg0[1] = 0; ct_lb_mark(backends=10.133.0.21:443);)
  # table=13(ls_in_lb           ), priority=120  , match=(ct.new && ip4.dst == 172.21.6.41 && tcp.dst == 80), action=(reg0[1] = 0; ct_lb_mark(backends=10.133.0.21:80);)


oc exec -it ${VAR_POD} -c ovn-controller -n openshift-ovn-kubernetes -- ovn-sbctl dump-flows | grep 172.22.145.65
  # table=5 (lr_in_defrag       ), priority=100  , match=(ip && ip4.dst == 172.22.145.65), action=(ct_dnat;)
  # table=7 (lr_in_dnat         ), priority=120  , match=(ct.new && !ct.rel && ip4 && ip4.dst == 172.22.145.65 && tcp && tcp.dst == 443), action=(flags.force_snat_for_lb = 1; ct_lb_mark(backends=10.133.0.21:443; force_snat);)
  # table=7 (lr_in_dnat         ), priority=120  , match=(ct.new && !ct.rel && ip4 && ip4.dst == 172.22.145.65 && tcp && tcp.dst == 80), action=(flags.force_snat_for_lb = 1; ct_lb_mark(backends=10.133.0.21:80; force_snat);)
  # table=6 (ls_in_pre_stateful ), priority=120  , match=(reg0[2] == 1 && ip4.dst == 172.22.145.65 && tcp.dst == 443), action=(reg1 = 172.22.145.65; reg2[0..15] = 443; ct_lb_mark;)
  # table=6 (ls_in_pre_stateful ), priority=120  , match=(reg0[2] == 1 && ip4.dst == 172.22.145.65 && tcp.dst == 80), action=(reg1 = 172.22.145.65; reg2[0..15] = 80; ct_lb_mark;)
  # table=13(ls_in_lb           ), priority=120  , match=(ct.new && ip4.dst == 172.22.145.65 && tcp.dst == 443), action=(reg0[1] = 0; ct_lb_mark(backends=10.133.0.21:443);)
  # table=13(ls_in_lb           ), priority=120  , match=(ct.new && ip4.dst == 172.22.145.65 && tcp.dst == 80), action=(reg0[1] = 0; ct_lb_mark(backends=10.133.0.21:80);)


oc get ep -A | grep 10.133.0.21
# nginx-ingress                                      nginxingress-sample-nginx-ingress-controller                10.133.0.21:443,10.133.0.21:80                                      6h55m


# on master/worker node
# We can access the IP address of the nginx service or the IP address corresponding to the application of metalLB.
# The Nginx server returns a 404 error because the domain name must be used to correctly find the backend service pod.
curl 172.22.145.65
# <html>
# <head><title>404 Not Found</title></head>
# <body>
# <center><h1>404 Not Found</h1></center>
# <hr><center>nginx/1.25.4</center>
# </body>
# </html>

curl 172.21.6.41
# <html>
# <head><title>404 Not Found</title></head>
# <body>
# <center><h1>404 Not Found</h1></center>
# <hr><center>nginx/1.25.4</center>
# </body>
# </html>

oc get pod -n nginx-ingress
# NAME                                                            READY   STATUS    RESTARTS   AGE
# nginx-ingress-operator-controller-manager-6d8cbd487d-4rp6m      2/2     Running   0          8h
# nginxingress-sample-nginx-ingress-controller-5ddd56f675-7g5k7   1/1     Running   0          7h2m


VAR_POD='nginxingress-sample-nginx-ingress-controller-5ddd56f675-7g5k7'

oc exec -it ${VAR_POD} -n nginx-ingress  -- ls /etc/nginx/conf.d
# llm-demo-ingress-demo-wzh.conf

oc exec -it ${VAR_POD} -n nginx-ingress  -- cat /etc/nginx/conf.d/llm-demo-ingress-demo-wzh.conf
# # configuration for llm-demo/ingress-demo-wzh
# upstream llm-demo-ingress-demo-wzh-nginx-demo.wzhlab.top-web-demo-8080 {zone llm-demo-ingress-demo-wzh-nginx-demo.wzhlab.top-web-demo-8080 256k;random two least_conn;
#         server 10.133.0.56:8080 max_fails=1 fail_timeout=10s max_conns=0;
# }

# server {
#         listen 80;listen [::]:80;

#         server_tokens on;

#         server_name nginx-demo.wzhlab.top;

#         set $resource_type "ingress";
#         set $resource_name "ingress-demo-wzh";
#         set $resource_namespace "llm-demo";
#         location / {
#                 set $service "web-demo";
#                 proxy_http_version 1.1;

#                 proxy_connect_timeout 60s;
#                 proxy_read_timeout 60s;
#                 proxy_send_timeout 60s;
#                 client_max_body_size 1m;
#                 proxy_set_header Host $host;
#                 proxy_set_header X-Real-IP $remote_addr;
#                 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
#                 proxy_set_header X-Forwarded-Host $host;
#                 proxy_set_header X-Forwarded-Port $server_port;
#                 proxy_set_header X-Forwarded-Proto $scheme;
#                 proxy_buffering on;
#                 proxy_pass http://llm-demo-ingress-demo-wzh-nginx-demo.wzhlab.top-web-demo-8080;
#         }

# }

oc get ep -A | grep 10.133.0.56
# llm-demo                                           web-demo                                                    10.133.0.56:8080                                                    55m

oc get pod -n llm-demo -o wide
# NAME                        READY   STATUS    RESTARTS   AGE   IP            NODE             NOMINATED NODE   READINESS GATES
# web-demo-755c9b6f47-xh8b8   1/1     Running   0          55m   10.133.0.56   worker-01-demo   <none>           <none>

```

# end