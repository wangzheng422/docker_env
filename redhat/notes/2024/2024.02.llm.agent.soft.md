# LLM agent 框架分析

看到了网上一个文章，列举了很多 ai agent 开发框架，这个很好，但是我们的目标是后端使用的是离线的LLM，也就是不依赖openai的在线服务，那么我们就来梳理一下，看看这些框架，能不能用离线的LLM。

我们的标准是能用离线的LLM，而且能够实现有意义的agent。我们提出这样的要求，是因为替换成开源离线LLM是很容易的，但是对应的prompt，也就是提示词的适配，是很复杂，工程量很大的，所以我们要研究一下，有没有现成的框架，能够支持离线LLM，已经适配好了的。

- [九大最热门的开源AI Agent框架](https://mp.weixin.qq.com/s/9WARognUD4sxZkPz-A__iw)

# AutoGen

- https://github.com/microsoft/autogen

这是一个天然的多agent对话框架，支持llama，他本身并不包括prompt适配，所以你写agent的时候，要自己把 prompt写进去，这个对于我们要做多个离线LLM的agent来说，是非常利好的。

- https://microsoft.github.io/autogen/docs/llm_configuration

项目本身也有很多agent的例子，我们可以参考。
- https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat_RAG.ipynb
- https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_llava.ipynb

我们可以用autogen做这么一个架构的应用，用 Mistral 8x7B 作为一个主力agent，然后找专业领域的LLM做辅助agent，还有一个可选的代码执行agent，这样就可以实现一个多agent的对话框架了。在这个框架里面，我们给主力agent提出一个问题，比如devops领域遇到的错误和问题，由主力agent加工问题，并循环的找专业领域的LLM，来解决问题。

以后有时间，我们来实际一下吧。

# ShortGPT

- https://github.com/RayVentura/ShortGPT 

这个就是给视频主角换讲话语言的项目，可以用来做demo。

# AutoGPT

- https://github.com/Significant-Gravitas/AutoGPT

根据代码分析

- https://github.com/Significant-Gravitas/AutoGPT/blob/master/autogpts/autogpt/autogpt/llm/api_manager.py

我们可以看到这个框架，只是给openai适配了。

当然，有人已经给llama做适配了

- https://github.com/rhohndorf/Auto-Llama-cpp

从项目介绍来看，llama的返回很不稳定，我们也就不用浪费时间去自己测试了。

# MetaGPT

- https://github.com/geekan/MetaGPT 

这个项目很好，文档很详细，也明说了，可以集成其他任何llm，但是当下，只有openai的服务，能提供稳定的输出。

项目也是一个软件公司，可以有多个角色，来完成软件项目。

可以关注，不用优先研究。

# SuperAGI

- https://github.com/TransformerOptimus/SuperAGI  

项目做的挺好的，就是深度依赖 openai 的服务，所以我们不用看了。

# CAMEL

- https://github.com/camel-ai/camel

官方支持 llama-2，但是文档一言难尽，可以关注，但是不要优先研究他了。

# BabyAGI

- https://github.com/yoheinakajima/babyagi 

不用看了，停更了。

# ChatDev

- https://github.com/OpenBMB/ChatDev

这更像一个游戏，可以用来耍酷，后端依赖 openai

# Langfuse

- https://github.com/langfuse/langfuse

感觉这是一个付费方案的软广，没有研究价值

