{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc21043",
   "metadata": {},
   "source": [
    "In this notebook, we will go over how to leverage the SDK to directly work interactively with a Ray Cluster during development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b55bc3ea-4ce3-49bf-bb1f-e209de8ca47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pieces from codeflare-sdk\n",
    "from codeflare_sdk import Cluster, ClusterConfiguration, TokenAuthentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abe0a3d8-1c53-4714-9c48-19a9158f4bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23.1\n"
     ]
    }
   ],
   "source": [
    "import codeflare_sdk\n",
    "print(codeflare_sdk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "614daa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insecure request warnings have been disabled\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Logged into https://api.demo-01-rhsys.wzhlab.top:6443'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create authentication object for user permissions\n",
    "# IF unused, SDK will automatically check for default kubeconfig, then in-cluster config\n",
    "# KubeConfigFileAuthentication can also be used to specify kubeconfig path manually\n",
    "auth = TokenAuthentication(\n",
    "    token = \"sha256~v-lxC7Fd_gnWMkVwxDJAVQ8uhZCLJ1kormSDd1JdDIk\",\n",
    "    server = \"https://api.demo-01-rhsys.wzhlab.top:6443\",\n",
    "    skip_tls= True\n",
    ")\n",
    "auth.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc27f84c",
   "metadata": {},
   "source": [
    "Once again, let's start by running through the same cluster setup as before:\n",
    "\n",
    "NOTE: The default images used by the CodeFlare SDK for creating a RayCluster resource depend on the installed Python version:\n",
    "\n",
    "- For Python 3.9: 'quay.io/modh/ray:2.35.0-py39-cu121'\n",
    "- For Python 3.11: 'quay.io/modh/ray:2.35.0-py311-cu121'\n",
    "\n",
    "If you prefer to use a custom Ray image that better suits your needs, you can specify it in the image field to override the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f4bc870-091f-4e11-9642-cba145710159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: TLS verification has been disabled - Endpoint checks will be bypassed\n",
      "Yaml resources loaded for llama-factory-test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5225b74ff6fa43ca9f3df0d59e0f5c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Button(description='Cluster Up', icon='play', style=ButtonStyle(), tooltip='Creaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718ec193a86643698973456e8e83c867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and configure our cluster object\n",
    "# The SDK will try to find the name of your default local queue based on the annotation \"kueue.x-k8s.io/default-queue\": \"true\" unless you specify the local queue manually below\n",
    "cluster_name = \"llama-factory-test\"\n",
    "cluster = Cluster(ClusterConfiguration(\n",
    "    name=cluster_name,\n",
    "    head_cpu_requests=1,\n",
    "    head_cpu_limits=1,\n",
    "    head_memory_requests=6,\n",
    "    head_memory_limits=6,\n",
    "    head_extended_resource_requests={'nvidia.com/gpu':0}, # For GPU enabled workloads set the head_extended_resource_requests and worker_extended_resource_requests\n",
    "    worker_extended_resource_requests={'nvidia.com/gpu':0},\n",
    "    num_workers=2,\n",
    "    worker_cpu_requests='2',\n",
    "    worker_cpu_limits=8,\n",
    "    worker_memory_requests=4,\n",
    "    worker_memory_limits=12,\n",
    "    image=\"quay.io/wangzheng422/qimgs:llama-factory-ray-20250106-v08\", # Optional Field \n",
    "    write_to_file=False, # When enabled Ray Cluster yaml files are written to /HOME/.codeflare/resources \n",
    "    # local_queue=\"local-queue-name\" # Specify the local queue manually\n",
    "    verify_tls=False,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0884bbc-c224-4ca0-98a0-02dfa09c2200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray Cluster: 'llama-factory-test' has successfully been created\n",
      "Waiting for requested resources to be set up...\n",
      "Requested cluster is up and running!\n",
      "Dashboard is ready!\n"
     ]
    }
   ],
   "source": [
    "# Bring up the cluster\n",
    "cluster.up()\n",
    "cluster.wait_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df71c1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                          </span><span style=\"font-weight: bold; font-style: italic\"> ðŸš€ CodeFlare Cluster Details ðŸš€</span><span style=\"font-style: italic\">                           </span>\n",
       "<span style=\"font-weight: bold\">                                                                                     </span>\n",
       " â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® \n",
       " â”‚   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #008000; font-weight: bold\">Name</span>                                                                          â”‚ \n",
       " â”‚   <span style=\"font-weight: bold; text-decoration: underline\">llama-factory-test</span>                                                Active âœ…   â”‚ \n",
       " â”‚                                                                                 â”‚ \n",
       " â”‚   <span style=\"font-weight: bold\">URI:</span> ray://llama-factory-test-head-svc.rhods-notebooks.svc:10001              â”‚ \n",
       " â”‚                                                                                 â”‚ \n",
       " â”‚   <a href=\"https://ray-dashboard-llama-factory-test-rhods-notebooks.apps.demo-01-rhsys.wzhlab.top\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">DashboardðŸ”—</span></a>                                                                   â”‚ \n",
       " â”‚                                                                                 â”‚ \n",
       " â”‚  <span style=\"font-style: italic\">                     Cluster Resources                     </span>                    â”‚ \n",
       " â”‚   â•­â”€â”€ Workers â”€â”€â•®  â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€ Worker specs(each) â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®                     â”‚ \n",
       " â”‚   â”‚ <span style=\"font-weight: bold\"> # Workers </span> â”‚  â”‚ <span style=\"font-weight: bold\"> Memory      CPU         GPU        </span> â”‚                     â”‚ \n",
       " â”‚   â”‚ <span style=\"color: #800080; text-decoration-color: #800080\">           </span> â”‚  â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">            </span><span style=\"color: #800080; text-decoration-color: #800080\">                        </span> â”‚                     â”‚ \n",
       " â”‚   â”‚ <span style=\"color: #800080; text-decoration-color: #800080\"> 2         </span> â”‚  â”‚ <span style=\"color: #008080; text-decoration-color: #008080\"> 4G~12G     </span><span style=\"color: #800080; text-decoration-color: #800080\"> 2~8         0          </span> â”‚                     â”‚ \n",
       " â”‚   â”‚ <span style=\"color: #800080; text-decoration-color: #800080\">           </span> â”‚  â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">            </span><span style=\"color: #800080; text-decoration-color: #800080\">                        </span> â”‚                     â”‚ \n",
       " â”‚   â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯  â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯                     â”‚ \n",
       " â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                          \u001b[0m\u001b[1;3m ðŸš€ CodeFlare Cluster Details ðŸš€\u001b[0m\u001b[3m                           \u001b[0m\n",
       "\u001b[1m \u001b[0m\u001b[1m                                                                                   \u001b[0m\u001b[1m \u001b[0m\n",
       " â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® \n",
       " â”‚   \u001b[1;37;42mName\u001b[0m                                                                          â”‚ \n",
       " â”‚   \u001b[1;4mllama-factory-test\u001b[0m                                                Active âœ…   â”‚ \n",
       " â”‚                                                                                 â”‚ \n",
       " â”‚   \u001b[1mURI:\u001b[0m ray://llama-factory-test-head-svc.rhods-notebooks.svc:10001              â”‚ \n",
       " â”‚                                                                                 â”‚ \n",
       " â”‚   \u001b]8;id=448480;https://ray-dashboard-llama-factory-test-rhods-notebooks.apps.demo-01-rhsys.wzhlab.top\u001b\\\u001b[4;34mDashboardðŸ”—\u001b[0m\u001b]8;;\u001b\\                                                                   â”‚ \n",
       " â”‚                                                                                 â”‚ \n",
       " â”‚  \u001b[3m                     Cluster Resources                     \u001b[0m                    â”‚ \n",
       " â”‚   â•­â”€â”€ Workers â”€â”€â•®  â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€ Worker specs(each) â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®                     â”‚ \n",
       " â”‚   â”‚ \u001b[1m \u001b[0m\u001b[1m# Workers\u001b[0m\u001b[1m \u001b[0m â”‚  â”‚ \u001b[1m \u001b[0m\u001b[1mMemory    \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mCPU       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mGPU       \u001b[0m\u001b[1m \u001b[0m â”‚                     â”‚ \n",
       " â”‚   â”‚ \u001b[35m \u001b[0m\u001b[35m         \u001b[0m\u001b[35m \u001b[0m â”‚  â”‚ \u001b[36m \u001b[0m\u001b[36m          \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m          \u001b[0m\u001b[35m \u001b[0m\u001b[35m \u001b[0m\u001b[35m          \u001b[0m\u001b[35m \u001b[0m â”‚                     â”‚ \n",
       " â”‚   â”‚ \u001b[35m \u001b[0m\u001b[35m2        \u001b[0m\u001b[35m \u001b[0m â”‚  â”‚ \u001b[36m \u001b[0m\u001b[36m4G~12G    \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m2~8       \u001b[0m\u001b[35m \u001b[0m\u001b[35m \u001b[0m\u001b[35m0         \u001b[0m\u001b[35m \u001b[0m â”‚                     â”‚ \n",
       " â”‚   â”‚ \u001b[35m \u001b[0m\u001b[35m         \u001b[0m\u001b[35m \u001b[0m â”‚  â”‚ \u001b[36m \u001b[0m\u001b[36m          \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m          \u001b[0m\u001b[35m \u001b[0m\u001b[35m \u001b[0m\u001b[35m          \u001b[0m\u001b[35m \u001b[0m â”‚                     â”‚ \n",
       " â”‚   â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯  â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯                     â”‚ \n",
       " â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RayCluster(name='llama-factory-test', status=<RayClusterStatus.READY: 'ready'>, head_cpu_requests=1, head_cpu_limits=1, head_mem_requests='6G', head_mem_limits='6G', num_workers=2, worker_mem_requests='4G', worker_mem_limits='12G', worker_cpu_requests='2', worker_cpu_limits=8, namespace='rhods-notebooks', dashboard='https://ray-dashboard-llama-factory-test-rhods-notebooks.apps.demo-01-rhsys.wzhlab.top', worker_extended_resources={'nvidia.com/gpu': 0}, head_extended_resources={'nvidia.com/gpu': 0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.details()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33663f47",
   "metadata": {},
   "source": [
    "This time we will demonstrate another potential method of use: working with the Ray cluster interactively.\n",
    "\n",
    "Using the SDK, we can get both the Ray cluster URI and dashboard URI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1719bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ray-dashboard-llama-factory-test-rhods-notebooks.apps.demo-01-rhsys.wzhlab.top\n",
      "ray://llama-factory-test-head-svc.rhods-notebooks.svc:10001\n"
     ]
    }
   ],
   "source": [
    "ray_dashboard_uri = cluster.cluster_dashboard_uri()\n",
    "ray_cluster_uri = cluster.cluster_uri()\n",
    "print(ray_dashboard_uri)\n",
    "print(ray_cluster_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2aca6a",
   "metadata": {},
   "source": [
    "Now we can connect directly to our Ray cluster via the Ray python client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9436436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codeflare_sdk import generate_cert\n",
    "# Create required TLS cert and export the environment variables to enable TLS\n",
    "generate_cert.generate_tls_cert(cluster_name, cluster.config.namespace)\n",
    "generate_cert.export_env(cluster_name, cluster.config.namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "300146dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 05:52:57,153\tINFO client_builder.py:244 -- Passing the following kwargs to ray.init() on the server: ignore_reinit_error\n",
      "SIGTERM handler is not set because current thread is not the main thread.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray cluster is up and running:  True\n"
     ]
    }
   ],
   "source": [
    "# before proceeding make sure the cluster exists and the uri is not empty\n",
    "assert ray_cluster_uri, \"Ray cluster needs to be started and set before proceeding\"\n",
    "\n",
    "import ray\n",
    "\n",
    "# reset the ray context in case there's already one. \n",
    "ray.shutdown()\n",
    "# establish connection to ray cluster\n",
    "\n",
    "# install additional libraries that will be required for model training\n",
    "# runtime_env = {\"pip\": [\"transformers==4.41.2\", \"datasets==2.17.0\", \"accelerate==0.31.0\", \"scikit-learn==1.5.0\"]}\n",
    "runtime_env = {}\n",
    "# NOTE: This will work for in-cluster notebook servers (RHODS/ODH), but not for local machines\n",
    "# To see how to connect from your laptop, go to demo-notebooks/additional-demos/local_interactive.ipynb\n",
    "ray.init(address=ray_cluster_uri, runtime_env=runtime_env, ignore_reinit_error=True)\n",
    "\n",
    "print(\"Ray cluster is up and running: \", ray.is_initialized())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9711030b",
   "metadata": {},
   "source": [
    "Now that we are connected (and have passed in some package requirements), let's try writing some training code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b36e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Job Submission Client\n",
    "client = cluster.job_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8fd65",
   "metadata": {},
   "source": [
    "Once we want to test our code out, we can run the training function we defined above remotely on our Ray cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5901d958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor 1 IP: 10.132.0.201\n",
      "Actor 2 IP: 10.132.0.201\n",
      "Actor 1 command result: [2025-01-09 05:54:35,074] [WARNING] [real_accelerator.py:162:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2025-01-09 05:54:35,087] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n",
      "[INFO|2025-01-09 05:54:39] llamafactory.cli:157 >> Initializing distributed tasks at: 10.132.0.201:29500\n",
      "[2025-01-09 05:54:46,833] [WARNING] [real_accelerator.py:162:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2025-01-09 05:54:46,845] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n",
      "[2025-01-09 05:54:48,450] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2025-01-09 05:54:48,450] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend gloo\n",
      "[1/3] c++ -MMD -MF shm_interface.o.d -DTORCH_EXTENSION_NAME=deepspeed_shm_comm -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/app-root/lib64/python3.11/site-packages/deepspeed/ops/csrc/cpu/includes -isystem /opt/app-root/lib64/python3.11/site-packages/torch/include -isystem /opt/app-root/lib64/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /opt/app-root/lib64/python3.11/site-packages/torch/include/TH -isystem /opt/app-root/lib64/python3.11/site-packages/torch/include/THC -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O2 -fopenmp -c /opt/app-root/lib64/python3.11/site-packages/deepspeed/ops/csrc/cpu/comm/shm_interface.cpp -o shm_interface.o \n",
      "/opt/app-root/lib64/python3.11/site-packages/deepspeed/ops/csrc/cpu/comm/shm_interface.cpp: In function â€˜void initialize(int, int)â€™:\n",
      "/opt/app-root/lib64/python3.11/site-packages/deepspeed/ops/csrc/cpu/comm/shm_interface.cpp:42:46: warning: ISO C++ forbids converting a string constant to â€˜char*â€™ [-Wwrite-strings]\n",
      "   42 |     if (addr_string == NULL) { addr_string = \"\"; }\n",
      "      |                                              ^~\n",
      "/opt/app-root/lib64/python3.11/site-packages/deepspeed/ops/csrc/cpu/comm/shm_interface.cpp:44:46: warning: ISO C++ forbids converting a string constant to â€˜char*â€™ [-Wwrite-strings]\n",
      "   44 |     if (port_string == NULL) { port_string = \"\"; }\n",
      "      |                                              ^~\n",
      "[2/3] c++ -MMD -MF shm.o.d -DTORCH_EXTENSION_NAME=deepspeed_shm_comm -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/app-root/lib64/python3.11/site-packages/deepspeed/ops/csrc/cpu/includes -isystem /opt/app-root/lib64/python3.11/site-packages/torch/include -isystem /opt/app-root/lib64/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /opt/app-root/lib64/python3.11/site-packages/torch/include/TH -isystem /opt/app-root/lib64/python3.11/site-packages/torch/include/THC -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O2 -fopenmp -c /opt/app-root/lib64/python3.11/site-packages/deepspeed/ops/csrc/cpu/comm/shm.cpp -o shm.o \n",
      "[3/3] c++ shm_interface.o shm.o -shared -L/opt/app-root/lib64/python3.11/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o deepspeed_shm_comm.so\n",
      "Time to load deepspeed_shm_comm op: 26.319498300552368 seconds\n",
      "DeepSpeed deepspeed.ops.comm.deepspeed_shm_comm_op built successfully\n",
      "[INFO|2025-01-09 05:55:15] llamafactory.hparams.parser:355 >> Process rank: 0, device: cpu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|2025-01-09 05:55:15] llamafactory.data.template:157 >> Add pad token: </s>\n",
      "[INFO|2025-01-09 05:55:15] llamafactory.data.loader:157 >> Loading dataset identity.json...\n",
      "[INFO|2025-01-09 05:55:16] llamafactory.data.loader:157 >> Loading dataset alpaca_en_demo.json...\n",
      "training example:\n",
      "input_ids:\n",
      "[1, 836, 1389, 2313, 31908, 23980, 836, 31873, 1389, 2313, 31908, 16644, 31905, 312, 705, 16717, 3227, 28035, 363, 7421, 8825, 3321, 417, 16717, 10935, 2338, 31843, 1035, 473, 312, 2803, 365, 31822, 31824, 16346, 31902, 2]\n",
      "inputs:\n",
      "<s> [INST] hi [/INST] Hello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?</s>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 16644, 31905, 312, 705, 16717, 3227, 28035, 363, 7421, 8825, 3321, 417, 16717, 10935, 2338, 31843, 1035, 473, 312, 2803, 365, 31822, 31824, 16346, 31902, 2]\n",
      "labels:\n",
      "Hello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?</s>\n",
      "[2025-01-09 05:55:18,418] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 75, num_elems = 0.00B\n",
      "[INFO|2025-01-09 05:55:18] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-01-09 05:55:18] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-01-09 05:55:18] llamafactory.model.adapter:157 >> ZeRO3 / FSDP detected, remaining trainable params in float32.\n",
      "[INFO|2025-01-09 05:55:18] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n",
      "[INFO|2025-01-09 05:55:18] llamafactory.model.model_utils.misc:157 >> Found linear modules: v_proj,gate_proj,down_proj,k_proj,q_proj,o_proj,up_proj\n",
      "[INFO|2025-01-09 05:55:18] llamafactory.model.loader:157 >> trainable params: 94,208 || all params: 4,715,584 || trainable%: 1.9978\n",
      "[2025-01-09 05:55:18,980] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.4, git-hash=unknown, git-branch=unknown\n",
      "[2025-01-09 05:55:19,025] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-01-09 05:55:19,027] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2025-01-09 05:55:19,027] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-01-09 05:55:19,031] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2025-01-09 05:55:19,031] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2025-01-09 05:55:19,031] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "[2025-01-09 05:55:19,031] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer\n",
      "[2025-01-09 05:55:19,298] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning\n",
      "[2025-01-09 05:55:19,299] [INFO] [utils.py:782:see_memory_usage] MA 0.79 GB         Max_MA 0.79 GB         CA 0.79 GB         Max_CA 1 GB \n",
      "[2025-01-09 05:55:19,299] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 31.14 GB, percent = 24.7%\n",
      "[2025-01-09 05:55:19,302] [INFO] [stage3.py:130:__init__] Reduce bucket size 4096\n",
      "[2025-01-09 05:55:19,302] [INFO] [stage3.py:131:__init__] Prefetch bucket size 3686\n",
      "[2025-01-09 05:55:19,569] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2025-01-09 05:55:19,570] [INFO] [utils.py:782:see_memory_usage] MA 0.79 GB         Max_MA 0.79 GB         CA 0.79 GB         Max_CA 1 GB \n",
      "[2025-01-09 05:55:19,570] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 31.14 GB, percent = 24.7%\n",
      "Parameter Offload: Total persistent parameters: 46144 in 105 params\n",
      "[2025-01-09 05:55:19,962] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2025-01-09 05:55:19,962] [INFO] [utils.py:782:see_memory_usage] MA 0.8 GB         Max_MA 0.8 GB         CA 0.8 GB         Max_CA 1 GB \n",
      "[2025-01-09 05:55:19,962] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 31.13 GB, percent = 24.7%\n",
      "[2025-01-09 05:55:20,253] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions\n",
      "[2025-01-09 05:55:20,254] [INFO] [utils.py:782:see_memory_usage] MA 0.8 GB         Max_MA 0.8 GB         CA 0.8 GB         Max_CA 1 GB \n",
      "[2025-01-09 05:55:20,254] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 31.12 GB, percent = 24.7%\n",
      "[2025-01-09 05:55:20,814] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 1\n",
      "[2025-01-09 05:55:20,814] [INFO] [utils.py:782:see_memory_usage] MA 0.8 GB         Max_MA 0.8 GB         CA 0.8 GB         Max_CA 1 GB \n",
      "[2025-01-09 05:55:20,814] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 31.13 GB, percent = 24.7%\n",
      "[2025-01-09 05:55:21,087] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions\n",
      "[2025-01-09 05:55:21,088] [INFO] [utils.py:782:see_memory_usage] MA 0.8 GB         Max_MA 0.8 GB         CA 0.8 GB         Max_CA 1 GB \n",
      "[2025-01-09 05:55:21,088] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 31.13 GB, percent = 24.7%\n",
      "[2025-01-09 05:55:21,364] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions\n",
      "[2025-01-09 05:55:21,365] [INFO] [utils.py:782:see_memory_usage] MA 0.8 GB         Max_MA 0.8 GB         CA 0.8 GB         Max_CA 1 GB \n",
      "[2025-01-09 05:55:21,365] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 31.13 GB, percent = 24.7%\n",
      "[2025-01-09 05:55:21,648] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-01-09 05:55:21,648] [INFO] [utils.py:782:see_memory_usage] MA 0.8 GB         Max_MA 0.8 GB         CA 0.8 GB         Max_CA 1 GB \n",
      "[2025-01-09 05:55:21,649] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 31.13 GB, percent = 24.7%\n",
      "[2025-01-09 05:55:21,929] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-01-09 05:55:21,929] [INFO] [utils.py:782:see_memory_usage] MA 0.8 GB         Max_MA 0.8 GB         CA 0.8 GB         Max_CA 1 GB \n",
      "[2025-01-09 05:55:21,929] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 31.14 GB, percent = 24.7%\n",
      "[2025-01-09 05:55:21,930] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized\n",
      "[2025-01-09 05:55:22,248] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-01-09 05:55:22,248] [INFO] [utils.py:782:see_memory_usage] MA 0.8 GB         Max_MA 0.8 GB         CA 0.8 GB         Max_CA 1 GB \n",
      "[2025-01-09 05:55:22,248] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 31.16 GB, percent = 24.8%\n",
      "[2025-01-09 05:55:22,249] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3\n",
      "[2025-01-09 05:55:22,249] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2025-01-09 05:55:22,249] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2025-01-09 05:55:22,249] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]\n",
      "[2025-01-09 05:55:22,250] [INFO] [config.py:997:print] DeepSpeedEngine configuration:\n",
      "[2025-01-09 05:55:22,251] [INFO] [config.py:1001:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-01-09 05:55:22,251] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2025-01-09 05:55:22,251] [INFO] [config.py:1001:print]   amp_enabled .................. False\n",
      "[2025-01-09 05:55:22,251] [INFO] [config.py:1001:print]   amp_params ................... False\n",
      "[2025-01-09 05:55:22,251] [INFO] [config.py:1001:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-01-09 05:55:22,251] [INFO] [config.py:1001:print]   bfloat16_enabled ............. True\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5ba43c28d0>\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   communication_data_type ...... None\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   disable_allgather ............ False\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   dump_state ................... False\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   elasticity_enabled ........... False\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   fp16_auto_cast ............... None\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   fp16_enabled ................. False\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   global_rank .................. 0\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 8\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   gradient_clipping ............ 1.0\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   graph_harvesting ............. False\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 1\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   loss_scale ................... 1.0\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   memory_breakdown ............. False\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   mics_shard_size .............. -1\n",
      "[2025-01-09 05:55:22,252] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   optimizer_name ............... None\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   optimizer_params ............. None\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   pld_enabled .................. False\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   pld_params ................... False\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   prescale_gradients ........... False\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   scheduler_name ............... None\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   scheduler_params ............. None\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   sparse_attention ............. None\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   steps_per_print .............. inf\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   train_batch_size ............. 16\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  1\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   use_node_local_storage ....... False\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   weight_quantization_config ... None\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   world_size ................... 2\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=4096 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=3686 param_persistence_threshold=640 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   zero_enabled ................. True\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 3\n",
      "[2025-01-09 05:55:22,253] [INFO] [config.py:987:print_user_config]   json = {\n",
      "    \"train_batch_size\": 16, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"gradient_accumulation_steps\": 8, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"zero_allow_untested_optimizer\": true, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "        \"reduce_bucket_size\": 4.096000e+03, \n",
      "        \"stage3_prefetch_bucket_size\": 3.686000e+03, \n",
      "        \"stage3_param_persistence_threshold\": 640, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
      "    }, \n",
      "    \"steps_per_print\": inf\n",
      "}\n",
      "[2025-01-09 05:55:45,018] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3 is about to be saved!\n",
      "[2025-01-09 05:55:45,029] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/tinyllama/lora/sft/checkpoint-3/global_step3/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
      "[2025-01-09 05:55:45,029] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/tinyllama/lora/sft/checkpoint-3/global_step3/zero_pp_rank_0_mp_rank_00_model_states.pt...\n",
      "[2025-01-09 05:55:45,035] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/tinyllama/lora/sft/checkpoint-3/global_step3/zero_pp_rank_0_mp_rank_00_model_states.pt.\n",
      "[2025-01-09 05:55:45,037] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/tinyllama/lora/sft/checkpoint-3/global_step3/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-01-09 05:55:45,038] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/tinyllama/lora/sft/checkpoint-3/global_step3/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-01-09 05:55:45,039] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved saves/tinyllama/lora/sft/checkpoint-3/global_step3/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-01-09 05:55:45,058] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3 is ready now!\n",
      "{'train_runtime': 22.8056, 'train_samples_per_second': 2.368, 'train_steps_per_second': 0.132, 'train_loss': 12.78365961710612, 'epoch': 2.67}\n",
      "***** train metrics *****\n",
      "  epoch                    =     2.6667\n",
      "  total_flos               =        1GF\n",
      "  train_loss               =    12.7837\n",
      "  train_runtime            = 0:00:22.80\n",
      "  train_samples_per_second =      2.368\n",
      "  train_steps_per_second   =      0.132\n",
      "[WARNING|2025-01-09 05:55:45] llamafactory.extras.ploting:162 >> No metric loss to plot.\n",
      "[WARNING|2025-01-09 05:55:45] llamafactory.extras.ploting:162 >> No metric eval_loss to plot.\n",
      "[WARNING|2025-01-09 05:55:45] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.\n",
      "***** eval metrics *****\n",
      "  epoch                   =     2.6667\n",
      "  eval_loss               =     7.5629\n",
      "  eval_runtime            = 0:00:00.24\n",
      "  eval_samples_per_second =       8.32\n",
      "  eval_steps_per_second   =       4.16\n",
      "\n",
      "Actor 2 command result: [2025-01-09 05:54:35,080] [WARNING] [real_accelerator.py:162:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2025-01-09 05:54:35,094] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n",
      "[INFO|2025-01-09 05:54:39] llamafactory.cli:157 >> Initializing distributed tasks at: 10.132.0.201:29500\n",
      "[2025-01-09 05:54:46,888] [WARNING] [real_accelerator.py:162:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2025-01-09 05:54:46,900] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n",
      "[2025-01-09 05:54:48,473] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "Time to load deepspeed_shm_comm op: 26.349100828170776 seconds\n",
      "DeepSpeed deepspeed.ops.comm.deepspeed_shm_comm_op built successfully\n",
      "[INFO|2025-01-09 05:55:15] llamafactory.hparams.parser:355 >> Process rank: 0, device: cpu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|2025-01-09 05:55:15] llamafactory.data.template:157 >> Add pad token: </s>\n",
      "[INFO|2025-01-09 05:55:15] llamafactory.data.loader:157 >> Loading dataset identity.json...\n",
      "[INFO|2025-01-09 05:55:16] llamafactory.data.loader:157 >> Loading dataset alpaca_en_demo.json...\n",
      "training example:\n",
      "input_ids:\n",
      "[1, 836, 1389, 2313, 31908, 23980, 836, 31873, 1389, 2313, 31908, 16644, 31905, 312, 705, 16717, 3227, 28035, 363, 7421, 8825, 3321, 417, 16717, 10935, 2338, 31843, 1035, 473, 312, 2803, 365, 31822, 31824, 16346, 31902, 2]\n",
      "inputs:\n",
      "<s> [INST] hi [/INST] Hello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?</s>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 16644, 31905, 312, 705, 16717, 3227, 28035, 363, 7421, 8825, 3321, 417, 16717, 10935, 2338, 31843, 1035, 473, 312, 2803, 365, 31822, 31824, 16346, 31902, 2]\n",
      "labels:\n",
      "Hello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?</s>\n",
      "[INFO|2025-01-09 05:55:18] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-01-09 05:55:18] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-01-09 05:55:18] llamafactory.model.adapter:157 >> ZeRO3 / FSDP detected, remaining trainable params in float32.\n",
      "[INFO|2025-01-09 05:55:18] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n",
      "[INFO|2025-01-09 05:55:18] llamafactory.model.model_utils.misc:157 >> Found linear modules: v_proj,o_proj,k_proj,q_proj,up_proj,gate_proj,down_proj\n",
      "[INFO|2025-01-09 05:55:18] llamafactory.model.loader:157 >> trainable params: 94,208 || all params: 4,715,584 || trainable%: 1.9978\n",
      "[2025-01-09 05:55:45,027] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: saves/tinyllama/lora/sft/checkpoint-3/global_step3/zero_pp_rank_1_mp_rank_00_model_states.pt\n",
      "[2025-01-09 05:55:45,028] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/tinyllama/lora/sft/checkpoint-3/global_step3/zero_pp_rank_1_mp_rank_00_model_states.pt...\n",
      "[2025-01-09 05:55:45,035] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/tinyllama/lora/sft/checkpoint-3/global_step3/zero_pp_rank_1_mp_rank_00_model_states.pt.\n",
      "[2025-01-09 05:55:45,037] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/tinyllama/lora/sft/checkpoint-3/global_step3/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...\n",
      "[2025-01-09 05:55:45,038] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/tinyllama/lora/sft/checkpoint-3/global_step3/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.\n",
      "[2025-01-09 05:55:45,038] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved saves/tinyllama/lora/sft/checkpoint-3/global_step3/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt\n",
      "[2025-01-09 05:55:45,058] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3 is ready now!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "submission_id = client.submit_job(\n",
    "    entrypoint=f\"llamafactory-cli train wzh/tinyllama_lora_sft_ray.yaml\",\n",
    "    runtime_env={\n",
    "        \"env_vars\": {\n",
    "            'USE_RAY': '1'\n",
    "        },\n",
    "        # 'pip': 'requirements.txt',\n",
    "        'working_dir': './',\n",
    "        \"excludes\": [\"/docs/\", \"*.ipynb\", \"*.md\"]\n",
    "    },\n",
    ")\n",
    "print(submission_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8cd32",
   "metadata": {},
   "source": [
    "Once complete, we can bring our Ray cluster down and clean up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f1ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.stop_job(submission_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f36db0f-31f6-4373-9503-dc3c1c4c3f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray Cluster: 'llama-factory-test' has successfully been deleted\n"
     ]
    }
   ],
   "source": [
    "cluster.down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d41b90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Successfully logged out of https://api.demo-01-rhsys.wzhlab.top:6443'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auth.logout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af72628-f335-4b09-bafb-ed89e00fcb54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
